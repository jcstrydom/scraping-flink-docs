{"success":true,"data":{"markdown":"[![](https://nightlies.apache.org/flink/flink-docs-release-1.20/flink-header-logo.svg)](https://nightlies.apache.org/flink/flink-docs-release-1.20)\n\nv1.20.2\n\n- [ ]\n    Try Flink\n  ▾  - [First steps](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/try-flink/local_installation/)\n  - [Fraud Detection with the DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/try-flink/datastream/)\n  - [Real Time Reporting with the Table API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/try-flink/table_api/)\n  - [Flink Operations Playground](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/try-flink/flink-operations-playground/)\n- [ ]\n    Learn Flink\n  ▾  - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview/)\n  - [Intro to the DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/datastream_api/)\n  - [Data Pipelines & ETL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl/)\n  - [Streaming Analytics](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics/)\n  - [Event-driven Applications](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/event_driven/)\n  - [Fault Tolerance](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance/)\n- [ ]\n    Concepts\n  ▾  - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/)\n  - [Stateful Stream Processing](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/)\n  - [Timely Stream Processing](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/time/)\n  - [Flink Architecture](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/flink-architecture/)\n  - [Glossary](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/glossary/)\n- [x]\n    Application Development\n  ▾  - [ ] Project Configuration▾    - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/configuration/overview/)\n    - [Using Maven](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/configuration/maven/)\n    - [Using Gradle](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/configuration/gradle/)\n    - [Connectors and Formats](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/configuration/connector/)\n    - [Test Dependencies](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/configuration/testing/)\n    - [Advanced Configuration](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/configuration/advanced/)\n  - [x] DataStream API▾    - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/)\n    - [Execution Mode (Batch/Streaming)](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/execution_mode/)\n    - [ ] Event Time▾      - [Generating Watermarks](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/event-time/generating_watermarks/)\n      - [Builtin Watermark Generators](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/event-time/built_in/)\n    - [ ] State & Fault Tolerance▾      - [Working with State](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/state/)\n      - [The Broadcast State Pattern](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/broadcast_state/)\n      - [Checkpointing](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/checkpointing/)\n      - [State Backends](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/state_backends/)\n      - [ ] Data Types & Serialization▾        - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/serialization/types_serialization/)\n        - [State Schema Evolution](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/)\n        - [Custom State Serialization](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/)\n        - [3rd Party Serializers](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/serialization/third_party_serializers/)\n    - [User-Defined Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/user_defined_functions/)\n    - [ ] Operators▾      - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/overview/)\n      - [Windows](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/windows/)\n      - [Joining](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/joining/)\n      - [Process Function](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function/)\n      - [Async I/O](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/asyncio/)\n      - [Full Window Partition](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/full_window_partition/)\n    - [Data Sources](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/sources/)\n    - [Side Outputs](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/side_output/)\n    - [Handling Application Parameters](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/application_parameters/)\n    - [Testing](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/testing/)\n    - [Experimental Features](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/experimental/)\n    - [Scala API Extensions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/scala_api_extensions/)\n    - [Java Lambda Expressions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/java_lambdas/)\n    - [ ] Managing Execution▾      - [Execution Configuration](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/execution/execution_configuration/)\n      - [Program Packaging](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/execution/packaging/)\n      - [Parallel Execution](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/execution/parallel/)\n    - [How to Migrate from DataSet to DataStream](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/dataset_migration/)\n  - [ ] Table API & SQL▾    - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/)\n    - [Concepts & Common API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/common/)\n    - [DataStream API Integration](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/data_stream_api/)\n    - [ ] Streaming Concepts▾      - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/concepts/overview/)\n      - [Determinism in Continuous Queries](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/concepts/determinism/)\n      - [Dynamic Tables](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/concepts/dynamic_tables/)\n      - [Time Attributes](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/concepts/time_attributes/)\n      - [Versioned Tables](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/concepts/versioned_tables/)\n      - [Temporal Table Function](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/concepts/temporal_table_function/)\n    - [Data Types](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/types/)\n    - [Time Zone](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/timezone/)\n    - [Table API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/tableapi/)\n    - [ ] SQL▾      - [SQL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/overview/)\n      - [Getting Started](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/gettingstarted/)\n      - [ ] Queries▾        - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/overview/)\n        - [Hints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/hints/)\n        - [WITH clause](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/with/)\n        - [SELECT & WHERE](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/select/)\n        - [SELECT DISTINCT](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/select-distinct/)\n        - [Windowing TVF](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/window-tvf/)\n        - [Window Aggregation](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/window-agg/)\n        - [Group Aggregation](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/group-agg/)\n        - [Over Aggregation](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/over-agg/)\n        - [Joins](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/joins/)\n        - [Window JOIN](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/window-join/)\n        - [Set Operations](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/set-ops/)\n        - [ORDER BY clause](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/orderby/)\n        - [LIMIT clause](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/limit/)\n        - [Top-N](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/topn/)\n        - [Window Top-N](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/window-topn/)\n        - [Deduplication](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/deduplication/)\n        - [Window Deduplication](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/window-deduplication/)\n        - [Pattern Recognition](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/match_recognize/)\n        - [Time Travel](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/queries/time-travel/)\n      - [CREATE Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/create/)\n      - [DROP Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/drop/)\n      - [ALTER Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/alter/)\n      - [INSERT Statement](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/insert/)\n      - [ANALYZE Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/analyze/)\n      - [DESCRIBE Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/describe/)\n      - [TRUNCATE Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/truncate/)\n      - [EXPLAIN Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/explain/)\n      - [USE Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/use/)\n      - [SHOW Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/show/)\n      - [LOAD Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/load/)\n      - [UNLOAD Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/unload/)\n      - [SET Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/set/)\n      - [RESET Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/reset/)\n      - [JAR Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/jar/)\n      - [JOB Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/job/)\n      - [UPDATE Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/update/)\n      - [DELETE Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/delete/)\n      - [CALL Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/call/)\n    - [ ] Materialized Table▾      - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/materialized-table/overview/)\n      - [Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/materialized-table/statements/)\n      - [Quickstart](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/materialized-table/quickstart/)\n    - [ ] Functions▾      - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/functions/overview/)\n      - [System (Built-in) Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/functions/systemfunctions/)\n      - [User-defined Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/functions/udfs/)\n    - [Procedures](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/procedures/)\n    - [Modules](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/modules/)\n    - [Catalogs](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/catalogs/)\n    - [Flink JDBC Driver](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/jdbcdriver/)\n    - [OLAP Quickstart](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/olap_quickstart/)\n    - [SQL Client](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sqlclient/)\n    - [ ] SQL Gateway▾      - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql-gateway/overview/)\n      - [REST Endpoint](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql-gateway/rest/)\n      - [HiveServer2 Endpoint](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql-gateway/hiveserver2/)\n    - [ ] Hive Compatibility▾      - [ ] Hive Dialect▾        - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/overview/)\n        - [ ] Queries▾          - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/overview/)\n          - [Sort/Cluster/Distributed By](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/sort-cluster-distribute-by/)\n          - [Group By](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/group-by/)\n          - [Join](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/join/)\n          - [Set Operations](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/set-op/)\n          - [Lateral View Clause](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/lateral-view/)\n          - [Window Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/window-functions/)\n          - [Sub-Queries](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/sub-queries/)\n          - [CTE](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/cte/)\n          - [Transform Clause](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/transform/)\n          - [Table Sample](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/queries/table-sample/)\n        - [CREATE Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/create/)\n        - [DROP Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/drop/)\n        - [ALTER Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/alter/)\n        - [INSERT Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/insert/)\n        - [Load Data Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/load-data/)\n        - [SHOW Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/show/)\n        - [ADD Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/add/)\n        - [SET Statements](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/hive-compatibility/hive-dialect/set/)\n    - [Configuration](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/config/)\n    - [Performance Tuning](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/tuning/)\n    - [User-defined Sources & Sinks](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sourcessinks/)\n  - [ ] Python API▾    - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/overview/)\n    - [Installation](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/installation/)\n    - [Table API Tutorial](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table_api_tutorial/)\n    - [DataStream API Tutorial](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/datastream_tutorial/)\n    - [ ] Table API▾      - [Intro to the Python Table API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/intro_to_table_api/)\n      - [TableEnvironment](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/table_environment/)\n      - [ ] Operations▾        - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/operations/operations/)\n        - [Row-based Operations](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/operations/row_based_operations/)\n      - [Data Types](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/python_types/)\n      - [System (Built-in) Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/system_functions/)\n      - [ ] User Defined Functions▾        - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/udfs/overview/)\n        - [General User-defined Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/udfs/python_udfs/)\n        - [Vectorized User-defined Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/udfs/vectorized_python_udfs/)\n      - [Conversions between PyFlink Table and Pandas DataFrame](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/conversion_of_pandas/)\n      - [Conversions between Table and DataStream](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/conversion_of_data_stream/)\n      - [SQL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/sql/)\n      - [Catalogs](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/catalogs/)\n      - [Metrics](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/metrics/)\n      - [Connectors](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/table/python_table_api_connectors/)\n    - [ ] DataStream API▾      - [Intro to the Python DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/datastream/intro_to_datastream_api/)\n      - [ ] Operators▾        - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/datastream/operators/overview/)\n        - [Windows](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/datastream/operators/windows/)\n        - [Process Function](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/datastream/operators/process_function/)\n      - [Data Types](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/datastream/data_types/)\n      - [State](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/datastream/state/)\n    - [Dependency Management](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/dependency_management/)\n    - [Execution Mode](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/python_execution_mode/)\n    - [Configuration](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/python_config/)\n    - [Debugging](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/debugging/)\n    - [Environment Variables](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/environment_variables/)\n    - [FAQ](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/faq/)\n- [ ]\n    Libraries\n  ▾  - [Event Processing (CEP)](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/libs/cep/)\n  - [State Processor API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/libs/state_processor_api/)\n- [ ]\n    Connectors\n  ▾  - [ ] DataStream Connectors▾    - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/overview/)\n    - [Fault Tolerance Guarantees](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/guarantees/)\n    - [ ] Formats▾      - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/formats/overview/)\n      - [Avro](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/formats/avro/)\n      - [Azure Table storage](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/formats/azure_table_storage/)\n      - [CSV](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/formats/csv/)\n      - [Hadoop](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/formats/hadoop/)\n      - [JSON](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/formats/json/)\n      - [Parquet](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/formats/parquet/)\n      - [Text files](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/formats/text_files/)\n    - [DataGen](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/datagen/)\n    - [Dynamic Kafka](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/dynamic-kafka/)\n    - [Kafka](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/kafka/)\n    - [Cassandra](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/cassandra/)\n    - [DynamoDB](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/dynamodb/)\n    - [Elasticsearch](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/elasticsearch/)\n    - [Firehose](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/firehose/)\n    - [Kinesis](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/kinesis/)\n    - [MongoDB](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/mongodb/)\n    - [Opensearch](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/opensearch/)\n    - [Prometheus](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/prometheus/)\n    - [SQS](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/sqs/)\n    - [FileSystem](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/filesystem/)\n    - [Kudu](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/kudu/)\n    - [RabbitMQ](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/rabbitmq/)\n    - [Google Cloud PubSub](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/pubsub/)\n    - [Hybrid Source](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/hybridsource/)\n    - [Pulsar](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/pulsar/)\n    - [JDBC](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/jdbc/)\n  - [ ] Table API Connectors▾    - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/overview/)\n    - [ ] Formats▾      - [Formats](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/overview/)\n      - [CSV](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/csv/)\n      - [JSON](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/json/)\n      - [Avro](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/avro/)\n      - [Confluent Avro](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/avro-confluent/)\n      - [Protobuf](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/protobuf/)\n      - [Debezium](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/debezium/)\n      - [Canal](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/canal/)\n      - [Maxwell](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/maxwell/)\n      - [Ogg](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/ogg/)\n      - [Parquet](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/parquet/)\n      - [Orc](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/orc/)\n      - [Raw](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/formats/raw/)\n    - [Kafka](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/kafka/)\n    - [Upsert Kafka](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/upsert-kafka/)\n    - [DynamoDB](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/dynamodb/)\n    - [Firehose](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/firehose/)\n    - [Kinesis](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/kinesis/)\n    - [MongoDB](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/mongodb/)\n    - [JDBC](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/jdbc/)\n    - [Kudu](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/kudu/)\n    - [Elasticsearch](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/elasticsearch/)\n    - [Opensearch](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/opensearch/)\n    - [FileSystem](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/filesystem/)\n    - [HBase](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/hbase/)\n    - [DataGen](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/datagen/)\n    - [Print](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/print/)\n    - [BlackHole](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/blackhole/)\n    - [ ] Hive▾      - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/hive/overview/)\n      - [Hive Catalog](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/hive/hive_catalog/)\n      - [Hive Read & Write](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/hive/hive_read_write/)\n      - [Hive Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/hive/hive_functions/)\n    - [Download](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/downloads/)\n- [ ]\n    Deployment\n  ▾  - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/overview/)\n  - [Java Compatibility](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/java_compatibility/)\n  - [ ] Resource Providers▾    - [ ] Standalone▾      - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/resource-providers/standalone/overview/)\n      - [Working Directory](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/resource-providers/standalone/working_directory/)\n      - [Docker](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/resource-providers/standalone/docker/)\n      - [Kubernetes](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/resource-providers/standalone/kubernetes/)\n    - [Native Kubernetes](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/resource-providers/native_kubernetes/)\n    - [YARN](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/resource-providers/yarn/)\n  - [Configuration](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/)\n  - [ ] Memory Configuration▾    - [Set up Flink's Process Memory](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/memory/mem_setup/)\n    - [Set up TaskManager Memory](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/memory/mem_setup_tm/)\n    - [Set up JobManager Memory](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/memory/mem_setup_jobmanager/)\n    - [Memory Tuning Guide](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/memory/mem_tuning/)\n    - [Troubleshooting](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/memory/mem_trouble/)\n    - [Migration Guide](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/memory/mem_migration/)\n    - [Network Buffer Tuning](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/memory/network_mem_tuning/)\n  - [Command-Line Interface](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/cli/)\n  - [Elastic Scaling](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/elastic_scaling/)\n  - [Fine-Grained Resource Management](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/finegrained_resource/)\n  - [Speculative Execution](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/speculative_execution/)\n  - [ ] File Systems▾    - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/overview/)\n    - [Common Configurations](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/common/)\n    - [Amazon S3](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/s3/)\n    - [Google Cloud Storage](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/gcs/)\n    - [Aliyun OSS](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/oss/)\n    - [Azure Blob Storage](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/azure/)\n    - [Plugins](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/plugins/)\n  - [ ] High Availability▾    - [Overview](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/)\n    - [ZooKeeper HA Services](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/zookeeper_ha/)\n    - [Kubernetes HA Services](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/kubernetes_ha/)\n  - [Metric Reporters](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/metric_reporters/)\n  - [Trace Reporters](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/trace_reporters/)\n  - [ ] REPLs▾    - [Python REPL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/repls/python_shell/)\n  - [ ] Security▾    - [SSL Setup](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/security/security-ssl/)\n    - [Kerberos](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/security/security-kerberos/)\n    - [Delegation tokens](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/security/security-delegation-token/)\n  - [ ] Advanced▾    - [External Resources](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/advanced/external_resources/)\n    - [History Server](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/advanced/historyserver/)\n    - [Logging](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/advanced/logging/)\n    - [Failure Enrichers](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/advanced/failure_enrichers/)\n- [ ]\n    Operations\n  ▾  - [ ] State & Fault Tolerance▾    - [Checkpoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/checkpoints/)\n    - [Checkpointing under backpressure](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/checkpointing_under_backpressure/)\n    - [Savepoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/savepoints/)\n    - [Checkpoints vs. Savepoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/checkpoints_vs_savepoints/)\n    - [State Backends](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/state_backends/)\n    - [Tuning Checkpoints and Large State](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/large_state_tuning/)\n    - [Task Failure Recovery](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/task_failure_recovery/)\n  - [Metrics](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/metrics/)\n  - [Traces](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/traces/)\n  - [REST API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/rest_api/)\n  - [ ] Batch▾    - [Batch Shuffle](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/batch/batch_shuffle/)\n    - [Recovery job progress from job master failures](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/batch/recovery_from_job_master_failure/)\n  - [ ] Debugging▾    - [Debugging Windows & Event Time](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/debugging/debugging_event_time/)\n    - [Debugging Classloading](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/debugging/debugging_classloading/)\n    - [Flame Graphs](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/debugging/flame_graphs/)\n    - [Profiler](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/debugging/profiler/)\n    - [Application Profiling & Debugging](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/debugging/application_profiling/)\n  - [ ] Monitoring▾    - [Monitoring Checkpointing](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/monitoring/checkpoint_monitoring/)\n    - [Monitoring Back Pressure](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/monitoring/back_pressure/)\n  - [Upgrading Applications and Flink Versions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/upgrading/)\n  - [Production Readiness Checklist](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/production_ready/)\n- [ ]\n    Flink Development\n  ▾  - [Importing Flink into an IDE](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/flinkdev/ide_setup/)\n  - [Building Flink from Source](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/flinkdev/building/)\n- [ ]\n    Internals\n  ▾  - [Jobs and Scheduling](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/internals/job_scheduling/)\n  - [Task Lifecycle](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/internals/task_lifecycle/)\n  - [File Systems](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/internals/filesystems/)\n\n* * *\n\n[Project Homepage](https://flink.apache.org/)\n\n[JavaDocs](https://nightlies.apache.org/flink/flink-docs-release-1.20/api/java/)\n\n[ScalaDocs](https://nightlies.apache.org/flink/flink-docs-release-1.20/api/scala/index.html#org.apache.flink.api.scala.package/)\n\n[PyDocs](https://nightlies.apache.org/flink/flink-docs-release-1.20/api/python/)\n\n* * *\n\n[ ]\nPick Docs Version\n▾[1.20 (✓)](https://nightlies.apache.org/flink/flink-docs-release-1.20)\n\n* * *\n\n- [v1.20](http://nightlies.apache.org/flink/flink-docs-release-1.20)\n- [v1.19](http://nightlies.apache.org/flink/flink-docs-release-1.19)\n\n* * *\n\n- [All Versions](https://nightlies.apache.org/flink/flink-docs-release-1.20/versions)\n[中文版](https://nightlies.apache.org/flink/flink-docs-release-1.20/zh/docs/dev/datastream/overview/)\n\n![Menu](https://nightlies.apache.org/flink/flink-docs-release-1.20/svg/menu.svg)**Overview**![Table of Contents](https://nightlies.apache.org/flink/flink-docs-release-1.20/svg/toc.svg)\n\n### On This Page\n\n- [Flink DataStream API Programming Guide](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#flink-datastream-api-programming-guide)\n  - [What is a DataStream?](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#what-is-a-datastream)\n  - [Anatomy of a Flink Program](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#anatomy-of-a-flink-program)\n  - [Example Program](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#example-program)\n  - [Data Sources](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#data-sources)\n  - [DataStream Transformations](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#datastream-transformations)\n  - [Data Sinks](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#data-sinks)\n  - [Execution Parameters](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#execution-parameters)\n    - [Fault Tolerance](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#fault-tolerance)\n    - [Controlling Latency](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#controlling-latency)\n  - [Debugging](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#debugging)\n    - [Local Execution Environment](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#local-execution-environment)\n    - [Collection Data Sources](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#collection-data-sources)\n    - [Iterator Data Sink](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#iterator-data-sink)\n  - [Where to go next?](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#where-to-go-next)\n\n# Flink DataStream API Programming Guide  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#flink-datastream-api-programming-guide)\n\nDataStream programs in Flink are regular programs that implement transformations on data streams\n(e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various\nsources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for\nexample write the data to files, or to standard output (for example the command line\nterminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs.\nThe execution can happen in a local JVM, or on clusters of many machines.\n\nIn order to create your own Flink DataStream program, we encourage you to start\nwith [anatomy of a Flink Program](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#anatomy-of-a-flink-program) and gradually\nadd your own [stream transformations](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/overview/). The remaining sections act as references for additional operations and advanced features.\n\n## What is a DataStream?  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#what-is-a-datastream)\n\nThe DataStream API gets its name from the special `DataStream` class that is\nused to represent a collection of data in a Flink program. You can think of\nthem as immutable collections of data that can contain duplicates. This data\ncan either be finite or unbounded, the API that you use to work on them is the\nsame.\n\nA `DataStream` is similar to a regular Java `Collection` in terms of usage but\nis quite different in some key ways. They are immutable, meaning that once they\nare created you cannot add or remove elements. You can also not simply inspect\nthe elements inside but only work on them using the `DataStream` API\noperations, which are also called transformations.\n\nYou can create an initial `DataStream` by adding a source in a Flink program.\nThen you can derive new streams from this and combine them by using API methods\nsuch as `map`, `filter`, and so on.\n\n## Anatomy of a Flink Program  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#anatomy-of-a-flink-program)\n\nFlink programs look like regular programs that transform `DataStreams`. Each\nprogram consists of the same basic parts:\n\n1. Obtain an `execution environment`,\n2. Load/create the initial data,\n3. Specify transformations on this data,\n4. Specify where to put the results of your computations,\n5. Trigger the program execution\n\n> All Flink Scala APIs are deprecated and will be removed in a future Flink version. You can still build your application in Scala, but you should move to the Java version of either the DataStream and/or Table API.\n>\n> See [FLIP-265 Deprecate and remove Scala API support](https://cwiki.apache.org/confluence/display/FLINK/FLIP-265+Deprecate+and+remove+Scala+API+support)\n\nJava\n\nWe will now give an overview of each of those steps, please refer to the\nrespective sections for more details. Note that all core classes of the Java\nDataStream API can be found in\n[org.apache.flink.streaming.api](https://github.com/apache/flink/blob/release-1.20//flink-streaming-java/src/main/java/org/apache/flink/streaming/api)\n.\n\nThe `StreamExecutionEnvironment` is the basis for all Flink programs. You can\nobtain one using these static methods on `StreamExecutionEnvironment`:\n\n```java\ngetExecutionEnvironment();\n\ncreateLocalEnvironment();\n\ncreateRemoteEnvironment(String host, int port, String... jarFiles);\n\n```\n\nTypically, you only need to use `getExecutionEnvironment()`, since this will do\nthe right thing depending on the context: if you are executing your program\ninside an IDE or as a regular Java program it will create a local environment\nthat will execute your program on your local machine. If you created a JAR file\nfrom your program, and invoke it through the [command line](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/cli/), the Flink cluster manager will execute your main method and\n`getExecutionEnvironment()` will return an execution environment for executing\nyour program on a cluster.\n\nFor specifying data sources the execution environment has several methods to\nread from files using various methods: you can just read them line by line, as\nCSV files, or using any of the other provided sources. To just read a text file\nas a sequence of lines, you can use:\n\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nDataStream<String> text = env.readTextFile(\"file:///path/to/file\");\n\n```\n\nThis will give you a DataStream on which you can then apply transformations to create new\nderived DataStreams.\n\nYou apply transformations by calling methods on DataStream with a\ntransformation functions. For example, a map transformation looks like this:\n\n```java\nDataStream<String> input = ...;\n\nDataStream<Integer> parsed = input.map(new MapFunction<String, Integer>() {\n    @Override\n    public Integer map(String value) {\n        return Integer.parseInt(value);\n    }\n});\n\n```\n\nThis will create a new DataStream by converting every String in the original\ncollection to an Integer.\n\nOnce you have a DataStream containing your final results, you can write it to\nan outside system by creating a sink. These are just some example methods for\ncreating a sink:\n\n```java\nwriteAsText(String path);\n\nprint();\n\n```\n\nScala\n\nWe will now give an overview of each of those steps, please refer to the\nrespective sections for more details. Note that all core classes of the Scala\nDataStream API can be found in\n[org.apache.flink.streaming.api.scala](https://github.com/apache/flink/blob/release-1.20//flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala)\n.\n\nThe `StreamExecutionEnvironment` is the basis for all Flink programs. You can\nobtain one using these static methods on `StreamExecutionEnvironment`:\n\n```scala\ngetExecutionEnvironment()\n\ncreateLocalEnvironment()\n\ncreateRemoteEnvironment(host: String, port: Int, jarFiles: String*)\n\n```\n\nTypically, you only need to use `getExecutionEnvironment()`, since this will do\nthe right thing depending on the context: if you are executing your program\ninside an IDE or as a regular Java program it will create a local environment\nthat will execute your program on your local machine. If you created a JAR file\nfrom your program, and invoke it through the [command line](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/cli/), the Flink cluster manager will execute your main method and\n`getExecutionEnvironment()` will return an execution environment for executing\nyour program on a cluster.\n\nFor specifying data sources the execution environment has several methods to\nread from files using various methods: you can just read them line by line, as\nCSV files, or using any of the other provided sources. To just read a text file\nas a sequence of lines, you can use:\n\n```scala\nval env = StreamExecutionEnvironment.getExecutionEnvironment()\n\nval text: DataStream[String] = env.readTextFile(\"file:///path/to/file\")\n\n```\n\nThis will give you a DataStream on which you can then apply transformations to\ncreate new derived DataStreams.\n\nYou apply transformations by calling methods on DataStream with a\ntransformation functions. For example, a map transformation looks like this:\n\n```scala\nval input: DataSet[String] = ...\n\nval mapped = input.map { x => x.toInt }\n\n```\n\nThis will create a new DataStream by converting every String in the original\ncollection to an Integer.\n\nOnce you have a DataStream containing your final results, you can write it to\nan outside system by creating a sink. These are just some example methods for\ncreating a sink:\n\n```scala\nwriteAsText(path: String)\n\nprint()\n\n```\n\nOnce you specified the complete program you need to **trigger the program**\n**execution** by calling `execute()` on the `StreamExecutionEnvironment`.\nDepending on the type of the `ExecutionEnvironment` the execution will be\ntriggered on your local machine or submit your program for execution on a\ncluster.\n\nThe `execute()` method will wait for the job to finish and then return a\n`JobExecutionResult`, this contains execution times and accumulator results.\n\nIf you don’t want to wait for the job to finish, you can trigger asynchronous\njob execution by calling `executeAsync()` on the `StreamExecutionEnvironment`.\nIt will return a `JobClient` with which you can communicate with the job you\njust submitted. For instance, here is how to implement the semantics of\n`execute()` by using `executeAsync()`.\n\n```java\nfinal JobClient jobClient = env.executeAsync();\n\nfinal JobExecutionResult jobExecutionResult = jobClient.getJobExecutionResult().get();\n\n```\n\nThat last part about program execution is crucial to understanding when and how\nFlink operations are executed. All Flink programs are executed lazily: When the\nprogram’s main method is executed, the data loading and transformations do not\nhappen directly. Rather, each operation is created and added to a dataflow\ngraph. The operations are actually executed when the execution is explicitly\ntriggered by an `execute()` call on the execution environment. Whether the\nprogram is executed locally or on a cluster depends on the type of execution\nenvironment.\n\nThe lazy evaluation lets you construct sophisticated programs that Flink\nexecutes as one holistically planned unit.\n\n[Back to top](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#top)\n\n## Example Program  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#example-program)\n\nThe following program is a complete, working example of streaming window word count application, that counts the\nwords coming from a web socket in 5 second windows. You can copy & paste the code to run it locally.\n\nJava\n\n```java\nimport org.apache.flink.api.common.functions.FlatMapFunction;\nimport org.apache.flink.api.java.tuple.Tuple2;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.api.windowing.time.Time;\nimport org.apache.flink.util.Collector;\n\npublic class WindowWordCount {\n\n    public static void main(String[] args) throws Exception {\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        DataStream<Tuple2<String, Integer>> dataStream = env\n                .socketTextStream(\"localhost\", 9999)\n                .flatMap(new Splitter())\n                .keyBy(value -> value.f0)\n                .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))\n                .sum(1);\n\n        dataStream.print();\n\n        env.execute(\"Window WordCount\");\n    }\n\n    public static class Splitter implements FlatMapFunction<String, Tuple2<String, Integer>> {\n        @Override\n        public void flatMap(String sentence, Collector<Tuple2<String, Integer>> out) throws Exception {\n            for (String word: sentence.split(\" \")) {\n                out.collect(new Tuple2<String, Integer>(word, 1));\n            }\n        }\n    }\n\n}\n\n```\n\nScala\n\n```scala\n\nimport org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.api.windowing.time.Time\n\nobject WindowWordCount {\n  def main(args: Array[String]) {\n\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val text = env.socketTextStream(\"localhost\", 9999)\n\n    val counts = text.flatMap { _.toLowerCase.split(\"\\\\W+\") filter { _.nonEmpty } }\n      .map { (_, 1) }\n      .keyBy(_._1)\n      .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))\n      .sum(1)\n\n    counts.print()\n\n    env.execute(\"Window Stream WordCount\")\n  }\n}\n\n```\n\nTo run the example program, start the input stream with netcat first from a terminal:\n\n```bash\nnc -lk 9999\n\n```\n\nJust type some words hitting return for a new word. These will be the input to the\nword count program. If you want to see counts greater than 1, type the same word again and again within\n5 seconds (increase the window size from 5 seconds if you cannot type that fast ☺).\n\n[Back to top](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#top)\n\n## Data Sources  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#data-sources)\n\nJava\n\nSources are where your program reads its input from. You can attach a source to your program by\nusing `StreamExecutionEnvironment.addSource(sourceFunction)`. Flink comes with a number of pre-implemented\nsource functions, but you can always write your own custom sources by implementing the `SourceFunction`\nfor non-parallel sources, or by implementing the `ParallelSourceFunction` interface or extending the\n`RichParallelSourceFunction` for parallel sources.\n\nThere are several predefined stream sources accessible from the `StreamExecutionEnvironment`:\n\nFile-based:\n\n- `readTextFile(path)` \\- Reads text files, i.e. files that respect the `TextInputFormat` specification, line-by-line and returns them as Strings.\n\n- `readFile(fileInputFormat, path)` \\- Reads (once) files as dictated by the specified file input format.\n\n- `readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)` \\- This is the method called internally by the two previous ones. It reads files in the `path` based on the given `fileInputFormat`. Depending on the provided `watchType`, this source may periodically monitor (every `interval` ms) the path for new data ( `FileProcessingMode.PROCESS_CONTINUOUSLY`), or process once the data currently in the path and exit ( `FileProcessingMode.PROCESS_ONCE`). Using the `pathFilter`, the user can further exclude files from being processed.\n\n_IMPLEMENTATION:_\n\nUnder the hood, Flink splits the file reading process into two sub-tasks, namely _directory monitoring_ and _data reading_. Each of these sub-tasks is implemented by a separate entity. Monitoring is implemented by a single, **non-parallel** (parallelism = 1) task, while reading is performed by multiple tasks running in parallel. The parallelism of the latter is equal to the job parallelism. The role of the single monitoring task is to scan the directory (periodically or only once depending on the `watchType`), find the files to be processed, divide them in _splits_, and assign these splits to the downstream readers. The readers are the ones who will read the actual data. Each split is read by only one reader, while a reader can read multiple splits, one-by-one.\n\n_IMPORTANT NOTES:_\n1. If the `watchType` is set to `FileProcessingMode.PROCESS_CONTINUOUSLY`, when a file is modified, its contents are re-processed entirely. This can break the “exactly-once” semantics, as appending data at the end of a file will lead to **all** its contents being re-processed.\n\n2. If the `watchType` is set to `FileProcessingMode.PROCESS_ONCE`, the source scans the path **once** and exits, without waiting for the readers to finish reading the file contents. Of course the readers will continue reading until all file contents are read. Closing the source leads to no more checkpoints after that point. This may lead to slower recovery after a node failure, as the job will resume reading from the last checkpoint.\n\nSocket-based:\n\n- `socketTextStream` \\- Reads from a socket. Elements can be separated by a delimiter.\n\nCollection-based:\n\n- `fromCollection(Collection)` \\- Creates a data stream from the Java Java.util.Collection. All elements\nin the collection must be of the same type.\n\n- `fromCollection(Iterator, Class)` \\- Creates a data stream from an iterator. The class specifies the\ndata type of the elements returned by the iterator.\n\n- `fromElements(T ...)` \\- Creates a data stream from the given sequence of objects. All objects must be\nof the same type.\n\n- `fromParallelCollection(SplittableIterator, Class)` \\- Creates a data stream from an iterator, in\nparallel. The class specifies the data type of the elements returned by the iterator.\n\n- `fromSequence(from, to)` \\- Generates the sequence of numbers in the given interval, in\nparallel.\n\n\nCustom:\n\n- `addSource` \\- Attach a new source function. For example, to read from Apache Kafka you can use\n`addSource(new FlinkKafkaConsumer<>(...))`. See [connectors](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/overview/) for more details.\n\nScala\n\nSources are where your program reads its input from. You can attach a source to your program by\nusing `StreamExecutionEnvironment.addSource(sourceFunction)`. Flink comes with a number of pre-implemented\nsource functions, but you can always write your own custom sources by implementing the `SourceFunction`\nfor non-parallel sources, or by implementing the `ParallelSourceFunction` interface or extending the\n`RichParallelSourceFunction` for parallel sources.\n\nThere are several predefined stream sources accessible from the `StreamExecutionEnvironment`:\n\nFile-based:\n\n- `readTextFile(path)` \\- Reads text files, i.e. files that respect the `TextInputFormat` specification, line-by-line and returns them as Strings.\n\n- `readFile(fileInputFormat, path)` \\- Reads (once) files as dictated by the specified file input format.\n\n- `readFile(fileInputFormat, path, watchType, interval, pathFilter)` \\- This is the method called internally by the two previous ones. It reads files in the `path` based on the given `fileInputFormat`. Depending on the provided `watchType`, this source may periodically monitor (every `interval` ms) the path for new data ( `FileProcessingMode.PROCESS_CONTINUOUSLY`), or process once the data currently in the path and exit ( `FileProcessingMode.PROCESS_ONCE`). Using the `pathFilter`, the user can further exclude files from being processed.\n\n_IMPLEMENTATION:_\n\nUnder the hood, Flink splits the file reading process into two sub-tasks, namely _directory monitoring_ and _data reading_. Each of these sub-tasks is implemented by a separate entity. Monitoring is implemented by a single, **non-parallel** (parallelism = 1) task, while reading is performed by multiple tasks running in parallel. The parallelism of the latter is equal to the job parallelism. The role of the single monitoring task is to scan the directory (periodically or only once depending on the `watchType`), find the files to be processed, divide them in _splits_, and assign these splits to the downstream readers. The readers are the ones who will read the actual data. Each split is read by only one reader, while a reader can read multiple splits, one-by-one.\n\n_IMPORTANT NOTES:_\n1. If the `watchType` is set to `FileProcessingMode.PROCESS_CONTINUOUSLY`, when a file is modified, its contents are re-processed entirely. This can break the “exactly-once” semantics, as appending data at the end of a file will lead to **all** its contents being re-processed.\n\n2. If the `watchType` is set to `FileProcessingMode.PROCESS_ONCE`, the source scans the path **once** and exits, without waiting for the readers to finish reading the file contents. Of course the readers will continue reading until all file contents are read. Closing the source leads to no more checkpoints after that point. This may lead to slower recovery after a node failure, as the job will resume reading from the last checkpoint.\n\nSocket-based:\n\n- `socketTextStream` \\- Reads from a socket. Elements can be separated by a delimiter.\n\nCollection-based:\n\n- `fromCollection(Seq)` \\- Creates a data stream from the Java Java.util.Collection. All elements\nin the collection must be of the same type.\n\n- `fromCollection(Iterator)` \\- Creates a data stream from an iterator. The class specifies the\ndata type of the elements returned by the iterator.\n\n- `fromElements(elements: _*)` \\- Creates a data stream from the given sequence of objects. All objects must be\nof the same type.\n\n- `fromParallelCollection(SplittableIterator)` \\- Creates a data stream from an iterator, in\nparallel. The class specifies the data type of the elements returned by the iterator.\n\n- `fromSequence(from, to)` \\- Generates the sequence of numbers in the given interval, in\nparallel.\n\n\nCustom:\n\n- `addSource` \\- Attach a new source function. For example, to read from Apache Kafka you can use\n`addSource(new FlinkKafkaConsumer<>(...))`. See [connectors](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/overview/) for more details.\n\n[Back to top](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#top)\n\n## DataStream Transformations  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#datastream-transformations)\n\nPlease see [operators](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/overview/) for an overview of the available stream transformations.\n\n[Back to top](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#top)\n\n## Data Sinks  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#data-sinks)\n\nJava\n\nData sinks consume DataStreams and forward them to files, sockets, external systems, or print them.\nFlink comes with a variety of built-in output formats that are encapsulated behind operations on the\nDataStreams:\n\n- `writeAsText()` / `TextOutputFormat` \\- Writes elements line-wise as Strings. The Strings are\nobtained by calling the _toString()_ method of each element.\n\n- `writeAsCsv(...)` / `CsvOutputFormat` \\- Writes tuples as comma-separated value files. Row and field\ndelimiters are configurable. The value for each field comes from the _toString()_ method of the objects.\n\n- `print()` / `printToErr()` \\- Prints the _toString()_ value\nof each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is\nprepended to the output. This can help to distinguish between different calls to _print_. If the parallelism is\ngreater than 1, the output will also be prepended with the identifier of the task which produced the output.\n\n- `writeUsingOutputFormat()` / `FileOutputFormat` \\- Method and base class for custom file outputs. Supports\ncustom object-to-bytes conversion.\n\n- `writeToSocket` \\- Writes elements to a socket according to a `SerializationSchema`\n\n- `addSink` \\- Invokes a custom sink function. Flink comes bundled with connectors to other systems (such as\nApache Kafka) that are implemented as sink functions.\n\n\nScala\n\nData sinks consume DataStreams and forward them to files, sockets, external systems, or print them.\nFlink comes with a variety of built-in output formats that are encapsulated behind operations on the\nDataStreams:\n\n- `writeAsText()` / `TextOutputFormat` \\- Writes elements line-wise as Strings. The Strings are\nobtained by calling the _toString()_ method of each element.\n\n- `writeAsCsv(...)` / `CsvOutputFormat` \\- Writes tuples as comma-separated value files. Row and field\ndelimiters are configurable. The value for each field comes from the _toString()_ method of the objects.\n\n- `print()` / `printToErr()` \\- Prints the _toString()_ value\nof each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is\nprepended to the output. This can help to distinguish between different calls to _print_. If the parallelism is\ngreater than 1, the output will also be prepended with the identifier of the task which produced the output.\n\n- `writeUsingOutputFormat()` / `FileOutputFormat` \\- Method and base class for custom file outputs. Supports\ncustom object-to-bytes conversion.\n\n- `writeToSocket` \\- Writes elements to a socket according to a `SerializationSchema`\n\n- `addSink` \\- Invokes a custom sink function. Flink comes bundled with connectors to other systems (such as\nApache Kafka) that are implemented as sink functions.\n\n\nNote that the `write*()` methods on `DataStream` are mainly intended for debugging purposes.\nThey are not participating in Flink’s checkpointing, this means these functions usually have\nat-least-once semantics. The data flushing to the target system depends on the implementation of the\nOutputFormat. This means that not all elements send to the OutputFormat are immediately showing up\nin the target system. Also, in failure cases, those records might be lost.\n\nFor reliable, exactly-once delivery of a stream into a file system, use the `FileSink`.\nAlso, custom implementations through the `.addSink(...)` method can participate in Flink’s checkpointing\nfor exactly-once semantics.\n\n[Back to top](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#top)\n\n## Execution Parameters  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#execution-parameters)\n\nThe `StreamExecutionEnvironment` contains the `ExecutionConfig` which allows to set job specific configuration values for the runtime.\n\nPlease refer to [execution configuration](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/)\nfor an explanation of most parameters. These parameters pertain specifically to the DataStream API:\n\n- `setAutoWatermarkInterval(long milliseconds)`: Set the interval for automatic watermark emission. You can\nget the current value with `long getAutoWatermarkInterval()`\n\n[Back to top](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#top)\n\n### Fault Tolerance  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#fault-tolerance)\n\n[State & Checkpointing](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/checkpointing/) describes how to enable and configure Flink’s checkpointing mechanism.\n\n### Controlling Latency  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#controlling-latency)\n\nBy default, elements are not transferred on the network one-by-one (which would cause unnecessary network traffic)\nbut are buffered. The size of the buffers (which are actually transferred between machines) can be set in the Flink config files.\nWhile this method is good for optimizing throughput, it can cause latency issues when the incoming stream is not fast enough.\nTo control throughput and latency, you can use `env.setBufferTimeout(timeoutMillis)` on the execution environment\n(or on individual operators) to set a maximum wait time for the buffers to fill up. After this time, the\nbuffers are sent automatically even if they are not full. The default value for this timeout is 100 ms.\n\nUsage:\n\nJava\n\n```java\nLocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();\nenv.setBufferTimeout(timeoutMillis);\n\nenv.generateSequence(1,10).map(new MyMapper()).setBufferTimeout(timeoutMillis);\n\n```\n\nScala\n\n```scala\nval env: LocalStreamEnvironment = StreamExecutionEnvironment.createLocalEnvironment\nenv.setBufferTimeout(timeoutMillis)\n\nenv.generateSequence(1,10).map(myMap).setBufferTimeout(timeoutMillis)\n\n```\n\nTo maximize throughput, set `setBufferTimeout(-1)` which will remove the timeout and buffers will only be\nflushed when they are full. To minimize latency, set the timeout to a value close to 0 (for example 5 or 10 ms).\nA buffer timeout of 0 should be avoided, because it can cause severe performance degradation.\n\n[Back to top](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#top)\n\n## Debugging  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#debugging)\n\nBefore running a streaming program in a distributed cluster, it is a good\nidea to make sure that the implemented algorithm works as desired. Hence, implementing data analysis\nprograms is usually an incremental process of checking results, debugging, and improving.\n\nFlink provides features to significantly ease the development process of data analysis\nprograms by supporting local debugging from within an IDE, injection of test data, and collection of\nresult data. This section give some hints how to ease the development of Flink programs.\n\n### Local Execution Environment  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#local-execution-environment)\n\nA `LocalStreamEnvironment` starts a Flink system within the same JVM process it was created in. If you\nstart the LocalEnvironment from an IDE, you can set breakpoints in your code and easily debug your\nprogram.\n\nA LocalEnvironment is created and used as follows:\n\nJava\n\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();\n\nDataStream<String> lines = env.addSource(/* some source */);\n// build your program\n\nenv.execute();\n\n```\n\nScala\n\n```scala\nval env = StreamExecutionEnvironment.createLocalEnvironment()\n\nval lines = env.addSource(/* some source */)\n// build your program\n\nenv.execute()\n\n```\n\n### Collection Data Sources  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#collection-data-sources)\n\nFlink provides special data sources which are backed\nby Java collections to ease testing. Once a program has been tested, the sources and sinks can be\neasily replaced by sources and sinks that read from / write to external systems.\n\nCollection data sources can be used as follows:\n\nJava\n\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();\n\n// Create a DataStream from a list of elements\nDataStream<Integer> myInts = env.fromElements(1, 2, 3, 4, 5);\n\n// Create a DataStream from any Java collection\nList<Tuple2<String, Integer>> data = ...\nDataStream<Tuple2<String, Integer>> myTuples = env.fromCollection(data);\n\n// Create a DataStream from an Iterator\nIterator<Long> longIt = ...;\nDataStream<Long> myLongs = env.fromCollection(longIt, Long.class);\n\n```\n\nScala\n\n```scala\nval env = StreamExecutionEnvironment.createLocalEnvironment()\n\n// Create a DataStream from a list of elements\nval myInts = env.fromElements(1, 2, 3, 4, 5)\n\n// Create a DataStream from any Collection\nval data: Seq[(String, Int)] = ...\nval myTuples = env.fromCollection(data)\n\n// Create a DataStream from an Iterator\nval longIt: Iterator[Long] = ...\nval myLongs = env.fromCollection(longIt)\n\n```\n\n**Note:** Currently, the collection data source requires that data types and iterators implement\n`Serializable`. Furthermore, collection data sources can not be executed in parallel (\nparallelism = 1).\n\n### Iterator Data Sink  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#iterator-data-sink)\n\nFlink also provides a sink to collect DataStream results for testing and debugging purposes. It can be used as follows:\n\nJava\n\n```java\nDataStream<Tuple2<String, Integer>> myResult = ...;\nIterator<Tuple2<String, Integer>> myOutput = myResult.collectAsync();\n\n```\n\nScala\n\n```scala\nval myResult: DataStream[(String, Int)] = ...\nval myOutput: Iterator[(String, Int)] = myResult.collectAsync()\n\n```\n\n## Where to go next?  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/\\#where-to-go-next)\n\n- [Operators](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/overview/): Specification of available streaming operators.\n- [Event Time](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/time/): Introduction to Flink’s notion of time.\n- [State & Fault Tolerance](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/state/): Explanation of how to develop stateful applications.\n- [Connectors](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/overview/): Description of available input and output connectors.\n\n[Back to top](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#top)\n\n[Want to contribute translation?](https://cwiki.apache.org/confluence/display/FLINK/Flink+Translation+Specifications)\n\n[Edit This Page](https://github.com/apache/flink/edit/release-1.20/docs/content/docs/dev/datastream/overview.md)\n\n### On This Page\n\n- [Flink DataStream API Programming Guide](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#flink-datastream-api-programming-guide)\n  - [What is a DataStream?](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#what-is-a-datastream)\n  - [Anatomy of a Flink Program](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#anatomy-of-a-flink-program)\n  - [Example Program](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#example-program)\n  - [Data Sources](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#data-sources)\n  - [DataStream Transformations](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#datastream-transformations)\n  - [Data Sinks](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#data-sinks)\n  - [Execution Parameters](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#execution-parameters)\n    - [Fault Tolerance](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#fault-tolerance)\n    - [Controlling Latency](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#controlling-latency)\n  - [Debugging](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#debugging)\n    - [Local Execution Environment](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#local-execution-environment)\n    - [Collection Data Sources](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#collection-data-sources)\n    - [Iterator Data Sink](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#iterator-data-sink)\n  - [Where to go next?](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/#where-to-go-next)","metadata":{"generator":"Hugo 0.110.0","favicon":"https://nightlies.apache.org/flink/flink-docs-release-1.20/favicon.png","language":"en","ogUrl":"//nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/","article:section":"docs","ogTitle":"Overview","description":"Flink DataStream API Programming Guide # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs.","ogDescription":"Flink DataStream API Programming Guide # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs.","og:description":"Flink DataStream API Programming Guide # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs.","title":"Overview | Apache Flink","og:type":"article","theme-color":"#FFFFFF","viewport":"width=device-width, initial-scale=1.0","og:url":"//nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/","og:title":"Overview","scrapeId":"1ca594b9-11ed-4263-9b27-60568a130e3e","sourceURL":"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/","url":"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/","statusCode":200,"contentType":"text/html","proxyUsed":"basic","cacheState":"hit","cachedAt":"2025-10-20T18:32:34.204Z","creditsUsed":1}}}