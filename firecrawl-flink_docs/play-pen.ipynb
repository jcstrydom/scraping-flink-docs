{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa795b2",
   "metadata": {},
   "source": [
    "# FireCrawl playpen\n",
    "\n",
    "This is a simple notebook to discover what the response of `Firecrawl`'s response object looks like...\n",
    "\n",
    "The documentation takes time... and I got a bit unpatient... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b7972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import Firecrawl\n",
    "import dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(\"firecrawl-flink_docs/.env\"))\n",
    "firecrawl = Firecrawl(api_key=os.getenv('FIRECRAWL_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8192396c",
   "metadata": {},
   "source": [
    "## /crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdec1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting crawl...\n",
      "\n",
      " Crawl finished...\n",
      "\n",
      " Crawl response:\n",
      "{'status': 'completed', 'total': 0, 'completed': 0, 'credits_used': 0, 'expires_at': datetime.datetime(2026, 1, 4, 11, 24, 44, tzinfo=TzInfo(0)), 'next': None, 'data': []}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Starting crawl...\")\n",
    "\n",
    "# Crawl with scrape options\n",
    "response = firecrawl.crawl('https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
    "    limit=3,\n",
    "    scrape_options={\n",
    "        \"maxDepth\": 1,\n",
    "        \"render\": False,\n",
    "        \"ignoreRobotsTxt\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Crawl finished...\")\n",
    "\n",
    "print(\"\\n Crawl response:\")\n",
    "print(response.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deda823",
   "metadata": {},
   "source": [
    "The above shows that the crawl does not really work. I suspect it has to do with the `robots.txt` restriction on flinks docs... Not sure why that is restricted..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13291fa2",
   "metadata": {},
   "source": [
    "## /scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9461c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting scrape...\n",
      "\n",
      " Scrape finished...\n",
      "\n",
      " Writing to file...\n",
      "\n",
      " Scrape response:\n",
      "# Concepts  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Starting scrape...\")\n",
    "\n",
    "# Crawl with scrape options\n",
    "response = firecrawl.scrape(\n",
    "    url='https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
    "    wait_for=2000,\n",
    "    only_main_content=True,\n",
    "    formats=['markdown'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Scrape finished...\")\n",
    "\n",
    "print('\\n Writing to file...')\n",
    "with open(\"./flink_firecrawl_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.model_dump()['markdown'])\n",
    "\n",
    "print(\"\\n Scrape response:\")\n",
    "print(response.model_dump()['markdown'][:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde48fa",
   "metadata": {},
   "source": [
    "This prints the markdown content of the scraped page. I.e. it works!!! YES!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eec95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_markdown_links(text):\n",
    "    \"\"\"\n",
    "    Extract markdown links (text, url) from `text`, excluding image links like ![alt](url).\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'(?<!\\!)\\[(?P<text>[^\\]]+)\\]\\((?P<url>https?://[^\\s)]+)\\)')\n",
    "    ret_list = [(m.group('text'), m.group('url')) for m in pattern.finditer(text) if not '\\\\#' in m.group('url')]\n",
    "\n",
    "    ## Clean return list descriptions\n",
    "    ret_list = [ (re.sub(r'\\s+', ' ', desc).strip().replace('\\\\',''), url) for desc, url in ret_list ]\n",
    "\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f40d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hands-on Training',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview/'),\n",
       " ('Data Pipelines & ETL',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl/#stateful-transformations'),\n",
       " ('Fault Tolerance',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance/'),\n",
       " ('Streaming Analytics',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics/'),\n",
       " ('DataStream API',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/'),\n",
       " ('Process Function',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function/'),\n",
       " ('DataStream API',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/'),\n",
       " ('Table API',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/'),\n",
       " ('SQL',\n",
       "  'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/#sql')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = response.model_dump()['markdown']\n",
    "\n",
    "extract_markdown_links(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab8d71",
   "metadata": {},
   "source": [
    "## Datamodel\n",
    "\n",
    "In this part we are describing the data that needs to be saved from the scraping per page.\n",
    "\n",
    "1. Main content into `.md`-file:\n",
    "    1. File name = `<prefix>_<page_id>.md`\n",
    "        1. `<prefix>` = url - `<https://../docs/>`\n",
    "        2. `<page_id>` = hash of `<prefix>`\n",
    "2. Meta-data:\n",
    "    1. page_id: int\n",
    "    2. title: str\n",
    "    3. url: str\n",
    "    4. previous_url: str\n",
    "    5. is_root_url: bool\n",
    "    6. next_urls (a list of tuples for ('link_text','link_url')): list[(str,str)]\n",
    "    7. scrape_timestamp: timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10841178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'markdown': '# Concepts  [\\\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\\\#concepts)\\n\\nThe [Hands-on Training](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview/) explains the basic concepts\\nof stateful and timely stream processing that underlie Flink’s APIs, and provides examples of how these mechanisms are used in applications. Stateful stream processing is introduced in the context of [Data Pipelines & ETL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl/#stateful-transformations)\\nand is further developed in the section on [Fault Tolerance](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance/).\\nTimely stream processing is introduced in the section on [Streaming Analytics](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics/).\\n\\nThis _Concepts in Depth_ section provides a deeper understanding of how Flink’s architecture and runtime implement these concepts.\\n\\n## Flink’s APIs  [\\\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\\\#flinks-apis)\\n\\nFlink offers different levels of abstraction for developing streaming/batch applications.\\n\\n![Programming levels of abstraction](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/levels_of_abstraction.svg)\\n\\n- The lowest level abstraction simply offers **stateful and timely stream processing**. It is\\nembedded into the [DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/) via the [Process\\\\\\\\\\nFunction](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function/). It allows\\nusers to freely process events from one or more streams, and provides consistent, fault tolerant\\n_state_. In addition, users can register event time and processing time callbacks, allowing\\nprograms to realize sophisticated computations.\\n\\n- In practice, many applications do not need the low-level\\nabstractions described above, and can instead program against the **Core APIs**: the\\n[DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/)\\n(bounded/unbounded streams). These fluent APIs offer the\\ncommon building blocks for data processing, like various forms of\\nuser-specified transformations, joins, aggregations, windows, state, etc.\\nData types processed in these APIs are represented as classes in the\\nrespective programming languages.\\n\\nThe low level _Process Function_ integrates with the _DataStream API_,\\nmaking it possible to use the lower-level abstraction on an as-needed basis.\\nThe _DataSet API_ offers additional primitives on bounded data sets,\\nlike loops/iterations.\\n\\n- The **Table API** is a declarative DSL centered around _tables_, which may\\nbe dynamically changing tables (when representing streams). The [Table\\\\\\\\\\nAPI](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/) follows the\\n(extended) relational model: Tables have a schema attached (similar to\\ntables in relational databases) and the API offers comparable operations,\\nsuch as select, project, join, group-by, aggregate, etc. Table API\\nprograms declaratively define _what logical operation should be done_\\nrather than specifying exactly _how the code for the operation looks_.\\nThough the Table API is extensible by various types of user-defined\\nfunctions, it is less expressive than the _Core APIs_, and more concise to\\nuse (less code to write). In addition, Table API programs also go through\\nan optimizer that applies optimization rules before execution.\\n\\nOne can seamlessly convert between tables and _DataStream_/ _DataSet_,\\nallowing programs to mix the _Table API_ with the _DataStream_ and\\n_DataSet_ APIs.\\n\\n- The highest level abstraction offered by Flink is **SQL**. This abstraction\\nis similar to the _Table API_ both in semantics and expressiveness, but\\nrepresents programs as SQL query expressions. The [SQL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/#sql) abstraction closely interacts with the\\nTable API, and SQL queries can be executed over tables defined in the\\n_Table API_.',\n",
       " 'html': None,\n",
       " 'raw_html': None,\n",
       " 'json': None,\n",
       " 'summary': None,\n",
       " 'metadata': {'title': 'Overview | Apache Flink',\n",
       "  'description': 'Concepts # The Hands-on Training explains the basic concepts of stateful and timely stream processing that underlie Flink’s APIs, and provides examples of how these mechanisms are used in applications. Stateful stream processing is introduced in the context of Data Pipelines & ETL and is further developed in the section on Fault Tolerance. Timely stream processing is introduced in the section on Streaming Analytics.\\nThis Concepts in Depth section provides a deeper understanding of how Flink’s architecture and runtime implement these concepts.',\n",
       "  'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
       "  'language': 'en',\n",
       "  'keywords': None,\n",
       "  'robots': None,\n",
       "  'og_title': 'Overview',\n",
       "  'og_description': 'Concepts # The Hands-on Training explains the basic concepts of stateful and timely stream processing that underlie Flink’s APIs, and provides examples of how these mechanisms are used in applications. Stateful stream processing is introduced in the context of Data Pipelines & ETL and is further developed in the section on Fault Tolerance. Timely stream processing is introduced in the section on Streaming Analytics.\\nThis Concepts in Depth section provides a deeper understanding of how Flink’s architecture and runtime implement these concepts.',\n",
       "  'og_url': '//nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
       "  'og_image': None,\n",
       "  'og_audio': None,\n",
       "  'og_determiner': None,\n",
       "  'og_locale': None,\n",
       "  'og_locale_alternate': None,\n",
       "  'og_site_name': None,\n",
       "  'og_video': None,\n",
       "  'favicon': 'https://nightlies.apache.org/flink/flink-docs-release-1.20/favicon.png',\n",
       "  'dc_terms_created': None,\n",
       "  'dc_date_created': None,\n",
       "  'dc_date': None,\n",
       "  'dc_terms_type': None,\n",
       "  'dc_type': None,\n",
       "  'dc_terms_audience': None,\n",
       "  'dc_terms_subject': None,\n",
       "  'dc_subject': None,\n",
       "  'dc_description': None,\n",
       "  'dc_terms_keywords': None,\n",
       "  'modified_time': None,\n",
       "  'published_time': None,\n",
       "  'article_tag': None,\n",
       "  'article_section': None,\n",
       "  'source_url': 'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
       "  'status_code': 200,\n",
       "  'scrape_id': '019b8f59-6e02-767d-bf46-0690425307de',\n",
       "  'num_pages': None,\n",
       "  'content_type': 'text/html',\n",
       "  'proxy_used': 'basic',\n",
       "  'timezone': 'America/New_York',\n",
       "  'cache_state': 'miss',\n",
       "  'cached_at': None,\n",
       "  'credits_used': 1,\n",
       "  'concurrency_limited': False,\n",
       "  'concurrency_queue_duration_ms': None,\n",
       "  'error': None,\n",
       "  'og:url': '//nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
       "  'og:title': 'Overview',\n",
       "  'viewport': 'width=device-width, initial-scale=1.0',\n",
       "  'generator': 'Hugo 0.110.0',\n",
       "  'og:type': 'article',\n",
       "  'og:description': 'Concepts # The Hands-on Training explains the basic concepts of stateful and timely stream processing that underlie Flink’s APIs, and provides examples of how these mechanisms are used in applications. Stateful stream processing is introduced in the context of Data Pipelines & ETL and is further developed in the section on Fault Tolerance. Timely stream processing is introduced in the section on Streaming Analytics.\\nThis Concepts in Depth section provides a deeper understanding of how Flink’s architecture and runtime implement these concepts.',\n",
       "  'theme-color': '#FFFFFF',\n",
       "  'article:section': 'docs',\n",
       "  'indexId': '2a63f795-1f18-4d10-a6c2-474de4abeab9'},\n",
       " 'links': None,\n",
       " 'images': None,\n",
       " 'screenshot': None,\n",
       " 'actions': None,\n",
       " 'warning': None,\n",
       " 'change_tracking': None,\n",
       " 'branding': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594e014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6456d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2e1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2542ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47824b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
