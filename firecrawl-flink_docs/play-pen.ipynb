{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa795b2",
   "metadata": {},
   "source": [
    "# FireCrawl playpen\n",
    "\n",
    "This is a simple notebook to discover what the response of `Firecrawl`'s response object looks like...\n",
    "\n",
    "The documentation takes time... and I got a bit unpatient... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b7972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import Firecrawl\n",
    "import dotenv, os, re, ast\n",
    "import urllib.parse\n",
    "import hashlib\n",
    "\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(\"firecrawl-flink_docs/.env\"))\n",
    "firecrawl = Firecrawl(api_key=os.getenv('FIRECRAWL_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8192396c",
   "metadata": {},
   "source": [
    "## /crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdec1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting crawl...\n",
      "\n",
      " Crawl finished...\n",
      "\n",
      " Crawl response:\n",
      "{'status': 'completed', 'total': 0, 'completed': 0, 'credits_used': 0, 'expires_at': datetime.datetime(2026, 1, 4, 11, 24, 44, tzinfo=TzInfo(0)), 'next': None, 'data': []}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Starting crawl...\")\n",
    "\n",
    "# Crawl with scrape options\n",
    "response = firecrawl.crawl('https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
    "    limit=3,\n",
    "    scrape_options={\n",
    "        \"maxDepth\": 1,\n",
    "        \"render\": False,\n",
    "        \"ignoreRobotsTxt\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Crawl finished...\")\n",
    "\n",
    "print(\"\\n Crawl response:\")\n",
    "print(response.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deda823",
   "metadata": {},
   "source": [
    "The above shows that the crawl does not really work. I suspect it has to do with the `robots.txt` restriction on flinks docs... Not sure why that is restricted..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13291fa2",
   "metadata": {},
   "source": [
    "## /scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9461c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting scrape...\n",
      "\n",
      " Scrape finished...\n",
      "\n",
      " Writing to file...\n",
      "\n",
      " Scrape response:\n",
      "# Concepts  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Starting scrape...\")\n",
    "\n",
    "# Crawl with scrape options\n",
    "response = firecrawl.scrape(\n",
    "    url='https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
    "    wait_for=2000,\n",
    "    only_main_content=True,\n",
    "    formats=['markdown'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Scrape finished...\")\n",
    "\n",
    "print('\\n Writing to file...')\n",
    "with open(\"./flink_firecrawl_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.model_dump()['markdown'])\n",
    "\n",
    "print(\"\\n Scrape response:\")\n",
    "print(response.model_dump()['markdown'][:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde48fa",
   "metadata": {},
   "source": [
    "This prints the markdown content of the scraped page. I.e. it works!!! YES!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250e294",
   "metadata": {},
   "source": [
    "# Metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eec95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/flink_firecrawl_markdown.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "md_content = '\\n'.join(lines)\n",
    "\n",
    "with open('./data/flink_firecrawl_response_full.txt', 'r', encoding='utf-8') as f:\n",
    "    full_content = f.read()\n",
    "\n",
    "file_response = ast.literal_eval(full_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab8d71",
   "metadata": {},
   "source": [
    "## Datamodel\n",
    "\n",
    "In this part we are describing the data that needs to be saved from the scraping per page.\n",
    "\n",
    "1. Main content into `.md`-file:\n",
    "    1. File name = `<prefix>_<page_id>.md`\n",
    "        1. `<prefix>` = url - `<https://../docs/>`\n",
    "        2. `<page_id>` = hash of `<prefix>`\n",
    "2. Meta-data:\n",
    "    1. page_id: hash\n",
    "    2. title: str\n",
    "    3. url: str\n",
    "    4. parent_url: str\n",
    "    5. is_root_url: bool\n",
    "    6. child_urls (a list of tuples for ('link_text','link_url')): list[(str,str)]\n",
    "    7. scrape_timestamp: timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594e014",
   "metadata": {},
   "source": [
    "Here are some suggestions from Copilot. It requires some work from an LLM (especially in the summary and stubb parts etc - but lets check if we can integrate this into ollama - i.e. not going out to external LLMs).\n",
    "\n",
    "The name of the file is interesting:\n",
    "> Suggested filename (example): overview_019b8f59-6e02.md\n",
    "> (If you prefer canonical-hash, replace the UUID prefix with sha256(canonical_url)[:12].)\n",
    "> Main .md file contents (save exactly as file body; no frontmatter):\n",
    ">\n",
    "\n",
    "Here is the json output:\n",
    "```\n",
    "{\n",
    "\"page_id\": \"sha256:<hex-of-canonical-url>\",\n",
    "\"content_hash\": \"sha256:<hex-of-normalized-markdown>\",\n",
    "\"slug\": \"overview\",\n",
    "\"title\": \"Overview | Apache Flink\",\n",
    "\"url\": \"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\",\n",
    "\"canonical_url\": \"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\",\n",
    "\"scrape_id\": \"019b8f59-6e02-767d-bf46-0690425307de\",\n",
    "\"index_id\": \"2a63f795-1f18-4d10-a6c2-474de4abeab9\",\n",
    "\"status_code\": 200,\n",
    "\"content_type\": \"text/html\",\n",
    "\"language\": \"en\",\n",
    "\"summary\": \"Overview of Flink concepts, APIs, and training resources.\",\n",
    "\"headings\": [{\"level\":1,\"text\":\"Concepts\"},{\"level\":2,\"text\":\"Flinkâ€™s APIs\"}],\n",
    "\"assets\": [{\"original_url\":\"https://nightlies.apache.org/flink/.../fig/levels_of_abstraction.svg\",\"inferred_filename\":\"levels_of_abstraction.svg\",\"content_type\":\"image/svg+xml\"}],\n",
    "\"previous_url\": null,\n",
    "\"next_urls\": [],\n",
    "\"is_root_url\": false,\n",
    "\"scrape_timestamp\": \"2026-01-05T12:00:00Z\",\n",
    "\"cached_at\": null,\n",
    "\"provenance\": \"nightlies.apache.org\",\n",
    "\"notes\": \"content taken from Firecrawl response.model_dump(); consider canonical_url normalization before dedup.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25be70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.metadata import PageMetadata\n",
    "\n",
    "class Tester:\n",
    "\n",
    "    def __init__(self,root_url: str = None):\n",
    "        if root_url:\n",
    "            self.root_url = root_url\n",
    "\n",
    "\n",
    "    def extract_prefix(self, url, remove_start: str = 'https://nightlies.apache.org/', remove_end: str = '/docs/') -> str:\n",
    "        pattern = re.compile(re.escape(remove_start) + r'.*?' + re.escape(remove_end))\n",
    "        rest = pattern.sub('', url, 1)\n",
    "        cleaned = re.sub(r'[^A-Za-z]+', '_', rest).strip('_')\n",
    "        return re.sub(r'_+', '_', cleaned)\n",
    "    \n",
    "    def prefix_to_hash(self, prefix: str, numeric: bool = False):\n",
    "        h = hashlib.sha256(prefix.encode('utf-8')).hexdigest()\n",
    "        return int(h[:16], 16) if numeric else h\n",
    "    \n",
    "    def _normalize_url(self, url: str) -> str:\n",
    "        parts = urllib.parse.urlsplit(url)\n",
    "\n",
    "        # normalize scheme and netloc, remove fragment\n",
    "        scheme = parts.scheme.lower()\n",
    "        netloc = parts.netloc.lower()\n",
    "        if (scheme == 'http' and netloc.endswith(':80')) or (scheme == 'https' and netloc.endswith(':443')):\n",
    "            netloc = netloc.rsplit(':', 1)[0]\n",
    "\n",
    "        normalized = urllib.parse.urlunsplit((scheme, netloc, parts.path or '/','',''))\n",
    "        normalized = normalized.rstrip('/')\n",
    "\n",
    "        return normalized\n",
    "    \n",
    "    \n",
    "    def extract_markdown_links(self, text):\n",
    "        \"\"\"\n",
    "        Extract unique markdown page links (text, url) from `text`, excluding image links.\n",
    "        Fragments (anchors) are removed so multiple section links to the same page yield one entry.\n",
    "        \"\"\"\n",
    "\n",
    "        pattern = re.compile(r'(?<!\\!)\\[(?P<text>[^\\]]+)\\]\\((?P<url>https?://[^\\s)]+)\\)')\n",
    "        seen = set()\n",
    "        ret = []\n",
    "\n",
    "        for m in pattern.finditer(text):\n",
    "            raw_url = m.group('url').replace('\\\\', '')\n",
    "            parts = urllib.parse.urlsplit(raw_url)\n",
    "\n",
    "            # normalize scheme and netloc, remove fragment\n",
    "            normalized = self._normalize_url(raw_url)\n",
    "\n",
    "            if (normalized in seen) or (normalized == self._normalize_url(self.root_url)):\n",
    "                continue\n",
    "            seen.add(normalized)\n",
    "\n",
    "            desc = re.sub(r'\\s+', ' ', m.group('text')).strip()\n",
    "            desc = re.sub(r'[^A-Za-z\\s]+', '', desc)\n",
    "            desc = re.sub(r'\\s+', ' ', desc).strip()\n",
    "            ret.append((desc, normalized))\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def save_markdown_file(self, data: dict, content: str, save_dir: str = './data/markdown_files/'):\n",
    "        file_name = f\"{save_dir}{data.get('prefix')}_{data.get('page_id')}.md\"\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        \n",
    "    def parse_raw_response(self, raw_response: str,parent_url: str = None) -> dict:\n",
    "        data_dict = {}\n",
    "\n",
    "        data_dict['title'] = raw_response['metadata']['title']\n",
    "        data_dict['url'] = raw_response['metadata']['url']\n",
    "        if parent_url:\n",
    "            data_dict['is_root_url'] = False\n",
    "        else:\n",
    "            data_dict['is_root_url'] = True\n",
    "            self.root_url = data_dict['url']\n",
    "        data_dict['parent_url'] = parent_url\n",
    "        \n",
    "        data_dict['prefix'] = self.extract_prefix(data_dict['url'])\n",
    "        data_dict['page_id'] = self.prefix_to_hash(data_dict['prefix'])\n",
    "        data_dict['child_urls'] = self.extract_markdown_links(raw_response['markdown'])\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    def process_response(self, raw_response: dict) -> dict:\n",
    "        data_dict = self.parse_raw_response(raw_response)\n",
    "        metadata = PageMetadata.model_validate(data_dict)\n",
    "        self.save_markdown_file(data_dict, raw_response['markdown'])\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    \n",
    "\n",
    "ts = Tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b6456d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'concepts_overview'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts.extract_prefix(response.model_dump()['metadata']['url'])\n",
    "ts.extract_prefix(file_response['metadata']['url'])\n",
    "# ts.extract_prefix('https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/')\n",
    "# ts.extract_prefix('https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/#keyed-and-non-keyed-operators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3c2e1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< PageMetadata page_id=d699b5373c84d3776703d9c89d472a1ecee196e604219eb74f8e5647e6a4513c,\n",
       "  url=https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview\n",
       "  title=Overview | Apache Flink,\n",
       "  is_root_url=True,\n",
       "  parent_url=None,\n",
       "  child_urls[7]=\n",
       "  -->  Handson Training (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview)\n",
       "  -->  Data Pipelines ETL (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl)\n",
       "  -->  Fault Tolerance (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance)\n",
       "  -->  Streaming Analytics (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics)\n",
       "  -->  DataStream API (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview)\n",
       "  -->  Process Function (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function)\n",
       "  -->  Table API (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview),\n",
       "  scrape_timestamp=2026-01-08 21:33:33.466915 >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.process_response(file_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f2542ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Overview | Apache Flink',\n",
       " 'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
       " 'is_root_url': True,\n",
       " 'parent_url': None,\n",
       " 'prefix': 'concepts_overview',\n",
       " 'page_id': 'd699b5373c84d3776703d9c89d472a1ecee196e604219eb74f8e5647e6a4513c',\n",
       " 'child_urls': [('Handson Training',\n",
       "   'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview'),\n",
       "  ('Data Pipelines ETL',\n",
       "   'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl'),\n",
       "  ('Fault Tolerance',\n",
       "   'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance'),\n",
       "  ('Streaming Analytics',\n",
       "   'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics'),\n",
       "  ('DataStream API',\n",
       "   'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview'),\n",
       "  ('Process Function',\n",
       "   'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function'),\n",
       "  ('Table API',\n",
       "   'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview')]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.parse_raw_response(file_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c47824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "drwxrwxr-x 2 joestry  4096 Jan  6 20:24 \u001b[0m\u001b[01;34mdata\u001b[0m/\n",
      "drwxrwxr-x 2 joestry  4096 Jan  7 20:50 \u001b[01;34mmodels\u001b[0m/\n",
      "-rw-rw-r-- 1 joestry 18213 Jan  8 20:50 play-pen.ipynb\n",
      "-rw-rw-r-- 1 joestry   838 Jan  2 15:46 run-crawling.py\n",
      "-rw-rw-r-- 1 joestry   624 Dec 28 09:57 run-scraping.py\n"
     ]
    }
   ],
   "source": [
    "%ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529be5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
