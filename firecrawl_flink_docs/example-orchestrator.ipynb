{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923d1b24",
   "metadata": {},
   "source": [
    "# /Traverse & Persist with ScrapingOrchestrator\n",
    "\n",
    "Now we have the `ScrapingOrchestrator` class that handles:\n",
    "- ‚úÖ Persist the metadata (SQLite database) and the markdown files\n",
    "- ‚úÖ Traverse the next set of child_urls\n",
    "- ‚úÖ Before scraping the next url first check if that specific page has been scraped\n",
    "\n",
    "### Features:\n",
    "1. **Database Persistence**: Uses SQLAlchemy ORM with SQLite\n",
    "2. **URL Deduplication**: Tracks all scraped URLs to prevent re-scraping\n",
    "3. **Queue Management**: FIFO queue for traversing child URLs\n",
    "4. **Batch Processing**: Scrape single URLs or batch operations\n",
    "5. **Depth Control**: Traverse URLs by depth level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed36d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 20:41:47,434 - models.processdata.ResponseProcessor - DEBUG - Initializing ResponseProcessor\n",
      "2026-01-17 20:41:47,444 - models.orchestrator.ScrapingOrchestrator - INFO - ScrapingOrchestrator initialized\n",
      "2026-01-17 20:41:47,446 - models.orchestrator.ScrapingOrchestrator - ERROR - Failed loading existing scraped URLs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ScrapingOrchestrator initialized\n",
      "Database location: /home/joestry/git-projects/github/scraping-projects/scraping-flink-docs/firecrawl_flink_docs/data/scraping.db\n"
     ]
    }
   ],
   "source": [
    "from models import ScrapingOrchestrator\n",
    "import logging\n",
    "import dotenv, os, pathlib\n",
    "\n",
    "\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(\".env\"))\n",
    "\n",
    "# Initialize the orchestrator\n",
    "root_url = 'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/'\n",
    "api_key = os.getenv('FIRECRAWL_API_KEY')\n",
    "\n",
    "orchestrator = ScrapingOrchestrator(\n",
    "    firecrawl_api_key=api_key,\n",
    "    root_url=root_url,\n",
    "    db_path=None,  # Uses default: ./data/scraping.db\n",
    "    log_level=logging.DEBUG,\n",
    "    ask_ollama=False  # Set to True if Ollama is running\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ScrapingOrchestrator initialized\")\n",
    "print(f\"Database location: {orchestrator.db_manager.db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529be5d",
   "metadata": {},
   "source": [
    "### Usage Examples\n",
    "\n",
    "#### Example 1: Scrape single URL with persistence\n",
    "This scrapes a URL, saves markdown file, and persists metadata to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7552f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 20:41:52,156 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:52,159 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:52,161 - models.orchestrator.ScrapingOrchestrator - INFO - Scraping URL\n",
      "2026-01-17 20:41:53,486 - models.processdata.ResponseProcessor - INFO - parse_raw_response called\n",
      "2026-01-17 20:41:53,487 - models.processdata.ResponseProcessor - DEBUG - parse_raw_response called\n",
      "2026-01-17 20:41:53,489 - models.processdata.ResponseProcessor - DEBUG - content_to_hash called\n",
      "2026-01-17 20:41:53,491 - models.processdata.ResponseProcessor - DEBUG - content_to_hash result\n",
      "2026-01-17 20:41:53,492 - models.processdata.ResponseProcessor - DEBUG - extract_version called\n",
      "2026-01-17 20:41:53,497 - models.processdata.ResponseProcessor - DEBUG - extract_version result\n",
      "2026-01-17 20:41:53,498 - models.processdata.ResponseProcessor - DEBUG - extract_prefix called\n",
      "2026-01-17 20:41:53,501 - models.processdata.ResponseProcessor - DEBUG - extract_prefix result\n",
      "2026-01-17 20:41:53,503 - models.processdata.ResponseProcessor - DEBUG - prefix_to_hash called\n",
      "2026-01-17 20:41:53,505 - models.processdata.ResponseProcessor - DEBUG - prefix_to_hash result\n",
      "2026-01-17 20:41:53,509 - models.processdata.ResponseProcessor - DEBUG - extract_markdown_links found matches\n",
      "2026-01-17 20:41:53,512 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,514 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,518 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,521 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,531 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,534 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,536 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,538 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,541 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,545 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,547 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,550 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,552 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,554 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,557 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,563 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,568 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,570 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,573 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,577 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,579 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,581 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-01-17 20:41:53,584 - models.processdata.ResponseProcessor - DEBUG - extract_markdown_links result\n",
      "2026-01-17 20:41:53,590 - models.processdata.ResponseProcessor - INFO - Saved markdown file\n",
      "2026-01-17 20:41:53,595 - models.orchestrator.ScrapingOrchestrator - ERROR - Failed to scrape and persist URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL already scraped or scrape failed\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Scrape single URL and persist\n",
    "# Note: You may want to use ask_ollama=False for faster testing\n",
    "test_url = 'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/'\n",
    "\n",
    "metadata = orchestrator.scrape_and_persist(test_url)\n",
    "if metadata:\n",
    "    print(f\"\\n‚úÖ Scraped and persisted!\")\n",
    "    print(f\"   Page ID: {metadata.page_id}\")\n",
    "    print(f\"   Title: {metadata.title}\")\n",
    "    print(f\"   Child URLs found: {len(metadata.child_urls)}\")\n",
    "    print(f\"   File saved: ./data/markdown_files/{metadata.prefix}_{metadata.page_id}.md\")\n",
    "else:\n",
    "    print(\"URL already scraped or scrape failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b11fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3be67",
   "metadata": {},
   "source": [
    "#### Example 2: Check if URL has been scraped (deduplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eabfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if URLs have been scraped\n",
    "urls_to_check = [\n",
    "    test_url,  # Should return True (we just scraped it)\n",
    "    'https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/try-flink/datastream/',\n",
    "]\n",
    "\n",
    "for url in urls_to_check:\n",
    "    has_been_scraped = orchestrator.has_been_scraped(url)\n",
    "    status = \"‚úÖ Already scraped\" if has_been_scraped else \"‚ùå Not yet scraped\"\n",
    "    print(f\"{status}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ba1f8",
   "metadata": {},
   "source": [
    "#### Example 3: Queue child URLs for traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If metadata had child URLs, they are automatically added to queue\n",
    "# You can also manually add URLs:\n",
    "\n",
    "sample_child_urls = [\n",
    "    (\"Getting Started\", \"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/try-flink/\"),\n",
    "    (\"DataStream API\", \"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/\"),\n",
    "]\n",
    "\n",
    "orchestrator.add_urls_to_queue(sample_child_urls)\n",
    "print(f\"Queue size: {orchestrator.queue_size()}\")\n",
    "print(f\"Queue contents:\")\n",
    "for i, (text, url) in enumerate(list(orchestrator.url_queue), 1):\n",
    "    print(f\"  {i}. {text}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15691153",
   "metadata": {},
   "source": [
    "#### Example 4: Batch scraping from queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a batch (e.g., first 3 URLs from queue)\n",
    "# Note: Adjust max_urls based on your API quota\n",
    "if orchestrator.queue_size() > 0:\n",
    "    stats = orchestrator.scrape_batch(max_urls=3, stop_on_failure=False)\n",
    "    print(\"\\nüìä Batch Scraping Results:\")\n",
    "    print(f\"   ‚úÖ Scraped: {stats['scraped']}\")\n",
    "    print(f\"   ‚ùå Failed: {stats['failed']}\")\n",
    "    print(f\"   ‚è≠Ô∏è  Skipped (already done): {stats['skipped']}\")\n",
    "    print(f\"   üìã Queue remaining: {stats['queue_remaining']}\")\n",
    "else:\n",
    "    print(\"Queue is empty! Add URLs first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb4ecb",
   "metadata": {},
   "source": [
    "#### Example 5: Full traversal from root (depth-limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to do a full traversal (WARNING: Be careful with API quotas!)\n",
    "# This will traverse all URLs starting from root up to max_depth levels\n",
    "# stats = orchestrator.scrape_from_root(max_depth=2)\n",
    "\n",
    "# For now, let's just show stats\n",
    "stats = orchestrator.get_scraping_stats()\n",
    "print(\"\\nüìà Overall Scraping Statistics:\")\n",
    "print(f\"   Root URL: {stats['root_url']}\")\n",
    "print(f\"   Total scraped URLs: {stats['total_scraped_urls']}\")\n",
    "print(f\"   Failed URLs: {stats['failed_urls']}\")\n",
    "print(f\"   Pending in queue: {stats['queue_pending']}\")\n",
    "print(f\"   Pages in database: {stats['database_pages']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a789e5",
   "metadata": {},
   "source": [
    "#### Example 6: Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all pages from database\n",
    "all_pages = orchestrator.db_manager.get_all_pages()\n",
    "print(f\"\\nüìö All pages in database ({len(all_pages)} total):\")\n",
    "for page in all_pages[:5]:  # Show first 5\n",
    "    print(f\"   - {page.title} ({page.page_id})\")\n",
    "    print(f\"     URL: {page.url}\")\n",
    "    print(f\"     Scraped at: {page.scrape_timestamp}\")\n",
    "    print()\n",
    "\n",
    "# Get pages by version (if multiple versions exist)\n",
    "pages_v120 = orchestrator.db_manager.get_pages_by_version('flink-docs-release-1.20')\n",
    "print(f\"Pages for Flink 1.20: {len(pages_v120)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
