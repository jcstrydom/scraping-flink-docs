> This documentation is for an out-of-date version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/ha/overview/).

# High Availability  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/\#high-availability)

JobManager High Availability (HA) hardens a Flink cluster against JobManager failures.
This feature ensures that a Flink cluster will always continue executing your submitted jobs.

## JobManager High Availability  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/\#jobmanager-high-availability)

The JobManager coordinates every Flink deployment.
It is responsible for both _scheduling_ and _resource management_.

By default, there is a single JobManager instance per Flink cluster.
This creates a _single point of failure_ (SPOF): if the JobManager crashes, no new programs can be submitted and running programs fail.

With JobManager High Availability, you can recover from JobManager failures and thereby eliminate the _SPOF_.
You can configure high availability for every cluster deployment.
See the [list of available high availability services](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/#high-availability-services) for more information.

### How to make a cluster highly available  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/\#how-to-make-a-cluster-highly-available)

The general idea of JobManager High Availability is that there is a _single leading JobManager_ at any time and _multiple standby JobManagers_ to take over leadership in case the leader fails.
This guarantees that there is _no single point of failure_ and programs can make progress as soon as a standby JobManager has taken leadership.

As an example, consider the following setup with three JobManager instances:

![](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/jobmanager_ha_overview.png)

Flink’s [high availability services](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/#high-availability-services) encapsulate the required services to make everything work:

- **Leader election**: Selecting a single leader out of a pool of `n` candidates
- **Service discovery**: Retrieving the address of the current leader
- **State persistence**: Persisting state which is required for the successor to resume the job execution (JobGraphs, user code jars, completed checkpoints)

## High Availability Services  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/\#high-availability-services)

Flink ships with two high availability service implementations:

- [ZooKeeper](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/zookeeper_ha/):
ZooKeeper HA services can be used with every Flink cluster deployment.
They require a running ZooKeeper quorum.

- [Kubernetes](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/kubernetes_ha/):
Kubernetes HA services only work when running on Kubernetes.


## High Availability data lifecycle  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/\#high-availability-data-lifecycle)

In order to recover submitted jobs, Flink persists metadata and the job artifacts.
The HA data will be kept until the respective job either succeeds, is cancelled or fails terminally.
Once this happens, all the HA data, including the metadata stored in the HA services, will be deleted.

## JobResultStore  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/ha/overview/\#jobresultstore)

The JobResultStore is used to archive the final result of a job that reached a globally-terminal
state (i.e. finished, cancelled or failed). The data is stored on a file system (see
[job-result-store.storage-path](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/#job-result-store-storage-path)).
Entries in this store are marked as dirty as long as the corresponding job wasn’t cleaned up properly
(artifacts are found in the job’s subfolder in [high-availability.storageDir](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/#high-availability-storagedir)).

Dirty entries are subject to cleanup, i.e. the corresponding job is either cleaned up by Flink at
the moment or will be picked up for cleanup as part of a recovery. The entries will be deleted as
soon as the cleanup succeeds. Check the JobResultStore configuration parameters under
[HA configuration options](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/#high-availability) for further
details on how to adapt the behavior.