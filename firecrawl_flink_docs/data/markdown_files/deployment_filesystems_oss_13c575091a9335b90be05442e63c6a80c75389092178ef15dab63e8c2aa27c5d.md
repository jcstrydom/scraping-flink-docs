> This documentation is for an out-of-date version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/oss/).

# Aliyun Object Storage Service (OSS)  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/oss/\#aliyun-object-storage-service-oss)

## OSS: Object Storage Service  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/oss/\#oss-object-storage-service)

[Aliyun Object Storage Service](https://www.aliyun.com/product/oss) (Aliyun OSS) is widely used, particularly popular among China’s cloud users, and it provides cloud object storage for a variety of use cases.
You can use OSS with Flink for **reading** and **writing data** as well in conjunction with the [streaming **state backends**](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/state_backends/)

You can use OSS objects like regular files by specifying paths in the following format:

```plain
oss://<your-bucket>/<object-name>
```

Below shows how to use OSS in a Flink job:

```java
// Read from OSS bucket
env.readTextFile("oss://<your-bucket>/<object-name>");

// Write to OSS bucket
stream.writeAsText("oss://<your-bucket>/<object-name>");

// Use OSS as checkpoint storage
Configuration config = new Configuration();
config.set(CheckpointingOptions.CHECKPOINT_STORAGE, "filesystem");
config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, "oss://<your-bucket>/<object-name>");
env.configure(config);
```

### Shaded Hadoop OSS file system  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/oss/\#shaded-hadoop-oss-file-system)

To use `flink-oss-fs-hadoop`, copy the respective JAR file from the `opt` directory to a directory in `plugins` directory of your Flink distribution before starting Flink, e.g.

```bash
mkdir ./plugins/oss-fs-hadoop
cp ./opt/flink-oss-fs-hadoop-1.20.3.jar ./plugins/oss-fs-hadoop/
```

`flink-oss-fs-hadoop` registers default FileSystem wrappers for URIs with the _oss://_ scheme.

#### Configurations setup  [\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/filesystems/oss/\#configurations-setup)

After setting up the OSS FileSystem wrapper, you need to add some configurations to make sure that Flink is allowed to access your OSS buckets.

To allow for easy adoption, you can use the same configuration keys in [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/#flink-configuration-file) as in Hadoop’s `core-site.xml`

You can see the configuration keys in the [Hadoop OSS documentation](http://hadoop.apache.org/docs/current/hadoop-aliyun/tools/hadoop-aliyun/index.html).

There are some required configurations that must be added to [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/#flink-configuration-file) ( **Other configurations defined in Hadoop OSS documentation are advanced configurations which used by performance tuning**):

```yaml
fs.oss.endpoint: Aliyun OSS endpoint to connect to
fs.oss.accessKeyId: Aliyun access key ID
fs.oss.accessKeySecret: Aliyun access key secret
```

An alternative `CredentialsProvider` can also be configured in the [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/#flink-configuration-file), e.g.

```yaml
# Read Credentials from OSS_ACCESS_KEY_ID and OSS_ACCESS_KEY_SECRET
fs.oss.credentials.provider: com.aliyun.oss.common.auth.EnvironmentVariableCredentialsProvider
```

Other credential providers can be found under [here](https://github.com/aliyun/aliyun-oss-java-sdk/tree/master/src/main/java/com/aliyun/oss/common/auth).