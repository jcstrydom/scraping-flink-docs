# Amazon S3  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#amazon-s3)

[Amazon Simple Storage Service](http://aws.amazon.com/s3/) (Amazon S3) provides cloud object storage for a variety of use cases. You can use S3 with Flink for **reading** and **writing data** as well in conjunction with the [streaming **state backends**](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/ops/state/state_backends/).

You can use S3 objects like regular files by specifying paths in the following format:

```plain
s3://<your-bucket>/<endpoint>
```

The endpoint can either be a single file or a directory, for example:

```java
// Read from S3 bucket
FileSource<String> fileSource = FileSource.forRecordStreamFormat(
            new TextLineInputFormat(), new Path("s3://<bucket>/<endpoint>")
    ).build();
env.fromSource(
    fileSource,
    WatermarkStrategy.noWatermarks(),
    "s3-input"
);

// Write to S3 bucket
stream.sinkTo(
        FileSink.forRowFormat(
            new Path("s3://<bucket>/<endpoint>"), new SimpleStringEncoder<>()
        ).build()
);

// Use S3 as checkpoint storage
Configuration config = new Configuration();
config.set(CheckpointingOptions.CHECKPOINT_STORAGE, "filesystem");
config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, "s3://<your-bucket>/<endpoint>");
env.configure(config);
```

Note that these examples are _not_ exhaustive and you can use S3 in other places as well, including your [high availability setup](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/ha/overview/) or the [EmbeddedRocksDBStateBackend](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/ops/state/state_backends/#the-rocksdbstatebackend); everywhere that Flink expects a FileSystem URI (unless otherwise stated).

For most use cases, you may use one of our `flink-s3-fs-hadoop` and `flink-s3-fs-presto` S3 filesystem plugins which are self-contained and easy to set up.
For some cases, however, e.g., for using S3 as YARN’s resource storage dir, it may be necessary to set up a specific Hadoop S3 filesystem implementation.

### Hadoop/Presto S3 File Systems plugins  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#hadooppresto-s3-file-systems-plugins)

> You don’t have to configure this manually if you are running [Flink on EMR](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-flink.html).

Flink provides two file systems to talk to Amazon S3, `flink-s3-fs-presto` and `flink-s3-fs-hadoop`.
Both implementations are self-contained with no dependency footprint, so there is no need to add Hadoop to the classpath to use them.

- `flink-s3-fs-presto`, registered under the scheme _s3://_ and _s3p://_, is based on code from the [Presto project](https://prestodb.io/).
You can configure it using [the same configuration keys as the Presto file system](https://prestodb.io/docs/0.272/connector/hive.html#amazon-s3-configuration), by adding the configurations to your [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file). The Presto S3 implementation is the recommended file system for checkpointing to S3.

- `flink-s3-fs-hadoop`, registered under _s3://_ and _s3a://_, based on code from the [Hadoop Project](https://hadoop.apache.org/).
The file system can be [configured using Hadoop’s s3a configuration keys](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#S3A) by adding the configurations to your [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file).

For example, Hadoop has a `fs.s3a.connection.maximum` configuration key. If you want to change it, you need to put `s3.connection.maximum: xyz` to the [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file). Flink will internally translate this back to `fs.s3a.connection.maximum`. There is no need to pass configuration parameters using Hadoop’s XML configuration files.

It is the only S3 file system with support for the [FileSystem](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/connectors/datastream/filesystem/).


Both `flink-s3-fs-hadoop` and `flink-s3-fs-presto` register default FileSystem
wrappers for URIs with the _s3://_ scheme, `flink-s3-fs-hadoop` also registers
for _s3a://_ and `flink-s3-fs-presto` also registers for _s3p://_, so you can
use this to use both at the same time.
For example, the job uses the [FileSystem](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/connectors/datastream/filesystem/) which only supports Hadoop, but uses Presto for checkpointing.
In this case, you should explicitly use _s3a://_ as a scheme for the sink (Hadoop) and _s3p://_ for checkpointing (Presto).

To use `flink-s3-fs-hadoop` or `flink-s3-fs-presto`, copy the respective JAR file from the `opt` directory to the `plugins` directory of your Flink distribution before starting Flink, e.g.

```bash
mkdir ./plugins/s3-fs-presto
cp ./opt/flink-s3-fs-presto-2.2.0.jar ./plugins/s3-fs-presto/
```

#### Configure Access Credentials  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#configure-access-credentials)

After setting up the S3 FileSystem wrapper, you need to make sure that Flink is allowed to access your S3 buckets.

##### Identity and Access Management (IAM) (Recommended)  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#identity-and-access-management-iam-recommended) [Anchor link for: identity and access management iam recommended](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#identity-and-access-management-iam-recommended)

The recommended way of setting up credentials on AWS is via [Identity and Access Management (IAM)](http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html). You can use IAM features to securely give Flink instances the credentials that they need to access S3 buckets. Details about how to do this are beyond the scope of this documentation. Please refer to the AWS user guide. What you are looking for are [IAM Roles](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html).

If you set this up correctly, you can manage access to S3 within AWS and don’t need to distribute any access keys to Flink.

##### Access Keys (Discouraged)  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#access-keys-discouraged) [Anchor link for: access keys discouraged](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#access-keys-discouraged)

Access to S3 can be granted via your **access and secret key pair**. Please note that this is discouraged since the [introduction of IAM roles](https://blogs.aws.amazon.com/security/post/Tx1XG3FX6VMU6O5/A-safer-way-to-distribute-AWS-credentials-to-EC2).

You need to configure both `s3.access-key` and `s3.secret-key` in Flink’s [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file):

```yaml
s3.access-key: your-access-key
s3.secret-key: your-secret-key
```

You can limit this configuration to JobManagers by using [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/security/security-delegation-token/).

```yaml
# flink-s3-fs-hadoop
fs.s3a.aws.credentials.provider: org.apache.flink.fs.s3.common.token.DynamicTemporaryAWSCredentialsProvider
# flink-s3-fs-presto
presto.s3.credentials-provider: org.apache.flink.fs.s3.common.token.DynamicTemporaryAWSCredentialsProvider
```

## Configure Non-S3 Endpoint  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#configure-non-s3-endpoint)

The S3 Filesystems also support using S3 compliant object stores such as [IBM’s Cloud Object Storage](https://www.ibm.com/cloud/object-storage) and [MinIO](https://min.io/).
To do so, configure your endpoint in [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file).

```yaml
s3.endpoint: your-endpoint-hostname
```

## Configure Path Style Access  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#configure-path-style-access)

Some S3 compliant object stores might not have virtual host style addressing enabled by default, for example when using Standalone MinIO for testing purpose. In such cases, you will have to provide the property to enable path style access in [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file).

```yaml
s3.path.style.access: true
```

## Entropy injection for S3 file systems  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#entropy-injection-for-s3-file-systems)

The bundled S3 file systems (`flink-s3-fs-presto` and `flink-s3-fs-hadoop`) support entropy injection. Entropy injection is
a technique to improve the scalability of AWS S3 buckets through adding some random characters near the beginning of the key.

If entropy injection is activated, a configured substring in the path is replaced with random characters. For example, path
`s3://my-bucket/_entropy_/checkpoints/dashboard-job/` would be replaced by something like `s3://my-bucket/gf36ikvg/checkpoints/dashboard-job/`.
**This only happens when the file creation passes the option to inject entropy!**
Otherwise, the file path removes the entropy key substring entirely. See [FileSystem.create(Path, WriteOption)](https://nightlies.apache.org/flink/flink-docs-release-1.6/api/java/org/apache/flink/core/fs/FileSystem.html#create-org.apache.flink.core.fs.Path-org.apache.flink.core.fs.FileSystem.WriteOptions-)
for details.

> The Flink runtime currently passes the option to inject entropy only to checkpoint data files. All other files, including checkpoint metadata and external URI, do not inject entropy to keep checkpoint URIs predictable.

To enable entropy injection, configure the _entropy key_ and the _entropy length_ parameters.

```
s3.entropy.key: _entropy_
s3.entropy.length: 4 (default)
```

The `s3.entropy.key` defines the string in paths that is replaced by the random characters. Paths that do not contain the entropy key are left unchanged.
If a file system operation does not pass the _“inject entropy”_ write option, the entropy key substring is simply removed.
The `s3.entropy.length` defines the number of random alphanumeric characters used for entropy.

## s5cmd  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#s5cmd)

Both `flink-s3-fs-hadoop` and `flink-s3-fs-presto` can be configured to use the [s5cmd tool](https://github.com/peak/s5cmd) for faster file upload and download.
[Benchmark results](https://cwiki.apache.org/confluence/display/FLINK/FLIP-444%3A+Native+file+copy+support) are showing that `s5cmd` can be over 2 times more CPU efficient.
Which means either using half the CPU to upload or download the same set of files, or doing that twice as fast with the same amount of available CPU.

In order to use this feature, the `s5cmd` binary has to be present and accessible to the Flink’s task managers, for example via embedding it in the used docker image.
Secondly, the path to the `s5cmd` has to be configured via:

```yaml
s3.s5cmd.path: /path/to/the/s5cmd
```

The remaining configuration options (with their default value listed below) are:

```yaml
# Extra arguments that will be passed directly to the s5cmd call. Please refer to the s5cmd's official documentation.
s3.s5cmd.args: -r 0
# Maximum size of files that will be uploaded via a single s5cmd call.
s3.s5cmd.batch.max-size: 1024mb
# Maximum number of files that will be uploaded via a single s5cmd call.
s3.s5cmd.batch.max-files: 100
```

Both `s3.s5cmd.batch.max-size` and `s3.s5cmd.batch.max-files` are used to control resource usage of the `s5cmd` binary, to prevent it from overloading the task manager.

It is recommended to first configure and making sure Flink works without using `s5cmd` and only then enabling this feature.

### Credentials  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#credentials)

If you are using [access keys](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/#access-keys-discouraged), they will be passed to the `s5cmd`.
Apart from that `s5cmd` has its own independent (but similar) of Flink way of [using credentials](https://github.com/peak/s5cmd?tab=readme-ov-file#specifying-credentials).

### Limitations  [\#](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/filesystems/s3/\#limitations)

Currently, Flink will use `s5cmd` only during recovery, when downloading state files from S3 and using RocksDB.