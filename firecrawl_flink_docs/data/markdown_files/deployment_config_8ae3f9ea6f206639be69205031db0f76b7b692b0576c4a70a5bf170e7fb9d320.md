> This documentation is for an unreleased version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/config/).

# Configuration  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#configuration)

All configuration can be set in Flink configuration file in the `conf/` directory (see [Flink Configuration File](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#flink-configuration-file)).

The configuration is parsed and evaluated when the Flink processes are started. Changes to the configuration file require restarting the relevant processes.

The out of the box configuration will use your default Java installation. You can manually set the environment variable `JAVA_HOME` or the configuration key `env.java.home` in Flink configuration file if you want to manually override the Java runtime to use. Note that the configuration key `env.java.home` must be specified in a flattened format (i.e. one-line key-value format) in the configuration file.

You can specify a different configuration directory location by defining the `FLINK_CONF_DIR` environment variable. For resource providers which provide non-session deployments, you can specify per-job configurations this way. Make a copy of the `conf` directory from the Flink distribution and modify the settings on a per-job basis. Note that this is not supported in Docker or standalone Kubernetes deployments. On Docker-based deployments, you can use the `FLINK_PROPERTIES` environment variable for passing configuration values.

On session clusters, the provided configuration will only be used for configuring [execution](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#execution) parameters, e.g. configuration parameters affecting the job, not the underlying cluster.

# Flink Configuration File  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#flink-configuration-file)

Starting with Flink version 2.0, Flink only supports the configuration file `config.yaml`, which adheres to the standard YAML 1.2 syntax. The previous `flink-conf.yaml` configuration file is no longer supported. Compared to the previous versions which only supported simple key-value pairs, this update provides users with more flexible and powerful configuration capabilities.

This section will help users understand how to configure the Flink cluster and jobs through the `config.yaml` configuration file, as well as how to migrate old configuration to the new configuration file.

### Usage  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#usage)

The usage for `config.yaml` is as follows:

#### Config Key  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#config-key)

- Users can organize Config Keys in a nested format, such as:

```config.yaml
restart-strategy:
  type: failure-rate
  failure-rate:
    delay: 1 s
    failure-rate-interval: 1 min
    max-failures-per-interval: 1
```

- Users can also organize Config Keys in a flatten format, such as:

```flink-conf.yaml
restart-strategy.type: failure-rate
restart-strategy.failure-rate.delay: 1 s
restart-strategy.failure-rate.failure-rate-interval: 1 min
restart-strategy.failure-rate.max-failures-per-interval: 1
```

#### Config Value  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#config-value)

The `config.yaml` configuration file allows users to configure values following the [YAML 1.2 core schema](https://yaml.org/spec/1.2.2/#103-core-schema).
Users can configure the values corresponding to the Config Type in the following format:

| Config Type | Format Reference |
| --- | --- |
| ##### Integer [Anchor link for: integer](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#integer) | Regular expression: `[-+]? [0-9]+`<br>Example: `100` |
| ##### Long [Anchor link for: long](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#long) | Regular expression: `[-+]? [0-9]+`<br>Example: `100` |
| ##### Float [Anchor link for: float](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#float) | Regular expression: `[-+]? ( \. [0-9]+ | [0-9]+ ( \. [0-9]* )? ) ( [eE] [-+]? [0-9]+ )?`<br>Example: `100.1` |
| ##### Double [Anchor link for: double](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#double) | Regular expression: `[-+]? ( \. [0-9]+ | [0-9]+ ( \. [0-9]* )? ) ( [eE] [-+]? [0-9]+ )?`<br>Example: `100.1` |
| ##### Boolean [Anchor link for: boolean](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#boolean) | Regular expression: `true | True | TRUE | false | False | FALSE`<br>Example: `true` |
| ##### String [Anchor link for: string](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#string) | Any character<br>**Note**: If the value contains special characters used in<br> [YAML](https://yaml.org/spec/1.2.2/#53-indicator-characters),<br> it is necessary to use single or double quotes to escape these characters. |
| ##### Map<String, String> [Anchor link for: map<string string>](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#map%3Cstring-string%3E) | - Flow Style:<br>  - Description: Enclosed with curly braces "{}" and pairs are separated by a comma ",". Within the mapping, key and value are separated by a colon ":" followed by a space.<br>  - Example: `{k1: v1, k2: v2}`<br>- Block Style:<br>  - Description: Uses indentation to represent the hierarchical structure of the data, with keys and values separated by a colon ":" and a space.<br>  - Example:<br>     <br>    <br>    ```<br>    k1: v1<br>    k2: v2<br>    ```<br>- Flink Legacy Map Pattern:<br>  - Description: Pairs are separated by a comma ",". Within the mapping, keys and values are separated by a colon ":".<br>  - Example: `k1:v1,k2:v2`<br> Note: For values containing special characters, consider escaping them; details can be found in the description of [Config Type: String](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#config-type-string). |
| ##### List [Anchor link for: list](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#list) | - Flow Style:<br>  - Description: Enclosed within square brackets "\[\]" and list items are separated by a comma ",".<br>  - Example: `[a, b, c]`<br>- Block Style:<br>  - Description: Uses indentation and a dash "-" to denote list items.<br>  - Example: <br>     <br>    <br>    ```<br>- a<br>- b<br>- c<br>```<br>- Flink Legacy Map Pattern:<br>  - Description: List items are separated by a semicolon ";".<br>  - Example: `a;b;c`<br> Note: For values containing special characters, consider escaping them; details can be found in the description of [Config Type: String](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#config-type-string). |
| ##### MemorySize [Anchor link for: memorysize](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#memorysize) | Regular expression: `[0-9]+ (b | kb | kibibytes | m | mb | mebibytes | g | gb | gibibytes | t | tb | tebibytes)?`<br>Example: `100 mb` |
| ##### Duration [Anchor link for: duration](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#duration) | Regular expression: `[0-9]+ (d | day | h | hour | m | min | minute | s | sec | second | ms | milli | millisecond | us | micro | microsecond | ns | nano | nanosecond)?`<br>Example: `10 s` |
| ##### Enum [Anchor link for: enum](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#enum) | Enum Constants |

Additionally, users can configure the value for all Config Types as strings by simply enclosing the original value in single quotes or double quotes.

### Migrate from flink-conf.yaml to config.yaml  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#migrate-from-flink-confyaml-to-configyaml)

#### Behavior Changes  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#behavior-changes)

`config.yaml` strictly follows the YAML 1.2 syntax and is compatible with `flink-conf.yaml` in most cases, except for the following behavior changes:

- Null value:
  - `flink-conf.yaml`: Only supports leaving the value blank.
  - `config.yaml`: Supports leaving it blank, or explicitly set it to null, Null, NULL, or `~`.
- Comment:
  - `flink-conf.yaml`: Everything after the first occurrence of `#` in each line is considered a comment.
  - `config.yaml`: When there is at least one space between the `#` and the preceding content, or when the `#` is at the beginning of a line, the subsequent content is considered a comment.
- Special character escaping in strings:
  - `flink-conf.yaml`: Only needs to escape elements in Lists and Maps.

    - List elements containing a semicolon “;” require escaping.
    - Map elements containing a comma “,” or colon “:” require escaping.
  - `config.yaml`: Requires escaping special characters as defined in the YAML 1.2 specification; see the definition of [special characters](https://yaml.org/spec/1.2.2/#53-indicator-characters).
- Duplicated keys:
  - `flink-conf.yaml`: Allows duplicated keys and takes the last key-value pair for the corresponding key that appears in the file.
  - `config.yaml`: Does not allow duplicated keys, and an error will be reported when loading the configuration.
- Handling of invalid configuration:
  - `flink-conf.yaml`: Invalid key-value pairs will be ignored.
  - `config.yaml`: An error will be reported when loading the configuration.

#### Migration Tool  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#migration-tool)

To facilitate user migration, Flink provides a configuration file migration script that can automate the migration process. The usage is as follows:

- Place the old configuration file `flink-conf.yaml` in the `conf/` directory.
- Execute the following command in the `$FLINK_HOME/` directory:

```migrate-tool.sh
bin/migrate-config-file.sh
```

After running the command above, the migration script will automatically read the old configuration file `flink-conf.yaml` from the `conf/` directory and output the migrated results to the new configuration file `config.yaml` in the `conf/` directory. Note that due to the limitation of the legacy configuration parser, all values in flink-conf.yaml will be recognized as String type, so the values in the generated config.yaml file will also be of String type, which means some values will be enclosed in quotes. However, Flink will convert them to the actual types defined using `ConfigOption` during subsequent configuration parsing.

# Basic Setup  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#basic-setup)

The default configuration supports starting a single-node Flink session cluster without any changes.
The options in this section are the ones most commonly needed for a basic distributed Flink setup.

**Hostnames / Ports**

These options are only necessary for _standalone_ application- or session deployments ( [simple standalone](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/overview/) or [Kubernetes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes/)).

If you use Flink with [Yarn](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/yarn/) or the [_active_ Kubernetes integration](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/), the hostnames and ports are automatically discovered.

- `rest.address`, `rest.port`: These are used by the client to connect to Flink. Set this to the hostname where the JobManager runs, or to the hostname of the (Kubernetes) service in front of the JobManager’s REST interface.

- The `jobmanager.rpc.address` (defaults to _“localhost”_) and `jobmanager.rpc.port` (defaults to _6123_) config entries are used by the TaskManager to connect to the JobManager/ResourceManager. Set this to the hostname where the JobManager runs, or to the hostname of the (Kubernetes internal) service for the JobManager. This option is ignored on [setups with high-availability](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/ha/overview/) where the leader election mechanism is used to discover this automatically.


**Memory Sizes**

The default memory sizes support simple streaming/batch applications, but are too low to yield good performance for more complex applications.

- `jobmanager.memory.process.size`: Total size of the _JobManager_ (JobMaster / ResourceManager / Dispatcher) process.
- `taskmanager.memory.process.size`: Total size of the TaskManager process.

The total sizes include everything. Flink will subtract some memory for the JVM’s own memory requirements (metaspace and others), and divide and configure the rest automatically between its components (JVM Heap, Off-Heap, for Task Managers also network, managed memory etc.).

These values are configured as memory sizes, for example _1536m_ or _2g_.

**Parallelism**

- `taskmanager.numberOfTaskSlots`: The number of slots that a TaskManager offers _(default: 1)_. Each slot can take one task or pipeline.
Having multiple slots in a TaskManager can help amortize certain constant overheads (of the JVM, application libraries, or network connections) across parallel tasks or pipelines. See the [Task Slots and Resources](https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/flink-architecture/#task-slots-and-resources) concepts section for details.

Running more smaller TaskManagers with one slot each is a good starting point and leads to the best isolation between tasks. Dedicating the same resources to fewer larger TaskManagers with more slots can help to increase resource utilization, at the cost of weaker isolation between the tasks (more tasks share the same JVM).

- `parallelism.default`: The default parallelism used when no parallelism is specified anywhere _(default: 1)_.


**Checkpointing**

You can configure checkpointing directly in code within your Flink job or application. Putting these values here in the configuration defines them as defaults in case the application does not configure anything.

- `state.backend.type`: The state backend to use. This defines the data structure mechanism for taking snapshots. Common values are `hashmap`, `rocksdb` or `forst`.
- `execution.checkpointing.dir`: The directory to write checkpoints to. This takes a path URI like _s3://mybucket/flink-app/checkpoints_ or _hdfs://namenode:port/flink/checkpoints_.
- `execution.checkpointing.savepoint-dir`: The default directory for savepoints. Takes a path URI, similar to `execution.checkpointing.dir`.
- `execution.checkpointing.interval`: The base interval setting. To enable checkpointing, you need to set this value larger than 0.

**Web UI**

- `web.submit.enable`: Enables uploading and starting jobs through the Flink UI _(true by default)_. Please note that even when this is disabled, session clusters still accept jobs through REST requests (HTTP calls). This flag only guards the feature to upload jobs in the UI.
- `web.cancel.enable`: Enables canceling jobs through the Flink UI _(true by default)_. Please note that even when this is disabled, session clusters still cancel jobs through REST requests (HTTP calls). This flag only guards the feature to cancel jobs in the UI.
- `web.upload.dir`: The directory where to store uploaded jobs. Only used when `web.submit.enable` is true.
- `web.exception-history-size`: Sets the size of the exception history that prints the most recent failures that were handled by Flink for a job.

**Other**

- `io.tmp.dirs`: The directories where Flink puts local data, defaults to the system temp directory (`java.io.tmpdir` property). If a list of directories is configured, Flink will rotate files across the directories.

The data put in these directories include by default the files created by RocksDB, spilled intermediate results (batch algorithms), and cached jar files.

This data is NOT relied upon for persistence/recovery, but if this data gets deleted, it typically causes a heavyweight recovery operation. It is hence recommended to set this to a directory that is not automatically periodically purged.

Yarn and Kubernetes setups automatically configure this value to the local working directories by default.


* * *

* * *

# Common Setup Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#common-setup-options)

_Common options to configure your Flink application or cluster._

### Hosts and Ports  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#hosts-and-ports)

Options to configure hostnames and ports for the different Flink components.

The JobManager hostname and port are only relevant for standalone setups without high-availability.
In that setup, the config values are used by the TaskManagers to find (and connect to) the JobManager.
In all highly-available setups, the TaskManagers discover the JobManager via the High-Availability-Service (for example ZooKeeper).

Setups using resource orchestration frameworks (K8s, Yarn) typically use the framework’s service discovery facilities.

You do not need to configure any TaskManager hosts and ports, unless the setup requires the use of specific port ranges or specific network interfaces to bind to.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### jobmanager.bind-host [Anchor link for: jobmanager bind host](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-bind-host) | (none) | String | The local address of the network interface that the job manager binds to. If not configured, '0.0.0.0' will be used. |
| ##### jobmanager.rpc.address [Anchor link for: jobmanager rpc address](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-rpc-address) | (none) | String | The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. |
| ##### jobmanager.rpc.bind-port [Anchor link for: jobmanager rpc bind port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-rpc-bind-port) | (none) | Integer | The local RPC port that the JobManager binds to. If not configured, the external port (configured by 'jobmanager.rpc.port') will be used. |
| ##### jobmanager.rpc.port [Anchor link for: jobmanager rpc port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-rpc-port) | 6123 | Integer | The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. |
| ##### metrics.internal.query-service.port [Anchor link for: metrics internal query service port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-internal-query-service-port) | "0" | String | The port range used for Flink's internal metric query service. Accepts a list of ports (“50100,50101”), ranges(“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port. |
| ##### rest.address [Anchor link for: rest address](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-address) | (none) | String | The address that should be used by clients to connect to the server. Attention: This option is respected only if the high-availability configuration is NONE. |
| ##### rest.bind-address [Anchor link for: rest bind address](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-bind-address) | (none) | String | The address that the server binds itself. |
| ##### rest.bind-port [Anchor link for: rest bind port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-bind-port) | "8081" | String | The port that the server binds itself. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Rest servers are running on the same machine. |
| ##### rest.path [Anchor link for: rest path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-path) | (none) | String | The path that should be used by clients to interact to the server which is accessible via URL. |
| ##### rest.port [Anchor link for: rest port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-port) | 8081 | Integer | The port that the client connects to. If rest.bind-port has not been specified, then the REST server will bind to this port. Attention: This option is respected only if the high-availability configuration is NONE. |
| ##### taskmanager.bind-host [Anchor link for: taskmanager bind host](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-bind-host) | (none) | String | The local address of the network interface that the task manager binds to. If not configured, '0.0.0.0' will be used. |
| ##### taskmanager.collect-sink.port [Anchor link for: taskmanager collect sink port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-collect-sink-port) | 0 | Integer | The port used for the client to retrieve query results from the TaskManager. The default value is 0, which corresponds to a random port assignment. |
| ##### taskmanager.data.bind-port [Anchor link for: taskmanager data bind port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-data-bind-port) | (none) | String | The task manager's bind port used for data exchange operations. Also accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. If not configured, 'taskmanager.data.port' will be used. |
| ##### taskmanager.data.port [Anchor link for: taskmanager data port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-data-port) | 0 | Integer | The task manager’s external port used for data exchange operations. |
| ##### taskmanager.host [Anchor link for: taskmanager host](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-host) | (none) | String | The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file. |
| ##### taskmanager.rpc.bind-port [Anchor link for: taskmanager rpc bind port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-rpc-bind-port) | (none) | Integer | The local RPC port that the TaskManager binds to. If not configured, the external port (configured by 'taskmanager.rpc.port') will be used. |
| ##### taskmanager.rpc.port [Anchor link for: taskmanager rpc port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-rpc-port) | "0" | String | The external RPC port where the TaskManager is exposed. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine. |

### Fault Tolerance  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#fault-tolerance)

These configuration options control Flink’s restart behaviour in case of failures during the execution.
By configuring these options in your `config.yaml`, you define the cluster’s default restart strategy.

The default restart strategy will only take effect if no job specific restart strategy has been configured via the `ExecutionConfig`.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### restart-strategy.type [Anchor link for: restart strategy type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-type) | (none) | String | Defines the restart strategy to use in case of job failures.<br>Accepted values are:<br>- `disable`, `off`, `none`: No restart strategy.<br>- `fixed-delay`, `fixeddelay`: Fixed delay restart strategy. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery#fixed-delay-restart-strategy).<br>- `failure-rate`, `failurerate`: Failure rate restart strategy. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery#failure-rate-restart-strategy).<br>- `exponential-delay`, `exponentialdelay`: Exponential delay restart strategy. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery#exponential-delay-restart-strategy).<br>If checkpointing is disabled, the default value is `disable`. If checkpointing is enabled, the default value is `exponential-delay`, and the default values of `exponential-delay` related config options will be used. |

**Fixed Delay Restart Strategy**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### restart-strategy.fixed-delay.attempts [Anchor link for: restart strategy fixed delay attempts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-fixed-delay-attempts) | 1 | Integer | The number of times that Flink retries the execution before the job is declared as failed if `restart-strategy.type` has been set to `fixed-delay`. |
| ##### restart-strategy.fixed-delay.delay [Anchor link for: restart strategy fixed delay delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-fixed-delay-delay) | 1 s | Duration | Delay between two consecutive restart attempts if `restart-strategy.type` has been set to `fixed-delay`. Delaying the retries can be helpful when the program interacts with external systems where for example connections or pending transactions should reach a timeout before re-execution is attempted. It can be specified using notation: "1 min", "20 s" |

**Exponential Delay Restart Strategy**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### restart-strategy.exponential-delay.attempts-before-reset-backoff [Anchor link for: restart strategy exponential delay attempts before reset backoff](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-exponential-delay-attempts-before-reset-backoff) | infinite | Integer | The number of times that Flink retries the execution before failing the job if `restart-strategy.type` has been set to `exponential-delay`. The number will be reset once the backoff is reset to its initial value. |
| ##### restart-strategy.exponential-delay.backoff-multiplier [Anchor link for: restart strategy exponential delay backoff multiplier](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-exponential-delay-backoff-multiplier) | 1.5 | Double | Backoff value is multiplied by this value after every failure,until max backoff is reached if `restart-strategy.type` has been set to `exponential-delay`. |
| ##### restart-strategy.exponential-delay.initial-backoff [Anchor link for: restart strategy exponential delay initial backoff](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-exponential-delay-initial-backoff) | 1 s | Duration | Starting duration between restarts if `restart-strategy.type` has been set to `exponential-delay`. It can be specified using notation: "1 min", "20 s" |
| ##### restart-strategy.exponential-delay.jitter-factor [Anchor link for: restart strategy exponential delay jitter factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-exponential-delay-jitter-factor) | 0.1 | Double | Jitter specified as a portion of the backoff if `restart-strategy.type` has been set to `exponential-delay`. It represents how large random value will be added or subtracted to the backoff. Useful when you want to avoid restarting multiple jobs at the same time. |
| ##### restart-strategy.exponential-delay.max-backoff [Anchor link for: restart strategy exponential delay max backoff](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-exponential-delay-max-backoff) | 1 min | Duration | The highest possible duration between restarts if `restart-strategy.type` has been set to `exponential-delay`. It can be specified using notation: "1 min", "20 s" |
| ##### restart-strategy.exponential-delay.reset-backoff-threshold [Anchor link for: restart strategy exponential delay reset backoff threshold](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-exponential-delay-reset-backoff-threshold) | 1 h | Duration | Threshold when the backoff is reset to its initial value if `restart-strategy.type` has been set to `exponential-delay`. It specifies how long the job must be running without failure to reset the exponentially increasing backoff to its initial value. It can be specified using notation: "1 min", "20 s" |

**Failure Rate Restart Strategy**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### restart-strategy.failure-rate.delay [Anchor link for: restart strategy failure rate delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-failure-rate-delay) | 1 s | Duration | Delay between two consecutive restart attempts if `restart-strategy.type` has been set to `failure-rate`. It can be specified using notation: "1 min", "20 s" |
| ##### restart-strategy.failure-rate.failure-rate-interval [Anchor link for: restart strategy failure rate failure rate interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-failure-rate-failure-rate-interval) | 1 min | Duration | Time interval for measuring failure rate if `restart-strategy.type` has been set to `failure-rate`. It can be specified using notation: "1 min", "20 s" |
| ##### restart-strategy.failure-rate.max-failures-per-interval [Anchor link for: restart strategy failure rate max failures per interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#restart-strategy-failure-rate-max-failures-per-interval) | 1 | Integer | Maximum number of restarts in given time interval before failing a job if `restart-strategy.type` has been set to `failure-rate`. |

### Retryable Cleanup  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#retryable-cleanup)

After jobs reach a globally-terminal state, a cleanup of all related resources is performed. This cleanup can be retried in case of failure. Different retry strategies can be configured to change this behavior:

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### cleanup-strategy.type [Anchor link for: cleanup strategy type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cleanup-strategy-type) | "exponential-delay" | String | Defines the cleanup strategy to use in case of cleanup failures.<br>Accepted values are:<br>- `none`, `disable`, `off`: Cleanup is only performed once. No retry will be initiated in case of failure. The job artifacts (and the job's JobResultStore entry) have to be cleaned up manually in case of a failure.<br>- `fixed-delay`, `fixeddelay`: Cleanup attempts will be separated by a fixed interval up to the point where the cleanup is considered successful or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.<br>- `exponential-delay`, `exponentialdelay`: Exponential delay restart strategy triggers the cleanup with an exponentially increasing delay up to the point where the cleanup succeeded or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.<br>The default configuration relies on an exponentially delayed retry strategy with the given default values. |

**Fixed-Delay Cleanup Retry Strategy**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### cleanup-strategy.fixed-delay.attempts [Anchor link for: cleanup strategy fixed delay attempts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cleanup-strategy-fixed-delay-attempts) | infinite | Integer | The number of times that Flink retries the cleanup before giving up if `cleanup-strategy.type` has been set to `fixed-delay`. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually. |
| ##### cleanup-strategy.fixed-delay.delay [Anchor link for: cleanup strategy fixed delay delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cleanup-strategy-fixed-delay-delay) | 1 min | Duration | Amount of time that Flink waits before re-triggering the cleanup after a failed attempt if the `cleanup-strategy.type` is set to `fixed-delay`. It can be specified using the following notation: "1 min", "20 s" |

**Exponential-Delay Cleanup Retry Strategy**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### cleanup-strategy.exponential-delay.attempts [Anchor link for: cleanup strategy exponential delay attempts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cleanup-strategy-exponential-delay-attempts) | infinite | Integer | The number of times a failed cleanup is retried if `cleanup-strategy.type` has been set to `exponential-delay`. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually. |
| ##### cleanup-strategy.exponential-delay.initial-backoff [Anchor link for: cleanup strategy exponential delay initial backoff](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cleanup-strategy-exponential-delay-initial-backoff) | 1 s | Duration | Starting duration between cleanup retries if `cleanup-strategy.type` has been set to `exponential-delay`. It can be specified using the following notation: "1 min", "20 s" |
| ##### cleanup-strategy.exponential-delay.max-backoff [Anchor link for: cleanup strategy exponential delay max backoff](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cleanup-strategy-exponential-delay-max-backoff) | 1 h | Duration | The highest possible duration between cleanup retries if `cleanup-strategy.type` has been set to `exponential-delay`. It can be specified using the following notation: "1 min", "20 s" |

### Checkpoints and State Backends  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#checkpoints-and-state-backends)

These options control the basic setup of state backends and checkpointing behavior.

The options are only relevant for jobs/applications executing in a continuous streaming fashion.
Jobs/applications executing in a batch fashion do not use state backends and checkpoints, but different internal data structures that are optimized for batch processing.

**State Backends**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.type [Anchor link for: state backend type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-type) | "hashmap" | String | The state backend to be used to store state.<br>The implementation can be specified either via their shortcut name, or via the class name of a `StateBackendFactory`. If a factory is specified it is instantiated via its zero argument constructor and its `StateBackendFactory#createFromConfig(ReadableConfig, ClassLoader)` method is called.<br>Recognized shortcut names are 'hashmap', 'rocksdb' and 'forst'. |

**Checkpoints**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### execution.checkpointing.storage [Anchor link for: execution checkpointing storage](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-storage) | (none) | String | The checkpoint storage implementation to be used to checkpoint state.<br>The implementation can be specified either via their shortcut name, or via the class name of a `CheckpointStorageFactory`. If a factory is specified it is instantiated via its zero argument constructor and its `CheckpointStorageFactory#createFromConfig(ReadableConfig, ClassLoader)` method is called.<br>Recognized shortcut names are 'jobmanager' and 'filesystem'.<br>'execution.checkpointing.storage' and 'execution.checkpointing.dir' are usually combined to configure the checkpoint location. By default, the checkpoint meta data and actual program state will be stored in the JobManager's memory directly. When 'execution.checkpointing.storage' is set to 'jobmanager', if 'execution.checkpointing.dir' is configured, the meta data of checkpoints will be persisted to the path specified by 'execution.checkpointing.dir'. Otherwise, the meta data will be stored in the JobManager's memory. When 'execution.checkpointing.storage' is set to 'filesystem', a valid path must be configured to 'execution.checkpointing.dir', and the checkpoint meta data and actual program state will both be persisted to the path. |
| ##### execution.checkpointing.dir [Anchor link for: execution checkpointing dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-dir) | (none) | String | The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers). If the 'execution.checkpointing.storage' is set to 'jobmanager', only the meta data of checkpoints will be stored in this directory. |
| ##### execution.checkpointing.savepoint-dir [Anchor link for: execution checkpointing savepoint dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-savepoint-dir) | (none) | String | The default directory for savepoints. Used by the state backends that write savepoints to file systems (HashMapStateBackend, EmbeddedRocksDBStateBackend). |
| ##### execution.checkpointing.cleaner.parallel-mode [Anchor link for: execution checkpointing cleaner parallel mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-cleaner-parallel-mode) | true | Boolean | Option whether to discard a checkpoint's states in parallel using the ExecutorService passed into the cleaner |
| ##### execution.checkpointing.incremental [Anchor link for: execution checkpointing incremental](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-incremental) | false | Boolean | Option whether to create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option. |
| ##### execution.checkpointing.local-backup.dirs [Anchor link for: execution checkpointing local backup dirs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-local-backup-dirs) | (none) | String | The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. If not configured it will default to <WORKING\_DIR>/localState. The <WORKING\_DIR> can be configured via `process.taskmanager.working-dir` |
| ##### execution.checkpointing.local-backup.enabled [Anchor link for: execution checkpointing local backup enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-local-backup-enabled) | false | Boolean | This option configures local backup for the state backend, which indicates whether to make backup checkpoint on local disk. If not configured, fallback to execution.state-recovery.from-local. By default, local backup is deactivated. Local backup currently only covers keyed state backends (including both the EmbeddedRocksDBStateBackend and the HashMapStateBackend). |
| ##### execution.checkpointing.num-retained [Anchor link for: execution checkpointing num retained](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-num-retained) | 1 | Integer | The maximum number of completed checkpoints to retain. |

### High Availability  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability)

High-availability here refers to the ability of the JobManager process to recover from failures.

The JobManager ensures consistency during recovery across TaskManagers. For the JobManager itself to recover consistently, an external service must store a minimal amount of recovery metadata (like “ID of last committed checkpoint”), as well as help to elect and lock which JobManager is the leader (to avoid split-brain situations).

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### high-availability.type [Anchor link for: high availability type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-type) | "NONE" | String | Defines high-availability mode used for cluster execution. To enable high-availability, set this mode to "ZOOKEEPER", "KUBERNETES", or specify the fully qualified name of the factory class. |
| ##### high-availability.cluster-id [Anchor link for: high availability cluster id](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-cluster-id) | "/default" | String | The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be set for standalone clusters but is automatically inferred in YARN. |
| ##### high-availability.storageDir [Anchor link for: high availability storagedir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-storagedir) | (none) | String | File system path (URI) where Flink persists metadata in high-availability setups. |

**Options for the JobResultStore in high-availability setups**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### job-result-store.delete-on-commit [Anchor link for: job result store delete on commit](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#job-result-store-delete-on-commit) | true | Boolean | Determines whether job results should be automatically removed from the underlying job result store when the corresponding entity transitions into a clean state. If false, the cleaned job results are, instead, marked as clean to indicate their state. In this case, Flink no longer has ownership and the resources need to be cleaned up by the user. |
| ##### job-result-store.storage-path [Anchor link for: job result store storage path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#job-result-store-storage-path) | (none) | String | Defines where job results should be stored. This should be an underlying file-system that provides read-after-write consistency. By default, this is `{high-availability.storageDir}/job-result-store/{high-availability.cluster-id}`. |

**Options for high-availability setups with ZooKeeper**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### high-availability.zookeeper.path.root [Anchor link for: high availability zookeeper path root](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-path-root) | "/flink" | String | The root path under which Flink stores its entries in ZooKeeper. |
| ##### high-availability.zookeeper.quorum [Anchor link for: high availability zookeeper quorum](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-quorum) | (none) | String | The ZooKeeper quorum to use, when running Flink in a high-availability mode with ZooKeeper. |

### Memory Configuration  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#memory-configuration)

These configuration values control the way that TaskManagers and JobManagers use memory.

Flink tries to shield users as much as possible from the complexity of configuring the JVM for data-intensive processing.
In most cases, users should only need to set the values `taskmanager.memory.process.size` or `taskmanager.memory.flink.size` (depending on how the setup), and possibly adjusting the ratio of JVM heap and Managed Memory via `taskmanager.memory.managed.fraction`. The other options below can be used for performance tuning and fixing memory related errors.

For a detailed explanation of how these options interact,
see the documentation on [TaskManager](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/) and
[JobManager](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_jobmanager/) memory configurations.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### jobmanager.memory.enable-jvm-direct-memory-limit [Anchor link for: jobmanager memory enable jvm direct memory limit](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-enable-jvm-direct-memory-limit) | false | Boolean | Whether to enable the JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize). The limit will be set to the value of 'jobmanager.memory.off-heap.size' option. |
| ##### jobmanager.memory.flink.size [Anchor link for: jobmanager memory flink size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-flink-size) | (none) | MemorySize | Total Flink Memory size for the JobManager. This includes all the memory that a JobManager consumes, except for JVM Metaspace and JVM Overhead. It consists of JVM Heap Memory and Off-heap Memory. See also 'jobmanager.memory.process.size' for total process memory size configuration. |
| ##### jobmanager.memory.heap.size [Anchor link for: jobmanager memory heap size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-heap-size) | (none) | MemorySize | JVM Heap Memory size for JobManager. The minimum recommended JVM Heap size is 128.000mb (134217728 bytes). |
| ##### jobmanager.memory.jvm-metaspace.size [Anchor link for: jobmanager memory jvm metaspace size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-jvm-metaspace-size) | 256 mb | MemorySize | JVM Metaspace Size for the JobManager. |
| ##### jobmanager.memory.jvm-overhead.fraction [Anchor link for: jobmanager memory jvm overhead fraction](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-jvm-overhead-fraction) | 0.1 | Float | Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. |
| ##### jobmanager.memory.jvm-overhead.max [Anchor link for: jobmanager memory jvm overhead max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-jvm-overhead-max) | 1 gb | MemorySize | Max JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. |
| ##### jobmanager.memory.jvm-overhead.min [Anchor link for: jobmanager memory jvm overhead min](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-jvm-overhead-min) | 192 mb | MemorySize | Min JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. |
| ##### jobmanager.memory.off-heap.size [Anchor link for: jobmanager memory off heap size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-off-heap-size) | 128 mb | MemorySize | Off-heap Memory size for JobManager. This option covers all off-heap memory usage including direct and native memory allocation. The JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize) will be set to this value if the limit is enabled by 'jobmanager.memory.enable-jvm-direct-memory-limit'. |
| ##### jobmanager.memory.process.size [Anchor link for: jobmanager memory process size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-memory-process-size) | (none) | MemorySize | Total Process Memory size for the JobManager. This includes all the memory that a JobManager JVM process consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. In containerized setups, this should be set to the container memory. See also 'jobmanager.memory.flink.size' for Total Flink Memory size configuration. |
| ##### taskmanager.memory.flink.size [Anchor link for: taskmanager memory flink size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-flink-size) | (none) | MemorySize | Total Flink Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, except for JVM Metaspace and JVM Overhead. It consists of Framework Heap Memory, Task Heap Memory, Task Off-Heap Memory, Managed Memory, and Network Memory. See also 'taskmanager.memory.process.size' for total process memory size configuration. |
| ##### taskmanager.memory.framework.heap.size [Anchor link for: taskmanager memory framework heap size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-framework-heap-size) | 128 mb | MemorySize | Framework Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for TaskExecutor framework, which will not be allocated to task slots. |
| ##### taskmanager.memory.framework.off-heap.batch-shuffle.size [Anchor link for: taskmanager memory framework off heap batch shuffle size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-framework-off-heap-batch-shuffle-size) | 64 mb | MemorySize | Size of memory used by batch shuffle for shuffle data read (currently only used by sort-shuffle and hybrid shuffle). Notes: 1) The memory is cut from 'taskmanager.memory.framework.off-heap.size' so must be smaller than that, which means you may also need to increase 'taskmanager.memory.framework.off-heap.size' after you increase this config value; 2) This memory size can influence the shuffle performance and you can increase this config value for large-scale batch jobs (for example, to 128M or 256M). |
| ##### taskmanager.memory.framework.off-heap.size [Anchor link for: taskmanager memory framework off heap size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-framework-off-heap-size) | 128 mb | MemorySize | Framework Off-Heap Memory size for TaskExecutors. This is the size of off-heap memory (JVM direct memory and native memory) reserved for TaskExecutor framework, which will not be allocated to task slots. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter. |
| ##### taskmanager.memory.jvm-metaspace.size [Anchor link for: taskmanager memory jvm metaspace size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-jvm-metaspace-size) | 256 mb | MemorySize | JVM Metaspace Size for the TaskExecutors. |
| ##### taskmanager.memory.jvm-overhead.fraction [Anchor link for: taskmanager memory jvm overhead fraction](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-jvm-overhead-fraction) | 0.1 | Float | Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. |
| ##### taskmanager.memory.jvm-overhead.max [Anchor link for: taskmanager memory jvm overhead max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-jvm-overhead-max) | 1 gb | MemorySize | Max JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. |
| ##### taskmanager.memory.jvm-overhead.min [Anchor link for: taskmanager memory jvm overhead min](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-jvm-overhead-min) | 192 mb | MemorySize | Min JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. |
| ##### taskmanager.memory.managed.consumer-weights [Anchor link for: taskmanager memory managed consumer weights](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-managed-consumer-weights) | OPERATOR:70,STATE\_BACKEND:70,PYTHON:30 | Map | Managed memory weights for different kinds of consumers. A slot’s managed memory is shared by all kinds of consumers it contains, proportionally to the kinds’ weights and regardless of the number of consumers from each kind. Currently supported kinds of consumers are OPERATOR (for built-in algorithms), STATE\_BACKEND (for RocksDB state backend) and PYTHON (for Python processes). |
| ##### taskmanager.memory.managed.fraction [Anchor link for: taskmanager memory managed fraction](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-managed-fraction) | 0.4 | Float | Fraction of Total Flink Memory to be used as Managed Memory, if Managed Memory size is not explicitly specified. |
| ##### taskmanager.memory.managed.size [Anchor link for: taskmanager memory managed size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-managed-size) | (none) | MemorySize | Managed Memory size for TaskExecutors. This is the size of off-heap memory managed by the memory manager, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend. Memory consumers can either allocate memory from the memory manager in the form of MemorySegments, or reserve bytes from the memory manager and keep their memory usage within that boundary. If unspecified, it will be derived to make up the configured fraction of the Total Flink Memory. |
| ##### taskmanager.memory.network.fraction [Anchor link for: taskmanager memory network fraction](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-network-fraction) | 0.1 | Float | Fraction of Total Flink Memory to be used as Network Memory. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max size to the same value. |
| ##### taskmanager.memory.network.max [Anchor link for: taskmanager memory network max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-network-max) | infinite | MemorySize | Max Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. By default, the max limit of Network Memory is Long.MAX\_VALUE. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value. |
| ##### taskmanager.memory.network.min [Anchor link for: taskmanager memory network min](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-network-min) | 64 mb | MemorySize | Min Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value. |
| ##### taskmanager.memory.process.size [Anchor link for: taskmanager memory process size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-process-size) | (none) | MemorySize | Total Process Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. On containerized setups, this should be set to the container memory. See also 'taskmanager.memory.flink.size' for total Flink memory size configuration. |
| ##### taskmanager.memory.task.heap.size [Anchor link for: taskmanager memory task heap size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-task-heap-size) | (none) | MemorySize | Task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory. |
| ##### taskmanager.memory.task.off-heap.size [Anchor link for: taskmanager memory task off heap size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-task-off-heap-size) | 0 bytes | MemorySize | Task Off-Heap Memory size for TaskExecutors. This is the size of off heap memory (JVM direct memory and native memory) reserved for tasks. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter. |

### Miscellaneous Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#miscellaneous-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### fs.allowed-fallback-filesystems [Anchor link for: fs allowed fallback filesystems](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#fs-allowed-fallback-filesystems) | (none) | String | A (semicolon-separated) list of file schemes, for which Hadoop can be used instead of an appropriate Flink plugin. (example: s3;wasb) |
| ##### fs.default-scheme [Anchor link for: fs default scheme](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#fs-default-scheme) | (none) | String | The default filesystem scheme, used for paths that do not declare a scheme explicitly. May contain an authority, e.g. host:port in case of an HDFS NameNode. |
| ##### io.tmp.dirs [Anchor link for: io tmp dirs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#io-tmp-dirs) | 'LOCAL\_DIRS' on Yarn. System.getProperty("java.io.tmpdir") in standalone. | String | Directories for temporary files, separated by",", "\|", or the system's java.io.File.pathSeparator. |
| ##### sink.committer.retries [Anchor link for: sink committer retries](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#sink-committer-retries) | 10 | Integer | The number of retries a Flink application attempts for committable operations (such as transactions) on retriable errors, as specified by the sink connector, before Flink fails and potentially restarts. |

* * *

* * *

# Security  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security)

Options for configuring Flink’s security and secure interaction with external systems.

### SSL  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#ssl)

Flink’s network connections can be secured via SSL. Please refer to the [SSL Setup Docs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/security/security-ssl/) for detailed setup guide and background.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### security.ssl.algorithms [Anchor link for: security ssl algorithms](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-algorithms) | "TLS\_RSA\_WITH\_AES\_128\_CBC\_SHA" | String | The comma separated list of standard SSL algorithms to be supported. Read more [here](http://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#ciphersuites) |
| ##### security.ssl.internal.cert.fingerprint [Anchor link for: security ssl internal cert fingerprint](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-cert-fingerprint) | (none) | String | The sha1 fingerprint of the internal certificate. This further protects the internal communication to present the exact certificate used by Flink.This is necessary where one cannot use private CA(self signed) or there is internal firm wide CA is required |
| ##### security.ssl.internal.enabled [Anchor link for: security ssl internal enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-enabled) | false | Boolean | Turns on SSL for internal network communication. Optionally, specific components may override this through their own settings (rpc, data transport, REST, etc). |
| ##### security.ssl.internal.key-password [Anchor link for: security ssl internal key password](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-key-password) | (none) | String | The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server). |
| ##### security.ssl.internal.keystore [Anchor link for: security ssl internal keystore](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-keystore) | (none) | String | The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server). |
| ##### security.ssl.internal.keystore-password [Anchor link for: security ssl internal keystore password](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-keystore-password) | (none) | String | The secret to decrypt the keystore file for Flink's for Flink's internal endpoints (rpc, data transport, blob server). |
| ##### security.ssl.internal.keystore-type [Anchor link for: security ssl internal keystore type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-keystore-type) | JVM default keystore type | String | The type of keystore for Flink's internal endpoints (rpc, data transport, blob server). |
| ##### security.ssl.internal.truststore [Anchor link for: security ssl internal truststore](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-truststore) | (none) | String | The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server). |
| ##### security.ssl.internal.truststore-password [Anchor link for: security ssl internal truststore password](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-truststore-password) | (none) | String | The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server). |
| ##### security.ssl.internal.truststore-type [Anchor link for: security ssl internal truststore type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-truststore-type) | JVM default keystore type | String | The type of truststore for Flink's internal endpoints (rpc, data transport, blob server). |
| ##### security.ssl.protocol [Anchor link for: security ssl protocol](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-protocol) | "TLSv1.2" | String | The SSL protocol version to be supported for the ssl transport. Note that it doesn’t support comma separated list. |
| ##### security.ssl.rest.authentication-enabled [Anchor link for: security ssl rest authentication enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-authentication-enabled) | false | Boolean | Turns on mutual SSL authentication for external communication via the REST endpoints. |
| ##### security.ssl.rest.cert.fingerprint [Anchor link for: security ssl rest cert fingerprint](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-cert-fingerprint) | (none) | String | The sha1 fingerprint of the rest certificate. This further protects the rest REST endpoints to present certificate which is only used by proxy serverThis is necessary where once uses public CA or internal firm wide CA |
| ##### security.ssl.rest.enabled [Anchor link for: security ssl rest enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-enabled) | false | Boolean | Turns on SSL for external communication via the REST endpoints. |
| ##### security.ssl.rest.key-password [Anchor link for: security ssl rest key password](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-key-password) | (none) | String | The secret to decrypt the key in the keystore for Flink's external REST endpoints. |
| ##### security.ssl.rest.keystore [Anchor link for: security ssl rest keystore](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-keystore) | (none) | String | The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints. |
| ##### security.ssl.rest.keystore-password [Anchor link for: security ssl rest keystore password](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-keystore-password) | (none) | String | The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints. |
| ##### security.ssl.rest.keystore-type [Anchor link for: security ssl rest keystore type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-keystore-type) | JVM default keystore type | String | The type of the keystore for Flink's external REST endpoints. |
| ##### security.ssl.rest.truststore [Anchor link for: security ssl rest truststore](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-truststore) | (none) | String | The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints. |
| ##### security.ssl.rest.truststore-password [Anchor link for: security ssl rest truststore password](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-truststore-password) | (none) | String | The password to decrypt the truststore for Flink's external REST endpoints. |
| ##### security.ssl.rest.truststore-type [Anchor link for: security ssl rest truststore type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-rest-truststore-type) | JVM default keystore type | String | The type of the truststore for Flink's external REST endpoints. |
| ##### security.ssl.verify-hostname [Anchor link for: security ssl verify hostname](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-verify-hostname) | true | Boolean | Flag to enable peer’s hostname verification during ssl handshake. |

### Auth with External Systems  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#auth-with-external-systems)

**Delegation token**

Flink has a pluggable authentication protocol agnostic delegation token framework.
Please refer to the [Flink and Delegation Token Docs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/security/security-delegation-token/) for further details.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### security.delegation.tokens.enabled [Anchor link for: security delegation tokens enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-delegation-tokens-enabled) | true | Boolean | Indicates whether to start delegation tokens system for external services. |
| ##### security.delegation.tokens.renewal.retry.backoff [Anchor link for: security delegation tokens renewal retry backoff](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-delegation-tokens-renewal-retry-backoff) | 1 h | Duration | The time period how long to wait before retrying to obtain new delegation tokens after a failure. |
| ##### security.delegation.tokens.renewal.time-ratio [Anchor link for: security delegation tokens renewal time ratio](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-delegation-tokens-renewal-time-ratio) | 0.75 | Double | Ratio of the tokens's expiration time when new credentials should be re-obtained. |
| ##### security.delegation.token.provider.<serviceName>.enabled [Anchor link for: security delegation token provider <servicename> enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-delegation-token-provider-%3Cservicename%3E-enabled) | true | Boolean | Controls whether to obtain credentials for services when security is enabled. By default, credentials for all supported services are retrieved when those services are configured, but it's possible to disable that behavior if it somehow conflicts with the application being run. |

**ZooKeeper Authentication / Authorization**

These options are necessary when connecting to a secured ZooKeeper quorum.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### zookeeper.sasl.disable [Anchor link for: zookeeper sasl disable](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#zookeeper-sasl-disable) | false | Boolean |  |
| ##### zookeeper.sasl.login-context-name [Anchor link for: zookeeper sasl login context name](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#zookeeper-sasl-login-context-name) | "Client" | String |  |
| ##### zookeeper.sasl.service-name [Anchor link for: zookeeper sasl service name](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#zookeeper-sasl-service-name) | "zookeeper" | String |  |

**Kerberos-based Authentication / Authorization**

Please refer to the [Flink and Kerberos Docs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/security/security-kerberos/) for a setup guide and a list of external system to which Flink can authenticate itself via Kerberos.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### security.kerberos.access.hadoopFileSystems [Anchor link for: security kerberos access hadoopfilesystems](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-kerberos-access-hadoopfilesystems) | (none) | List<String> | A semicolon-separated list of Kerberos-secured Hadoop filesystems Flink is going to access. For example, security.kerberos.access.hadoopFileSystems=hdfs://namenode2:9002;hdfs://namenode3:9003. The JobManager needs to have access to these filesystems to retrieve the security tokens. |
| ##### security.kerberos.login.contexts [Anchor link for: security kerberos login contexts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-kerberos-login-contexts) | (none) | String | A comma-separated list of login contexts to provide the Kerberos credentials to (for example, \`Client,KafkaClient\` to use the credentials for ZooKeeper authentication and for Kafka authentication) |
| ##### security.kerberos.login.keytab [Anchor link for: security kerberos login keytab](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-kerberos-login-keytab) | (none) | String | Absolute path to a Kerberos keytab file that contains the user credentials. |
| ##### security.kerberos.login.principal [Anchor link for: security kerberos login principal](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-kerberos-login-principal) | (none) | String | Kerberos principal name associated with the keytab. |
| ##### security.kerberos.login.use-ticket-cache [Anchor link for: security kerberos login use ticket cache](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-kerberos-login-use-ticket-cache) | true | Boolean | Indicates whether to read from your Kerberos ticket cache. |
| ##### security.kerberos.relogin.period [Anchor link for: security kerberos relogin period](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-kerberos-relogin-period) | 1 min | Duration | The time period when keytab login happens automatically in order to always have a valid TGT. |

* * *

* * *

# Resource Orchestration Frameworks  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resource-orchestration-frameworks)

This section contains options related to integrating Flink with resource orchestration frameworks, like Kubernetes, Yarn, etc.

Note that is not always necessary to integrate Flink with the resource orchestration framework.
For example, you can easily deploy Flink applications on Kubernetes without Flink knowing that it runs on Kubernetes (and without specifying any of the Kubernetes config options here.) See [this setup guide](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes/) for an example.

The options in this section are necessary for setups where Flink itself actively requests and releases resources from the orchestrators.

### YARN  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### external-resource.<resource\_name>.yarn.config-key [Anchor link for: external resource <resource_name> yarn config key](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#external-resource-%3Cresource_name%3E-yarn-config-key) | (none) | String | If configured, Flink will add this key to the resource profile of container request to Yarn. The value will be set to the value of external-resource.<resource\_name>.amount. |
| ##### flink.hadoop.<key> [Anchor link for: flink hadoop <key>](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#flink-hadoop-%3Ckey%3E) | (none) | String | A general option to probe Hadoop configuration through prefix 'flink.hadoop.'. Flink will remove the prefix to get <key> (from [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml) and [hdfs-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)) then set the <key> and value to Hadoop configuration. For example, flink.hadoop.dfs.replication=5 in Flink configuration and convert to dfs.replication=5 in Hadoop configuration. |
| ##### flink.yarn.<key> [Anchor link for: flink yarn <key>](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#flink-yarn-%3Ckey%3E) | (none) | String | A general option to probe Yarn configuration through prefix 'flink.yarn.'. Flink will remove the prefix 'flink.' to get yarn.<key> (from [yarn-default.xml](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)) then set the yarn.<key> and value to Yarn configuration. For example, flink.yarn.resourcemanager.container.liveness-monitor.interval-ms=300000 in Flink configuration and convert to yarn.resourcemanager.container.liveness-monitor.interval-ms=300000 in Yarn configuration. |
| ##### yarn.application-attempt-failures-validity-interval [Anchor link for: yarn application attempt failures validity interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-attempt-failures-validity-interval) | 10000 | Long | Time window in milliseconds which defines the number of application attempt failures when restarting the AM. Failures which fall outside of this window are not being considered. Set this value to -1 in order to count globally. See [here](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Application_Attempts_API) for more information. |
| ##### yarn.application-attempts [Anchor link for: yarn application attempts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-attempts) | (none) | Integer | Number of ApplicationMaster restarts. By default, the value will be set to 1. If high availability is enabled, then the default value will be 2. The restart number is also limited by YARN (configured via [yarn.resourcemanager.am.max-attempts](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)). Note that the entire Flink cluster will restart and the YARN Client will lose the connection. |
| ##### yarn.application-master.port [Anchor link for: yarn application master port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-master-port) | "0" | String | With this configuration option, users can specify a port, a range of ports or a list of ports for the Application Master (and JobManager) RPC port. By default we recommend using the default value (0) to let the operating system choose an appropriate port. In particular when multiple AMs are running on the same physical host, fixed port assignments prevent the AM from starting. For example when running Flink on YARN on an environment with a restrictive firewall, this option allows specifying a range of allowed ports. |
| ##### yarn.application.id [Anchor link for: yarn application id](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-id) | (none) | String | The YARN application id of the running yarn cluster. This is the YARN cluster where the pipeline is going to be executed. |
| ##### yarn.application.name [Anchor link for: yarn application name](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-name) | (none) | String | A custom name for your YARN application. |
| ##### yarn.application.node-label [Anchor link for: yarn application node label](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-node-label) | (none) | String | Specify YARN node label for the YARN application. |
| ##### yarn.application.priority [Anchor link for: yarn application priority](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-priority) | -1 | Integer | A non-negative integer indicating the priority for submitting a Flink YARN application. It will only take effect if YARN priority scheduling setting is enabled. Larger integer corresponds with higher priority. If priority is negative or set to '-1'(default), Flink will unset yarn priority setting and use cluster default priority. Please refer to YARN's official documentation for specific settings required to enable priority scheduling for the targeted YARN version. |
| ##### yarn.application.queue [Anchor link for: yarn application queue](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-queue) | (none) | String | The YARN queue on which to put the current pipeline. |
| ##### yarn.application.type [Anchor link for: yarn application type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-application-type) | (none) | String | A custom type for your YARN application.. |
| ##### yarn.appmaster.vcores [Anchor link for: yarn appmaster vcores](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-appmaster-vcores) | 1 | Integer | The number of virtual cores (vcores) used by YARN application master. |
| ##### yarn.classpath.include-user-jar [Anchor link for: yarn classpath include user jar](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-classpath-include-user-jar) | ORDER | Enum | Defines whether user-jars are included in the system class path as well as their positioning in the path.<br>Possible values:<br>- "DISABLED": Exclude user jars from the system class path<br>- "FIRST": Position at the beginning<br>- "LAST": Position at the end<br>- "ORDER": Position based on the name of the jar |
| ##### yarn.container-start-command-template [Anchor link for: yarn container start command template](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-container-start-command-template) | "%java% %jvmmem% %jvmopts% %logging% %class% %args% %redirects%" | String | This configuration parameter allows users to pass custom settings (such as JVM paths, arguments etc.) to start the YARN. The following placeholders will be replaced: <br>- %java%: Path to the Java executable<br>- %jvmmem%: JVM memory limits and tweaks<br>- %jvmopts%: Options for the Java VM<br>- %logging%: Logging-related configuration settings<br>- %class%: Main class to execute<br>- %args%: Arguments for the main class<br>- %redirects%: Output redirects |
| ##### yarn.containers.vcores [Anchor link for: yarn containers vcores](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-containers-vcores) | -1 | Integer | The number of virtual cores (vcores) per YARN container. By default, the number of vcores is set to the number of slots per TaskManager, if set, or to 1, otherwise. In order for this parameter to be used your cluster must have CPU scheduling enabled. You can do this by setting the `org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler`. |
| ##### yarn.file-replication [Anchor link for: yarn file replication](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-file-replication) | -1 | Integer | Number of file replication of each local resource file. If it is not configured, Flink will use the default replication value in hadoop configuration. |
| ##### yarn.flink-dist-jar [Anchor link for: yarn flink dist jar](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-flink-dist-jar) | (none) | String | The location of the Flink dist jar. |
| ##### yarn.heartbeat.container-request-interval [Anchor link for: yarn heartbeat container request interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-heartbeat-container-request-interval) | 500 ms | Duration | Time between heartbeats with the ResourceManager if Flink requests containers:<br>- The lower this value is, the faster Flink will get notified about container allocations since requests and allocations are transmitted via heartbeats.<br>- The lower this value is, the more excessive containers might get allocated which will eventually be released but put pressure on Yarn.<br>If you observe too many container allocations on the ResourceManager, then it is recommended to increase this value. See [this link](https://issues.apache.org/jira/browse/YARN-1902) for more information. |
| ##### yarn.heartbeat.interval [Anchor link for: yarn heartbeat interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-heartbeat-interval) | 5 | Integer | Time between heartbeats with the ResourceManager in seconds. |
| ##### yarn.modify.acls [Anchor link for: yarn modify acls](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-modify-acls) | (none) | String | Users and groups to give MODIFY access. The ACLs are of for comma-separated-users&lt;space&gt;comma-separated-groups. Wildcard ACL is also supported. The only valid wildcard ACL is \*, which grants permission to all users and groups. |
| ##### yarn.properties-file.location [Anchor link for: yarn properties file location](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-properties-file-location) | (none) | String | When a Flink job is submitted to YARN, the JobManager’s host and the number of available processing slots is written into a properties file, so that the Flink client is able to pick those details up. This configuration parameter allows changing the default location of that file (for example for environments sharing a Flink installation between users). |
| ##### yarn.provided.lib.dirs [Anchor link for: yarn provided lib dirs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-provided-lib-dirs) | (none) | List<String> | A semicolon-separated list of provided lib directories. They should be pre-uploaded and world-readable. Flink will use them to exclude the local Flink jars(e.g. flink-dist, lib/, plugins/)uploading to accelerate the job submission process. Also YARN will cache them on the nodes so that they doesn't need to be downloaded every time for each application. An example could be hdfs://$namenode\_address/path/of/flink/lib |
| ##### yarn.provided.usrlib.dir [Anchor link for: yarn provided usrlib dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-provided-usrlib-dir) | (none) | String | The provided usrlib directory in remote. It should be pre-uploaded and world-readable. Flink will use it to exclude the local usrlib directory(i.e. usrlib/ under the parent directory of FLINK\_LIB\_DIR). Unlike yarn.provided.lib.dirs, YARN will not cache it on the nodes as it is for each application. An example could be hdfs://$namenode\_address/path/of/flink/usrlib |
| ##### yarn.rolled-logs.exclude-pattern [Anchor link for: yarn rolled logs exclude pattern](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-rolled-logs-exclude-pattern) | (none) | String | Java regular expression to exclude certain log files from rolling log aggregation. Log files matching the defined exclude pattern will be ignored during aggregation. If a log file matches both the include and exclude patterns, the exclude pattern takes precedence and the file will be excluded from aggregation. |
| ##### yarn.rolled-logs.include-pattern [Anchor link for: yarn rolled logs include pattern](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-rolled-logs-include-pattern) | (none) | String | Java regular expression to match log file names for inclusion in rolling log aggregation. This regex is used by YARN’s log aggregation mechanism to identify which log files to collect. To enable rolling aggregation in YARN, set the \`yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds\` property in \`yarn-site.xml\`. Ensure that Flink’s Log4J configuration uses FileAppender or a compatible appender that can handle file deletions during runtime. The regex pattern (e.g., \`jobmanager\*\`) must align with the log file names defined in the Log4J configuration (e.g., \`jobmanager.log\`) to ensure all relevant files will be aggregated. |
| ##### yarn.security.appmaster.delegation.token.services [Anchor link for: yarn security appmaster delegation token services](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-security-appmaster-delegation-token-services) | "hadoopfs" | List<String> | The delegation token provider services are allowed to pass obtained tokens to YARN application master. For backward compatibility to make log aggregation to work, we add tokens obtained by \`hadoopfs\` provider to AM by default. |
| ##### yarn.security.kerberos.localized-keytab-path [Anchor link for: yarn security kerberos localized keytab path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-security-kerberos-localized-keytab-path) | "krb5.keytab" | String | Local (on NodeManager) path where kerberos keytab file will be localized to. If yarn.security.kerberos.ship-local-keytab set to true, Flink will ship the keytab file as a YARN local resource. In this case, the path is relative to the local resource directory. If set to false, Flink will try to directly locate the keytab from the path itself. |
| ##### yarn.security.kerberos.ship-local-keytab [Anchor link for: yarn security kerberos ship local keytab](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-security-kerberos-ship-local-keytab) | true | Boolean | When this is true Flink will ship the keytab file configured via security.kerberos.login.keytab as a localized YARN resource. |
| ##### yarn.ship-archives [Anchor link for: yarn ship archives](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-ship-archives) | (none) | List<String> | A semicolon-separated list of archives to be shipped to the YARN cluster. These archives can come from the local path of flink client or HDFS. They will be un-packed when localizing and they can be any of the following types: ".tar.gz", ".tar", ".tgz", ".dst", ".jar", ".zip". For example, "/path/to/local/archive.jar;hdfs://$namenode\_address/path/to/archive.jar" |
| ##### yarn.ship-files [Anchor link for: yarn ship files](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-ship-files) | (none) | List<String> | A semicolon-separated list of files and/or directories to be shipped to the YARN cluster. These files/directories can come from the local path of flink client or HDFS. For example, "/path/to/local/file;/path/to/local/directory;hdfs://$namenode\_address/path/of/file;hdfs://$namenode\_address/path/of/directory" |
| ##### yarn.staging-directory [Anchor link for: yarn staging directory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-staging-directory) | (none) | String | Staging directory used to store YARN files while submitting applications. Per default, it uses the home directory of the configured file system. |
| ##### yarn.tags [Anchor link for: yarn tags](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-tags) | (none) | String | A comma-separated list of tags to apply to the Flink YARN application. |
| ##### yarn.taskmanager.node-label [Anchor link for: yarn taskmanager node label](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-taskmanager-node-label) | (none) | String | Specify YARN node label for the Flink TaskManagers, it will override the yarn.application.node-label for TaskManagers if both are set. |
| ##### yarn.view.acls [Anchor link for: yarn view acls](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#yarn-view-acls) | (none) | String | Users and groups to give VIEW access. The ACLs are of for comma-separated-users&lt;space&gt;comma-separated-groups. Wildcard ACL is also supported. The only valid wildcard ACL is \*, which grants permission to all users and groups. |

### Kubernetes  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### external-resource.<resource\_name>.kubernetes.config-key [Anchor link for: external resource <resource_name> kubernetes config key](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#external-resource-%3Cresource_name%3E-kubernetes-config-key) | (none) | String | If configured, Flink will add "resources.limits.<config-key>" and "resources.requests.<config-key>" to the main container of TaskExecutor and set the value to the value of external-resource.<resource\_name>.amount. |
| ##### kubernetes.artifacts.local-upload-enabled [Anchor link for: kubernetes artifacts local upload enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-artifacts-local-upload-enabled) | false | Boolean | Enables uploading 'local://' schemed artifacts to DFS before the application cluster deployment. |
| ##### kubernetes.artifacts.local-upload-overwrite [Anchor link for: kubernetes artifacts local upload overwrite](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-artifacts-local-upload-overwrite) | false | Boolean | If enabled, overwrites any existing artifact on the remote target. Disabled by default. |
| ##### kubernetes.artifacts.local-upload-target [Anchor link for: kubernetes artifacts local upload target](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-artifacts-local-upload-target) | (none) | String | The target remote DFS directory to upload local artifacts. |
| ##### kubernetes.client.io-pool.size [Anchor link for: kubernetes client io pool size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-client-io-pool-size) | 4 | Integer | The size of the IO executor pool used by the Kubernetes client to execute blocking IO operations (e.g. start/stop TaskManager pods, update leader related ConfigMaps, etc.). Increasing the pool size allows to run more IO operations concurrently. |
| ##### kubernetes.client.user-agent [Anchor link for: kubernetes client user agent](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-client-user-agent) | "flink" | String | The user agent to be used for contacting with Kubernetes APIServer. |
| ##### kubernetes.cluster-id [Anchor link for: kubernetes cluster id](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-cluster-id) | (none) | String | The cluster-id, which should be no more than 45 characters, is used for identifying a unique Flink cluster. The id must only contain lowercase alphanumeric characters and "-". The required format is `[a-z]([-a-z0-9]*[a-z0-9])`. If not set, the client will automatically generate it with a random ID. |
| ##### kubernetes.config.file [Anchor link for: kubernetes config file](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-config-file) | (none) | String | The kubernetes config file will be used to create the client. The default is located at ~/.kube/config |
| ##### kubernetes.container.image.pull-policy [Anchor link for: kubernetes container image pull policy](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-container-image-pull-policy) | IfNotPresent | Enum | The Kubernetes container image pull policy. The default policy is IfNotPresent to avoid putting pressure to image repository.<br>Possible values:<br>- "IfNotPresent"<br>- "Always"<br>- "Never" |
| ##### kubernetes.container.image.pull-secrets [Anchor link for: kubernetes container image pull secrets](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-container-image-pull-secrets) | (none) | List<String> | A semicolon-separated list of the Kubernetes secrets used to access private image registries. |
| ##### kubernetes.container.image.ref [Anchor link for: kubernetes container image ref](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-container-image-ref) | The default value depends on the actually running version. In general it looks like "flink:<FLINK\_VERSION>-scala\_<SCALA\_VERSION>" | String | Image to use for Flink containers. The specified image must be based upon the same Apache Flink and Scala versions as used by the application. Visit [here](https://hub.docker.com/_/flink?tab=tags) for the official docker images provided by the Flink project. The Flink project also publishes docker images to [apache/flink DockerHub repository](https://hub.docker.com/r/apache/flink/tags). |
| ##### kubernetes.context [Anchor link for: kubernetes context](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-context) | (none) | String | The desired context from your Kubernetes config file used to configure the Kubernetes client for interacting with the cluster. This could be helpful if one has multiple contexts configured and wants to administrate different Flink clusters on different Kubernetes clusters/contexts. |
| ##### kubernetes.decorator.hadoop-conf-mount.enabled [Anchor link for: kubernetes decorator hadoop conf mount enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-decorator-hadoop-conf-mount-enabled) | true | Boolean | Whether to enable Hadoop configuration mount decorator. This must be set to false when Hadoop config is mounted outside of Flink. A typical use-case is when one uses Flink Kubernetes Operator. |
| ##### kubernetes.decorator.kerberos-mount.enabled [Anchor link for: kubernetes decorator kerberos mount enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-decorator-kerberos-mount-enabled) | true | Boolean | Whether to enable Kerberos mount decorator. This must be set to false when Kerberos config and keytab is mounted outside of Flink. A typical use-case is when one uses Flink Kubernetes Operator. |
| ##### kubernetes.entry.path [Anchor link for: kubernetes entry path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-entry-path) | "/docker-entrypoint.sh" | String | The entrypoint script of kubernetes in the image. It will be used as command for jobmanager and taskmanager container. |
| ##### kubernetes.env.secretKeyRef [Anchor link for: kubernetes env secretkeyref](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-env-secretkeyref) | (none) | List<Map> | The user-specified secrets to set env variables in Flink container. The value should be in the form of `env:FOO_ENV,secret:foo_secret,key:foo_key;env:BAR_ENV,secret:bar_secret,key:bar_key`. |
| ##### kubernetes.flink.conf.dir [Anchor link for: kubernetes flink conf dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-flink-conf-dir) | "/opt/flink/conf" | String | The flink conf directory that will be mounted in pod. The config.yaml, log4j.properties, logback.xml in this path will be overwritten from config map. |
| ##### kubernetes.flink.log.dir [Anchor link for: kubernetes flink log dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-flink-log-dir) | (none) | String | The directory that logs of jobmanager and taskmanager be saved in the pod. The default value is $FLINK\_HOME/log. |
| ##### kubernetes.hadoop.conf.config-map.name [Anchor link for: kubernetes hadoop conf config map name](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-hadoop-conf-config-map-name) | (none) | String | Specify the name of an existing ConfigMap that contains custom Hadoop configuration to be mounted on the JobManager(s) and TaskManagers. |
| ##### kubernetes.hostnetwork.enabled [Anchor link for: kubernetes hostnetwork enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-hostnetwork-enabled) | false | Boolean | Whether to enable HostNetwork mode. The HostNetwork allows the pod could use the node network namespace instead of the individual pod network namespace. Please note that the JobManager service account should have the permission to update Kubernetes service. |
| ##### kubernetes.internal-service.annotations [Anchor link for: kubernetes internal service annotations](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-internal-service-annotations) | (none) | Map | The user-specified annotations that are set to the internal Service. The value should be in the form of a1:v1,a2:v2 |
| ##### kubernetes.internal-service.labels [Anchor link for: kubernetes internal service labels](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-internal-service-labels) | (none) | Map | The user-specified labels that are set to the internal Service. The value should be in the form of a1:v1,a2:v2 |
| ##### kubernetes.jobmanager.annotations [Anchor link for: kubernetes jobmanager annotations](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-annotations) | (none) | Map | The user-specified annotations that are set to the JobManager pod. The value could be in the form of a1:v1,a2:v2 |
| ##### kubernetes.jobmanager.cpu.amount [Anchor link for: kubernetes jobmanager cpu amount](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-cpu-amount) | 1.0 | Double | The number of cpu used by job manager |
| ##### kubernetes.jobmanager.cpu.limit-factor [Anchor link for: kubernetes jobmanager cpu limit factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-cpu-limit-factor) | 1.0 | Double | The limit factor of cpu used by job manager. The resources limit cpu will be set to cpu \* limit-factor. |
| ##### kubernetes.jobmanager.entrypoint.args [Anchor link for: kubernetes jobmanager entrypoint args](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-entrypoint-args) | (none) | String | Extra arguments used when starting the job manager. |
| ##### kubernetes.jobmanager.labels [Anchor link for: kubernetes jobmanager labels](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-labels) | (none) | Map | The labels to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test. |
| ##### kubernetes.jobmanager.memory.limit-factor [Anchor link for: kubernetes jobmanager memory limit factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-memory-limit-factor) | 1.0 | Double | The limit factor of memory used by job manager. The resources limit memory will be set to memory \* limit-factor. |
| ##### kubernetes.jobmanager.node-selector [Anchor link for: kubernetes jobmanager node selector](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-node-selector) | (none) | Map | The node selector to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd. |
| ##### kubernetes.jobmanager.owner.reference [Anchor link for: kubernetes jobmanager owner reference](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-owner-reference) | (none) | List<Map> | The user-specified [Owner References](https://nightlies.apache.org/flink/flink-docs-master/deployment/resource-providers/native_kubernetes.html#manual-resource-cleanup) to be set to the JobManager Deployment. When all the owner resources are deleted, the JobManager Deployment will be deleted automatically, which also deletes all the resources created by this Flink cluster. The value should be formatted as a semicolon-separated list of owner references, where each owner reference is a comma-separated list of \`key:value\` pairs. E.g., apiVersion:v1,blockOwnerDeletion:true,controller:true,kind:FlinkApplication,name:flink-app-name,uid:flink-app-uid;apiVersion:v1,kind:Deployment,name:deploy-name,uid:deploy-uid |
| ##### kubernetes.jobmanager.replicas [Anchor link for: kubernetes jobmanager replicas](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-replicas) | 1 | Integer | Specify how many JobManager pods will be started simultaneously. Configure the value to greater than 1 to start standby JobManagers. It will help to achieve faster recovery. Notice that high availability should be enabled when starting standby JobManagers. |
| ##### kubernetes.jobmanager.service-account [Anchor link for: kubernetes jobmanager service account](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-service-account) | "default" | String | Service account that is used by jobmanager within kubernetes cluster. The job manager uses this service account when requesting taskmanager pods from the API server. If not explicitly configured, config option 'kubernetes.service-account' will be used. |
| ##### kubernetes.jobmanager.tolerations [Anchor link for: kubernetes jobmanager tolerations](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-jobmanager-tolerations) | (none) | List<Map> | The user-specified tolerations to be set to the JobManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000 |
| ##### kubernetes.namespace [Anchor link for: kubernetes namespace](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-namespace) | "default" | String | The namespace that will be used for running the jobmanager and taskmanager pods. |
| ##### kubernetes.pod-template-file.default [Anchor link for: kubernetes pod template file default](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-pod-template-file-default) | (none) | String | Specify a local file that contains the pod template definition. It will be used to initialize the jobmanager and taskmanager pod. The main container should be defined with name 'flink-main-container'. Notice that this can be overwritten by config options 'kubernetes.pod-template-file.jobmanager' and 'kubernetes.pod-template-file.taskmanager' for jobmanager and taskmanager respectively. |
| ##### kubernetes.pod-template-file.jobmanager [Anchor link for: kubernetes pod template file jobmanager](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-pod-template-file-jobmanager) | (none) | String | Specify a local file that contains the jobmanager pod template definition. It will be used to initialize the jobmanager pod. The main container should be defined with name 'flink-main-container'. If not explicitly configured, config option 'kubernetes.pod-template-file.default' will be used. |
| ##### kubernetes.pod-template-file.taskmanager [Anchor link for: kubernetes pod template file taskmanager](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-pod-template-file-taskmanager) | (none) | String | Specify a local file that contains the taskmanager pod template definition. It will be used to initialize the taskmanager pod. The main container should be defined with name 'flink-main-container'. If not explicitly configured, config option 'kubernetes.pod-template-file.default' will be used. |
| ##### kubernetes.rest-service.annotations [Anchor link for: kubernetes rest service annotations](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-rest-service-annotations) | (none) | Map | The user-specified annotations that are set to the rest Service. The value should be in the form of a1:v1,a2:v2 |
| ##### kubernetes.rest-service.exposed.node-port-address-type [Anchor link for: kubernetes rest service exposed node port address type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-rest-service-exposed-node-port-address-type) | InternalIP | Enum | The user-specified [address type](https://kubernetes.io/docs/concepts/architecture/nodes/#addresses) that is used for filtering node IPs when constructing a [node port](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport) connection string. This option is only considered when 'kubernetes.rest-service.exposed.type' is set to 'NodePort'.<br>Possible values:<br>- "InternalIP"<br>- "ExternalIP" |
| ##### kubernetes.rest-service.exposed.type [Anchor link for: kubernetes rest service exposed type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-rest-service-exposed-type) | ClusterIP | Enum | The exposed type of the rest service. The exposed rest service could be used to access the Flink’s Web UI and REST endpoint.<br>Possible values:<br>- "ClusterIP"<br>- "NodePort"<br>- "LoadBalancer"<br>- "Headless\_ClusterIP" |
| ##### kubernetes.rest-service.labels [Anchor link for: kubernetes rest service labels](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-rest-service-labels) | (none) | Map | The user-specified labels that are set to the rest Service. The value should be in the form of a1:v1,a2:v2 |
| ##### kubernetes.secrets [Anchor link for: kubernetes secrets](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-secrets) | (none) | Map | The user-specified secrets that will be mounted into Flink container. The value should be in the form of `foo:/opt/secrets-foo,bar:/opt/secrets-bar`. |
| ##### kubernetes.service-account [Anchor link for: kubernetes service account](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-service-account) | "default" | String | Service account that is used by jobmanager and taskmanager within kubernetes cluster. Notice that this can be overwritten by config options 'kubernetes.jobmanager.service-account' and 'kubernetes.taskmanager.service-account' for jobmanager and taskmanager respectively. |
| ##### kubernetes.taskmanager.annotations [Anchor link for: kubernetes taskmanager annotations](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-annotations) | (none) | Map | The user-specified annotations that are set to the TaskManager pod. The value could be in the form of a1:v1,a2:v2 |
| ##### kubernetes.taskmanager.cpu.amount [Anchor link for: kubernetes taskmanager cpu amount](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-cpu-amount) | -1.0 | Double | The number of cpu used by task manager. By default, the cpu is set to the number of slots per TaskManager |
| ##### kubernetes.taskmanager.cpu.limit-factor [Anchor link for: kubernetes taskmanager cpu limit factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-cpu-limit-factor) | 1.0 | Double | The limit factor of cpu used by task manager. The resources limit cpu will be set to cpu \* limit-factor. |
| ##### kubernetes.taskmanager.entrypoint.args [Anchor link for: kubernetes taskmanager entrypoint args](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-entrypoint-args) | (none) | String | Extra arguments used when starting the task manager. |
| ##### kubernetes.taskmanager.labels [Anchor link for: kubernetes taskmanager labels](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-labels) | (none) | Map | The labels to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test. |
| ##### kubernetes.taskmanager.memory.limit-factor [Anchor link for: kubernetes taskmanager memory limit factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-memory-limit-factor) | 1.0 | Double | The limit factor of memory used by task manager. The resources limit memory will be set to memory \* limit-factor. |
| ##### kubernetes.taskmanager.node-selector [Anchor link for: kubernetes taskmanager node selector](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-node-selector) | (none) | Map | The node selector to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd. |
| ##### kubernetes.taskmanager.service-account [Anchor link for: kubernetes taskmanager service account](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-service-account) | "default" | String | Service account that is used by taskmanager within kubernetes cluster. The task manager uses this service account when watching config maps on the API server to retrieve leader address of jobmanager and resourcemanager. If not explicitly configured, config option 'kubernetes.service-account' will be used. |
| ##### kubernetes.taskmanager.tolerations [Anchor link for: kubernetes taskmanager tolerations](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-taskmanager-tolerations) | (none) | List<Map> | The user-specified tolerations to be set to the TaskManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000 |
| ##### kubernetes.transactional-operation.initial-retry-delay [Anchor link for: kubernetes transactional operation initial retry delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-transactional-operation-initial-retry-delay) | 50 ms | Duration | Defines the initial duration of Kubernetes transactional operation retries after fail |
| ##### kubernetes.transactional-operation.max-retries [Anchor link for: kubernetes transactional operation max retries](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-transactional-operation-max-retries) | 15 | Integer | Defines the number of Kubernetes transactional operation retries before the client gives up. For example, `FlinkKubeClient#checkAndUpdateConfigMap`. |
| ##### kubernetes.transactional-operation.max-retry-delay [Anchor link for: kubernetes transactional operation max retry delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#kubernetes-transactional-operation-max-retry-delay) | 1 min | Duration | Defines the max duration of Kubernetes transactional operation retries after fail |

* * *

* * *

# State Backends  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backends)

Please refer to the [State Backend Documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/) for background on State Backends.

### RocksDB State Backend  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rocksdb-state-backend)

These are the options commonly needed to configure the RocksDB state backend. See the [Advanced RocksDB Backend Section](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#advanced-rocksdb-state-backends-options) for options necessary for advanced low level configurations and trouble-shooting.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.rocksdb.memory.fixed-per-slot [Anchor link for: state backend rocksdb memory fixed per slot](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-memory-fixed-per-slot) | (none) | MemorySize | The fixed total amount of memory, shared among all RocksDB instances per slot. This option overrides the 'state.backend.rocksdb.memory.managed' option when configured. |
| ##### state.backend.rocksdb.memory.fixed-per-tm [Anchor link for: state backend rocksdb memory fixed per tm](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-memory-fixed-per-tm) | (none) | MemorySize | The fixed total amount of memory, shared among all RocksDB instances per Task Manager (cluster-level option). This option only takes effect if neither 'state.backend.rocksdb.memory.managed' nor 'state.backend.rocksdb.memory.fixed-per-slot' are not configured. If none is configured then each RocksDB column family state has its own memory caches (as controlled by the column family options). The relevant options for the shared resources (e.g. write-buffer-ratio) can be set on the same level (config.yaml).Note, that this feature breaks resource isolation between the slots |
| ##### state.backend.rocksdb.memory.high-prio-pool-ratio [Anchor link for: state backend rocksdb memory high prio pool ratio](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-memory-high-prio-pool-ratio) | 0.1 | Double | The fraction of cache memory that is reserved for high-priority data like index, filter, and compression dictionary blocks. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. |
| ##### state.backend.rocksdb.memory.managed [Anchor link for: state backend rocksdb memory managed](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-memory-managed) | true | Boolean | If set, the RocksDB state backend will automatically configure itself to use the managed memory budget of the task slot, and divide the memory over write buffers, indexes, block caches, etc. That way, the three major uses of memory of RocksDB will be capped. |
| ##### state.backend.rocksdb.memory.partitioned-index-filters [Anchor link for: state backend rocksdb memory partitioned index filters](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-memory-partitioned-index-filters) | false | Boolean | With partitioning, the index/filter block of an SST file is partitioned into smaller blocks with an additional top-level index on them. When reading an index/filter, only top-level index is loaded into memory. The partitioned index/filter then uses the top-level index to load on demand into the block cache the partitions that are required to perform the index/filter query. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. |
| ##### state.backend.rocksdb.memory.write-buffer-ratio [Anchor link for: state backend rocksdb memory write buffer ratio](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-memory-write-buffer-ratio) | 0.5 | Double | The maximum amount of memory that write buffers may take, as a fraction of the total shared memory. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. |
| ##### state.backend.rocksdb.timer-service.cache-size [Anchor link for: state backend rocksdb timer service cache size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-timer-service-cache-size) | 128 | Integer | The cache size per keyGroup of rocksdb timer service factory. This option only has an effect when 'state.backend.rocksdb.timer-service.factory' is configured to 'ROCKSDB'. Increasing this value can improve the performance of rocksdb timer service, but consumes more heap memory at the same time. |
| ##### state.backend.rocksdb.timer-service.factory [Anchor link for: state backend rocksdb timer service factory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-timer-service-factory) | ROCKSDB | Enum | This determines the factory for timer service state implementation.<br>Possible values:<br>- "HEAP": Heap-based<br>- "ROCKSDB": Implementation based on RocksDB |

### ForSt State Backend  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#forst-state-backend)

These are the options commonly needed to configure the ForSt state backend. See the [Advanced ForSt Backend Section](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#advanced-forst-state-backends-options) for options necessary for advanced low level configurations and trouble-shooting.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.forst.cache.dir [Anchor link for: state backend forst cache dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-cache-dir) | (none) | String | The directory where ForSt caches its SST files, fallback to the subdirectory of '/cache' under the value of 'state.backend.forst.local-dir' if not configured. |
| ##### state.backend.forst.cache.reserve-size [Anchor link for: state backend forst cache reserve size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-cache-reserve-size) | 256 mb | MemorySize | The amount of reserved size on disk space, and remaining space can be leveraged by the cache. The cache will evict the oldest files when the reserved space on disk (the disk where cache directory is) is not enough. User should specify at least one cache size limit to enable the cache, either this option or the 'state.backend.forst.cache.size-based-limit' option. They can be set simultaneously, and in this case, cache will grow if meet the requirements of both two options. If the specified file system of the cache directory does not support reading the remaining space, the cache will not be able to reserve the specified space, hence this option will be ignored. The default value is '256 mb', meaning the disk will be reserved that much space, and the remaining of the disk can be used for cache. A configured value of '0 bytes' means that this option is disabled. |
| ##### state.backend.forst.cache.size-based-limit [Anchor link for: state backend forst cache size based limit](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-cache-size-based-limit) | 0 bytes | MemorySize | An upper-bound of the size that can be used for cache. User should specify at least one cache size limit to enable the cache, either this option or the 'state.backend.forst.cache.reserve-size' option. They can be set simultaneously, and in this case, cache will grow if meet the requirements of both two options. The default value is '0 bytes', meaning that this option is disabled. |
| ##### state.backend.forst.executor.read-io-parallelism [Anchor link for: state backend forst executor read io parallelism](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-executor-read-io-parallelism) | 3 | Integer | The number of threads used for read IO operations in the executor. |
| ##### state.backend.forst.executor.write-io-parallelism [Anchor link for: state backend forst executor write io parallelism](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-executor-write-io-parallelism) | 1 | Integer | The number of threads used for write IO operations in the executor. Only valid when 'state.backend.forst.executor.inline-write' is false. |
| ##### state.backend.forst.memory.partitioned-index-filters [Anchor link for: state backend forst memory partitioned index filters](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-memory-partitioned-index-filters) | true | Boolean | With partitioning, the index/filter block of an SST file is partitioned into smaller blocks with an additional top-level index on them. When reading an index/filter, only top-level index is loaded into memory. The partitioned index/filter then uses the top-level index to load on demand into the block cache the partitions that are required to perform the index/filter query. This option only has an effect when 'state.backend.forst.memory.managed' or 'state.backend.forst.memory.fixed-per-slot' are configured. |
| ##### state.backend.forst.primary-dir [Anchor link for: state backend forst primary dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-primary-dir) | "checkpoint-dir" | String | The primary directory where ForSt puts its SST files. By default, it will be the same as the checkpoint directory. Recognized shortcut name is 'checkpoint-dir', which means that ForSt shares the directory with checkpoint, and 'local-dir', which means that ForSt will use the local directory of TaskManager. |
| ##### state.backend.forst.sync.enforce-local [Anchor link for: state backend forst sync enforce local](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-sync-enforce-local) | true | Boolean | Whether to enforce local state for operators in synchronous mode when enabling disaggregated state. This is useful in cases where both synchronous operators and asynchronous operators are used in the same job. |
| ##### state.backend.forst.timer-service.cache-size [Anchor link for: state backend forst timer service cache size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-timer-service-cache-size) | 128 | Integer | The cache size per keyGroup of ForSt timer service factory. This option only has an effect when 'state.backend.forst.timer-service.factory' is configured to 'ForStDB'. Increasing this value can improve the performance of ForSt timer service, but consumes more heap memory at the same time. |
| ##### state.backend.forst.timer-service.factory [Anchor link for: state backend forst timer service factory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-timer-service-factory) | ForStDB | Enum | This determines the factory for timer service state implementation.<br>Possible values:<br>- "HEAP": Heap-based<br>- "ForStDB": Implementation based on RocksDB |

* * *

* * *

# Metrics  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics)

Please refer to the [metrics system documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/) for background on Flink’s metrics infrastructure.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### metrics.fetcher.update-interval [Anchor link for: metrics fetcher update interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-fetcher-update-interval) | 10 s | Duration | Update interval for the metric fetcher used by the web UI. Decrease this value for faster updating metrics. Increase this value if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely. |
| ##### metrics.internal.query-service.port [Anchor link for: metrics internal query service port 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-internal-query-service-port-1) | "0" | String | The port range used for Flink's internal metric query service. Accepts a list of ports (“50100,50101”), ranges(“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port. |
| ##### metrics.internal.query-service.thread-priority [Anchor link for: metrics internal query service thread priority](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-internal-query-service-thread-priority) | 1 | Integer | The thread priority used for Flink's internal metric query service. The thread is created by Pekko's thread pool executor. The range of the priority is from 1 (MIN\_PRIORITY) to 10 (MAX\_PRIORITY). Warning, increasing this value may bring the main Flink components down. |
| ##### metrics.job.status.enable [Anchor link for: metrics job status enable](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-job-status-enable) | CURRENT\_TIME | List<Enum> | The selection of job status metrics that should be reported.<br>Possible values:<br>- "STATE": For a given state, return 1 if the job is currently in that state, otherwise return 0.<br>- "CURRENT\_TIME": For a given state, if the job is currently in that state, return the time since the job transitioned into that state, otherwise return 0.<br>- "TOTAL\_TIME": For a given state, return how much time the job has spent in that state in total. |
| ##### metrics.latency.granularity [Anchor link for: metrics latency granularity](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-latency-granularity) | "operator" | String | Defines the granularity of latency metrics. Accepted values are:<br>- single - Track latency without differentiating between sources and subtasks.<br>- operator - Track latency while differentiating between sources, but not subtasks.<br>- subtask - Track latency while differentiating between sources and subtasks. |
| ##### metrics.latency.history-size [Anchor link for: metrics latency history size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-latency-history-size) | 128 | Integer | Defines the number of measured latencies to maintain at each operator. |
| ##### metrics.latency.interval [Anchor link for: metrics latency interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-latency-interval) | 0 ms | Duration | Defines the interval at which latency tracking marks are emitted from the sources. Disables latency tracking if set to 0 or a negative value. Enabling this feature can significantly impact the performance of the cluster. |
| ##### metrics.reporter.<name>.<parameter> [Anchor link for: metrics reporter <name> <parameter>](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporter-%3Cname%3E-%3Cparameter%3E) | (none) | String | Configures the parameter <parameter> for the reporter named <name>. |
| ##### metrics.reporter.<name>.factory.class [Anchor link for: metrics reporter <name> factory class](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporter-%3Cname%3E-factory-class) | (none) | String | The reporter factory class to use for the reporter named <name>. |
| ##### metrics.reporter.<name>.filter.excludes [Anchor link for: metrics reporter <name> filter excludes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporter-%3Cname%3E-filter-excludes) |  | List<String> | The metrics that should be excluded for the reporter named <name>. The format is identical to `filter.includes` |
| ##### metrics.reporter.<name>.filter.includes [Anchor link for: metrics reporter <name> filter includes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporter-%3Cname%3E-filter-includes) | "\*:\*:\*" | List<String> | The metrics that should be included for the reporter named <name>. Filters are specified as a list, with each filter following this format:<br>`<scope>[:<name>[,<name>][:<type>[,<type>]]]`<br>A metric matches a filter if the scope pattern and at least one of the name patterns and at least one of the types match.<br>- scope: Filters based on the logical scope.<br>  <br>  Specified as a pattern where `*` matches any sequence of characters and `.` separates scope components.<br>  <br>  <br>  <br>  For example:<br>  <br>   "`jobmanager.job`" matches any job-related metrics on the JobManager,<br>  <br>   "`*.job`" matches all job-related metrics and<br>  <br>   "`*.job.*`" matches all metrics below the job-level (i.e., task/operator metrics etc.).<br>  <br>- name: Filters based on the metric name.<br>  <br>  Specified as a comma-separate list of patterns where `*` matches any sequence of characters.<br>  <br>  <br>  <br>  For example, "`*Records*,*Bytes*`" matches any metrics where the name contains `"Records" or "Bytes"`.<br>  <br>- type: Filters based on the metric type. Specified as a comma-separated list of metric types: `[counter, meter, gauge, histogram]`<br>Examples:<br>- "`*:numRecords*`" Matches metrics like `numRecordsIn`.<br>- "`*.job.task.operator:numRecords*`" Matches metrics like `numRecordsIn` on the operator level.<br>- "`*.job.task.operator:numRecords*:meter`" Matches meter metrics like `numRecordsInPerSecond` on the operator level.<br>- "`*:numRecords*,numBytes*:counter,meter`" Matches all counter/meter metrics like or `numRecordsInPerSecond`. |
| ##### metrics.reporter.<name>.interval [Anchor link for: metrics reporter <name> interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporter-%3Cname%3E-interval) | 10 s | Duration | The reporter interval to use for the reporter named <name>. Only applicable to push-based reporters. |
| ##### metrics.reporter.<name>.scope.delimiter [Anchor link for: metrics reporter <name> scope delimiter](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporter-%3Cname%3E-scope-delimiter) | "." | String | The delimiter used to assemble the metric identifier for the reporter named <name>. |
| ##### metrics.reporter.<name>.scope.variables.additional [Anchor link for: metrics reporter <name> scope variables additional](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporter-%3Cname%3E-scope-variables-additional) |  | Map | The map of additional variables that should be included for the reporter named <name>. Only applicable to tag-based reporters. |
| ##### metrics.reporter.<name>.scope.variables.excludes [Anchor link for: metrics reporter <name> scope variables excludes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporter-%3Cname%3E-scope-variables-excludes) | "." | String | The set of variables that should be excluded for the reporter named <name>. Only applicable to tag-based reporters. |
| ##### metrics.reporters [Anchor link for: metrics reporters](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-reporters) | (none) | String | An optional list of reporter names. If configured, only reporters whose name matches any of the names in the list will be started. Otherwise, all reporters that could be found in the configuration will be started. |
| ##### metrics.scope.delimiter [Anchor link for: metrics scope delimiter](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-scope-delimiter) | "." | String | Delimiter used to assemble the metric identifier. |
| ##### metrics.scope.jm [Anchor link for: metrics scope jm](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-scope-jm) | "<host>.jobmanager" | String | Defines the scope format string that is applied to all metrics scoped to a JobManager. Only effective when a identifier-based reporter is configured. |
| ##### metrics.scope.jm-job [Anchor link for: metrics scope jm job](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-scope-jm-job) | "<host>.jobmanager.<job\_name>" | String | Defines the scope format string that is applied to all metrics scoped to a job on a JobManager. Only effective when a identifier-based reporter is configured |
| ##### metrics.scope.jm-operator [Anchor link for: metrics scope jm operator](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-scope-jm-operator) | "<host>.jobmanager.<job\_name>.<operator\_name>" | String | Defines the scope format string that is applied to all metrics scoped to the components running on a JobManager of an Operator, like OperatorCoordinator for Source Enumerator metrics. |
| ##### metrics.scope.operator [Anchor link for: metrics scope operator](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-scope-operator) | "<host>.taskmanager.<tm\_id>.<job\_name>.<operator\_name>.<subtask\_index>" | String | Defines the scope format string that is applied to all metrics scoped to an operator. Only effective when a identifier-based reporter is configured |
| ##### metrics.scope.task [Anchor link for: metrics scope task](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-scope-task) | "<host>.taskmanager.<tm\_id>.<job\_name>.<task\_name>.<subtask\_index>" | String | Defines the scope format string that is applied to all metrics scoped to a task. Only effective when a identifier-based reporter is configured |
| ##### metrics.scope.tm [Anchor link for: metrics scope tm](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-scope-tm) | "<host>.taskmanager.<tm\_id>" | String | Defines the scope format string that is applied to all metrics scoped to a TaskManager. Only effective when a identifier-based reporter is configured |
| ##### metrics.scope.tm-job [Anchor link for: metrics scope tm job](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-scope-tm-job) | "<host>.taskmanager.<tm\_id>.<job\_name>" | String | Defines the scope format string that is applied to all metrics scoped to a job on a TaskManager. Only effective when a identifier-based reporter is configured |
| ##### metrics.system-resource [Anchor link for: metrics system resource](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-system-resource) | false | Boolean | Flag indicating whether Flink should report system resource metrics such as machine's CPU, memory or network usage. |
| ##### metrics.system-resource-probing-interval [Anchor link for: metrics system resource probing interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#metrics-system-resource-probing-interval) | 5 s | Duration | Interval between probing of system resource metrics specified. Has an effect only when 'metrics.system-resource' is enabled. |

### RocksDB Native Metrics  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rocksdb-native-metrics)

Flink can report metrics from RocksDB’s native code, for applications using the RocksDB state backend.
The metrics here are scoped to the operators with unsigned longs and have two kinds of types：

1. RocksDB property-based metrics, which is broken down by column family, e.g. number of currently running compactions of one specific column family.
2. RocksDB statistics-based metrics, which holds at the database level, e.g. total block cache hit count within the DB.

> Enabling RocksDB’s native metrics may cause degraded performance and should be set carefully.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.rocksdb.metrics.actual-delayed-write-rate [Anchor link for: state backend rocksdb metrics actual delayed write rate](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-actual-delayed-write-rate) | false | Boolean | Monitor the current actual delayed write rate. 0 means no delay. |
| ##### state.backend.rocksdb.metrics.background-errors [Anchor link for: state backend rocksdb metrics background errors](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-background-errors) | false | Boolean | Monitor the number of background errors in RocksDB. |
| ##### state.backend.rocksdb.metrics.block-cache-capacity [Anchor link for: state backend rocksdb metrics block cache capacity](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-block-cache-capacity) | false | Boolean | Monitor block cache capacity. |
| ##### state.backend.rocksdb.metrics.block-cache-hit [Anchor link for: state backend rocksdb metrics block cache hit](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-block-cache-hit) | false | Boolean | Monitor the total count of block cache hit in RocksDB (BLOCK\_CACHE\_HIT == BLOCK\_CACHE\_INDEX\_HIT + BLOCK\_CACHE\_FILTER\_HIT + BLOCK\_CACHE\_DATA\_HIT). |
| ##### state.backend.rocksdb.metrics.block-cache-miss [Anchor link for: state backend rocksdb metrics block cache miss](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-block-cache-miss) | false | Boolean | Monitor the total count of block cache misses in RocksDB (BLOCK\_CACHE\_MISS == BLOCK\_CACHE\_INDEX\_MISS + BLOCK\_CACHE\_FILTER\_MISS + BLOCK\_CACHE\_DATA\_MISS). |
| ##### state.backend.rocksdb.metrics.block-cache-pinned-usage [Anchor link for: state backend rocksdb metrics block cache pinned usage](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-block-cache-pinned-usage) | false | Boolean | Monitor the memory size for the entries being pinned in block cache. |
| ##### state.backend.rocksdb.metrics.block-cache-usage [Anchor link for: state backend rocksdb metrics block cache usage](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-block-cache-usage) | false | Boolean | Monitor the memory size for the entries residing in block cache. |
| ##### state.backend.rocksdb.metrics.bloom-filter-full-positive [Anchor link for: state backend rocksdb metrics bloom filter full positive](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-bloom-filter-full-positive) | false | Boolean | Monitor the total count of reads not avoided by bloom full filter. |
| ##### state.backend.rocksdb.metrics.bloom-filter-full-true-positive [Anchor link for: state backend rocksdb metrics bloom filter full true positive](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-bloom-filter-full-true-positive) | false | Boolean | Monitor the total count of reads not avoided by bloom full filter and the data actually exists in RocksDB. |
| ##### state.backend.rocksdb.metrics.bloom-filter-useful [Anchor link for: state backend rocksdb metrics bloom filter useful](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-bloom-filter-useful) | false | Boolean | Monitor the total count of reads avoided by bloom filter. |
| ##### state.backend.rocksdb.metrics.bytes-read [Anchor link for: state backend rocksdb metrics bytes read](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-bytes-read) | false | Boolean | Monitor the number of uncompressed bytes read (from memtables/cache/sst) from Get() operation in RocksDB. |
| ##### state.backend.rocksdb.metrics.bytes-written [Anchor link for: state backend rocksdb metrics bytes written](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-bytes-written) | false | Boolean | Monitor the number of uncompressed bytes written by DB::{Put(), Delete(), Merge(), Write()} operations, which does not include the compaction written bytes, in RocksDB. |
| ##### state.backend.rocksdb.metrics.column-family-as-variable [Anchor link for: state backend rocksdb metrics column family as variable](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-column-family-as-variable) | false | Boolean | Whether to expose the column family as a variable for RocksDB property based metrics. |
| ##### state.backend.rocksdb.metrics.compaction-pending [Anchor link for: state backend rocksdb metrics compaction pending](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-compaction-pending) | false | Boolean | Track pending compactions in RocksDB. Returns 1 if a compaction is pending, 0 otherwise. |
| ##### state.backend.rocksdb.metrics.compaction-read-bytes [Anchor link for: state backend rocksdb metrics compaction read bytes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-compaction-read-bytes) | false | Boolean | Monitor the bytes read during compaction in RocksDB. |
| ##### state.backend.rocksdb.metrics.compaction-write-bytes [Anchor link for: state backend rocksdb metrics compaction write bytes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-compaction-write-bytes) | false | Boolean | Monitor the bytes written during compaction in RocksDB. |
| ##### state.backend.rocksdb.metrics.cur-size-active-mem-table [Anchor link for: state backend rocksdb metrics cur size active mem table](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-cur-size-active-mem-table) | false | Boolean | Monitor the approximate size of the active memtable in bytes. |
| ##### state.backend.rocksdb.metrics.cur-size-all-mem-tables [Anchor link for: state backend rocksdb metrics cur size all mem tables](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-cur-size-all-mem-tables) | false | Boolean | Monitor the approximate size of the active and unflushed immutable memtables in bytes. |
| ##### state.backend.rocksdb.metrics.estimate-live-data-size [Anchor link for: state backend rocksdb metrics estimate live data size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-estimate-live-data-size) | false | Boolean | Estimate of the amount of live data in bytes (usually smaller than sst files size due to space amplification). |
| ##### state.backend.rocksdb.metrics.estimate-num-keys [Anchor link for: state backend rocksdb metrics estimate num keys](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-estimate-num-keys) | false | Boolean | Estimate the number of keys in RocksDB. |
| ##### state.backend.rocksdb.metrics.estimate-pending-compaction-bytes [Anchor link for: state backend rocksdb metrics estimate pending compaction bytes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-estimate-pending-compaction-bytes) | false | Boolean | Estimated total number of bytes compaction needs to rewrite to get all levels down to under target size. Not valid for other compactions than level-based. |
| ##### state.backend.rocksdb.metrics.estimate-table-readers-mem [Anchor link for: state backend rocksdb metrics estimate table readers mem](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-estimate-table-readers-mem) | false | Boolean | Estimate the memory used for reading SST tables, excluding memory used in block cache (e.g.,filter and index blocks) in bytes. |
| ##### state.backend.rocksdb.metrics.is-write-stopped [Anchor link for: state backend rocksdb metrics is write stopped](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-is-write-stopped) | false | Boolean | Track whether write has been stopped in RocksDB. Returns 1 if write has been stopped, 0 otherwise. |
| ##### state.backend.rocksdb.metrics.iter-bytes-read [Anchor link for: state backend rocksdb metrics iter bytes read](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-iter-bytes-read) | false | Boolean | Monitor the number of uncompressed bytes read (from memtables/cache/sst) from an iterator operation in RocksDB. |
| ##### state.backend.rocksdb.metrics.live-sst-files-size [Anchor link for: state backend rocksdb metrics live sst files size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-live-sst-files-size) | false | Boolean | Monitor the total size (bytes) of all SST files belonging to the latest version.WARNING: may slow down online queries if there are too many files. |
| ##### state.backend.rocksdb.metrics.mem-table-flush-pending [Anchor link for: state backend rocksdb metrics mem table flush pending](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-mem-table-flush-pending) | false | Boolean | Monitor the number of pending memtable flushes in RocksDB. |
| ##### state.backend.rocksdb.metrics.num-deletes-active-mem-table [Anchor link for: state backend rocksdb metrics num deletes active mem table](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-deletes-active-mem-table) | false | Boolean | Monitor the total number of delete entries in the active memtable. |
| ##### state.backend.rocksdb.metrics.num-deletes-imm-mem-tables [Anchor link for: state backend rocksdb metrics num deletes imm mem tables](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-deletes-imm-mem-tables) | false | Boolean | Monitor the total number of delete entries in the unflushed immutable memtables. |
| ##### state.backend.rocksdb.metrics.num-entries-active-mem-table [Anchor link for: state backend rocksdb metrics num entries active mem table](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-entries-active-mem-table) | false | Boolean | Monitor the total number of entries in the active memtable. |
| ##### state.backend.rocksdb.metrics.num-entries-imm-mem-tables [Anchor link for: state backend rocksdb metrics num entries imm mem tables](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-entries-imm-mem-tables) | false | Boolean | Monitor the total number of entries in the unflushed immutable memtables. |
| ##### state.backend.rocksdb.metrics.num-files-at-level [Anchor link for: state backend rocksdb metrics num files at level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-files-at-level) | false | Boolean | Monitor the number of files at each level. |
| ##### state.backend.rocksdb.metrics.num-immutable-mem-table [Anchor link for: state backend rocksdb metrics num immutable mem table](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-immutable-mem-table) | false | Boolean | Monitor the number of immutable memtables in RocksDB. |
| ##### state.backend.rocksdb.metrics.num-live-versions [Anchor link for: state backend rocksdb metrics num live versions](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-live-versions) | false | Boolean | Monitor number of live versions. Version is an internal data structure. See RocksDB file version\_set.h for details. More live versions often mean more SST files are held from being deleted, by iterators or unfinished compactions. |
| ##### state.backend.rocksdb.metrics.num-running-compactions [Anchor link for: state backend rocksdb metrics num running compactions](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-running-compactions) | false | Boolean | Monitor the number of currently running compactions. |
| ##### state.backend.rocksdb.metrics.num-running-flushes [Anchor link for: state backend rocksdb metrics num running flushes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-running-flushes) | false | Boolean | Monitor the number of currently running flushes. |
| ##### state.backend.rocksdb.metrics.num-snapshots [Anchor link for: state backend rocksdb metrics num snapshots](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-num-snapshots) | false | Boolean | Monitor the number of unreleased snapshots of the database. |
| ##### state.backend.rocksdb.metrics.size-all-mem-tables [Anchor link for: state backend rocksdb metrics size all mem tables](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-size-all-mem-tables) | false | Boolean | Monitor the approximate size of the active, unflushed immutable, and pinned immutable memtables in bytes. |
| ##### state.backend.rocksdb.metrics.stall-micros [Anchor link for: state backend rocksdb metrics stall micros](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-stall-micros) | false | Boolean | Monitor the duration of writer requiring to wait for compaction or flush to finish in RocksDB. |
| ##### state.backend.rocksdb.metrics.total-sst-files-size [Anchor link for: state backend rocksdb metrics total sst files size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-metrics-total-sst-files-size) | false | Boolean | Monitor the total size (bytes) of all SST files of all versions.WARNING: may slow down online queries if there are too many files. |

### ForSt Native Metrics  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#forst-native-metrics)

ForSt has similar native metric mechanism to RocksDB.

> Enabling ForSt’s native metrics may cause degraded performance and should be set carefully.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.forst.metrics.actual-delayed-write-rate [Anchor link for: state backend forst metrics actual delayed write rate](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-actual-delayed-write-rate) | false | Boolean | Monitor the current actual delayed write rate. 0 means no delay. |
| ##### state.backend.forst.metrics.background-errors [Anchor link for: state backend forst metrics background errors](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-background-errors) | false | Boolean | Monitor the number of background errors in ForSt. |
| ##### state.backend.forst.metrics.block-cache-capacity [Anchor link for: state backend forst metrics block cache capacity](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-block-cache-capacity) | false | Boolean | Monitor block cache capacity. |
| ##### state.backend.forst.metrics.block-cache-hit [Anchor link for: state backend forst metrics block cache hit](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-block-cache-hit) | false | Boolean | Monitor the total count of block cache hit in ForSt (BLOCK\_CACHE\_HIT == BLOCK\_CACHE\_INDEX\_HIT + BLOCK\_CACHE\_FILTER\_HIT + BLOCK\_CACHE\_DATA\_HIT). |
| ##### state.backend.forst.metrics.block-cache-miss [Anchor link for: state backend forst metrics block cache miss](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-block-cache-miss) | false | Boolean | Monitor the total count of block cache misses in ForSt (BLOCK\_CACHE\_MISS == BLOCK\_CACHE\_INDEX\_MISS + BLOCK\_CACHE\_FILTER\_MISS + BLOCK\_CACHE\_DATA\_MISS). |
| ##### state.backend.forst.metrics.block-cache-pinned-usage [Anchor link for: state backend forst metrics block cache pinned usage](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-block-cache-pinned-usage) | false | Boolean | Monitor the memory size for the entries being pinned in block cache. |
| ##### state.backend.forst.metrics.block-cache-usage [Anchor link for: state backend forst metrics block cache usage](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-block-cache-usage) | false | Boolean | Monitor the memory size for the entries residing in block cache. |
| ##### state.backend.forst.metrics.bloom-filter-full-positive [Anchor link for: state backend forst metrics bloom filter full positive](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-bloom-filter-full-positive) | false | Boolean | Monitor the total count of reads not avoided by bloom full filter. |
| ##### state.backend.forst.metrics.bloom-filter-full-true-positive [Anchor link for: state backend forst metrics bloom filter full true positive](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-bloom-filter-full-true-positive) | false | Boolean | Monitor the total count of reads not avoided by bloom full filter and the data actually exists in ForSt. |
| ##### state.backend.forst.metrics.bloom-filter-useful [Anchor link for: state backend forst metrics bloom filter useful](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-bloom-filter-useful) | false | Boolean | Monitor the total count of reads avoided by bloom filter. |
| ##### state.backend.forst.metrics.bytes-read [Anchor link for: state backend forst metrics bytes read](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-bytes-read) | false | Boolean | Monitor the number of uncompressed bytes read (from memtables/cache/sst) from Get() operation in ForSt. |
| ##### state.backend.forst.metrics.bytes-written [Anchor link for: state backend forst metrics bytes written](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-bytes-written) | false | Boolean | Monitor the number of uncompressed bytes written by DB::{Put(), Delete(), Merge(), Write()} operations, which does not include the compaction written bytes, in ForSt. |
| ##### state.backend.forst.metrics.column-family-as-variable [Anchor link for: state backend forst metrics column family as variable](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-column-family-as-variable) | false | Boolean | Whether to expose the column family as a variable for ForSt property based metrics. |
| ##### state.backend.forst.metrics.compaction-pending [Anchor link for: state backend forst metrics compaction pending](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-compaction-pending) | false | Boolean | Track pending compactions in ForSt. Returns 1 if a compaction is pending, 0 otherwise. |
| ##### state.backend.forst.metrics.compaction-read-bytes [Anchor link for: state backend forst metrics compaction read bytes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-compaction-read-bytes) | false | Boolean | Monitor the bytes read during compaction in ForSt. |
| ##### state.backend.forst.metrics.compaction-write-bytes [Anchor link for: state backend forst metrics compaction write bytes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-compaction-write-bytes) | false | Boolean | Monitor the bytes written during compaction in ForSt. |
| ##### state.backend.forst.metrics.cur-size-active-mem-table [Anchor link for: state backend forst metrics cur size active mem table](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-cur-size-active-mem-table) | false | Boolean | Monitor the approximate size of the active memtable in bytes. |
| ##### state.backend.forst.metrics.cur-size-all-mem-tables [Anchor link for: state backend forst metrics cur size all mem tables](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-cur-size-all-mem-tables) | false | Boolean | Monitor the approximate size of the active and unflushed immutable memtables in bytes. |
| ##### state.backend.forst.metrics.estimate-live-data-size [Anchor link for: state backend forst metrics estimate live data size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-estimate-live-data-size) | false | Boolean | Estimate of the amount of live data in bytes (usually smaller than sst files size due to space amplification). |
| ##### state.backend.forst.metrics.estimate-num-keys [Anchor link for: state backend forst metrics estimate num keys](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-estimate-num-keys) | false | Boolean | Estimate the number of keys in ForSt. |
| ##### state.backend.forst.metrics.estimate-pending-compaction-bytes [Anchor link for: state backend forst metrics estimate pending compaction bytes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-estimate-pending-compaction-bytes) | false | Boolean | Estimated total number of bytes compaction needed to rewrite to get all levels down to under target size. Not valid for other compactions than level-based. |
| ##### state.backend.forst.metrics.estimate-table-readers-mem [Anchor link for: state backend forst metrics estimate table readers mem](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-estimate-table-readers-mem) | false | Boolean | Estimate the memory used for reading SST tables, excluding memory used in block cache (e.g. filter and index blocks) in bytes. |
| ##### state.backend.forst.metrics.is-write-stopped [Anchor link for: state backend forst metrics is write stopped](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-is-write-stopped) | false | Boolean | Track whether write has been stopped in ForSt. The metric will return 1 if write has been stopped, 0 otherwise. |
| ##### state.backend.forst.metrics.iter-bytes-read [Anchor link for: state backend forst metrics iter bytes read](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-iter-bytes-read) | false | Boolean | Monitor the number of uncompressed bytes read (from memtables/cache/sst) from an iterator operation in ForSt. |
| ##### state.backend.forst.metrics.live-sst-files-size [Anchor link for: state backend forst metrics live sst files size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-live-sst-files-size) | false | Boolean | Monitor the total size (bytes) of all SST files belonging to the latest version. WARNING: may slow down online queries if there are too many files. |
| ##### state.backend.forst.metrics.mem-table-flush-pending [Anchor link for: state backend forst metrics mem table flush pending](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-mem-table-flush-pending) | false | Boolean | Monitor the number of pending memtable flushes in ForSt. |
| ##### state.backend.forst.metrics.num-deletes-active-mem-table [Anchor link for: state backend forst metrics num deletes active mem table](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-deletes-active-mem-table) | false | Boolean | Monitor the total number of delete entries in the active memtable. |
| ##### state.backend.forst.metrics.num-deletes-imm-mem-tables [Anchor link for: state backend forst metrics num deletes imm mem tables](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-deletes-imm-mem-tables) | false | Boolean | Monitor the total number of delete entries in the unflushed immutable memtables. |
| ##### state.backend.forst.metrics.num-entries-active-mem-table [Anchor link for: state backend forst metrics num entries active mem table](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-entries-active-mem-table) | false | Boolean | Monitor the total number of entries in the active memtable. |
| ##### state.backend.forst.metrics.num-entries-imm-mem-tables [Anchor link for: state backend forst metrics num entries imm mem tables](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-entries-imm-mem-tables) | false | Boolean | Monitor the total number of entries in the unflushed immutable memtables. |
| ##### state.backend.forst.metrics.num-files-at-level [Anchor link for: state backend forst metrics num files at level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-files-at-level) | false | Boolean | Monitor the number of files at each level. |
| ##### state.backend.forst.metrics.num-immutable-mem-table [Anchor link for: state backend forst metrics num immutable mem table](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-immutable-mem-table) | false | Boolean | Monitor the number of immutable memtables in ForSt. |
| ##### state.backend.forst.metrics.num-live-versions [Anchor link for: state backend forst metrics num live versions](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-live-versions) | false | Boolean | Monitor number of live versions. Version is an internal data structure. See ForSt file version\_set.h for details. More live versions often mean more SST files are held from being deleted, by iterators or unfinished compactions. |
| ##### state.backend.forst.metrics.num-running-compactions [Anchor link for: state backend forst metrics num running compactions](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-running-compactions) | false | Boolean | Monitor the number of currently running compactions. |
| ##### state.backend.forst.metrics.num-running-flushes [Anchor link for: state backend forst metrics num running flushes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-running-flushes) | false | Boolean | Monitor the number of currently running flushes. |
| ##### state.backend.forst.metrics.num-snapshots [Anchor link for: state backend forst metrics num snapshots](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-num-snapshots) | false | Boolean | Monitor the number of unfinished snapshots of the ForSt. |
| ##### state.backend.forst.metrics.size-all-mem-tables [Anchor link for: state backend forst metrics size all mem tables](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-size-all-mem-tables) | false | Boolean | Monitor the approximate size of the active, unflushed immutable, and pinned immutable memtables in bytes. |
| ##### state.backend.forst.metrics.stall-micros [Anchor link for: state backend forst metrics stall micros](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-stall-micros) | false | Boolean | Monitor the writer wait duration for compaction or flush to finish in ForSt. |
| ##### state.backend.forst.metrics.total-sst-files-size [Anchor link for: state backend forst metrics total sst files size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-metrics-total-sst-files-size) | false | Boolean | Monitor the total size (bytes) of all SST files of all versions. WARNING: may slow down online queries if there are too many files. |

* * *

* * *

# Traces  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces)

Please refer to the [tracing system documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/traces/) for background on Flink’s tracing infrastructure.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### traces.checkpoint.span-detail-level [Anchor link for: traces checkpoint span detail level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-checkpoint-span-detail-level) | SPAN\_PER\_CHECKPOINT | Enum | Detail level for reporting checkpoint spans. Possible values:<br>- '`SPAN_PER_CHECKPOINT`' (default): Single span per checkpoint. Aggregated sum/max for sub-metrics from all tasks and subtasks per checkpoint.<br>- '`SPAN_PER_CHECKPOINT_WITH_TASKS`': Single span per checkpoint. Same as '`SPAN_PER_CHECKPOINT`', plus arrays of aggregated values per task.<br>- '`CHILDREN_SPANS_PER_TASK`': Same as '`SPAN_PER_CHECKPOINT`' plus children spans per each task. Each task span with aggregated sum/max sub-metrics from subtasks.<br>- '`CHILDREN_SPANS_PER_SUBTASK`': Same as '`CHILDREN_SPANS_PER_TASK`' plus children spans per each subtask. Child spans for tasks and grand-child spans for subtasks.<br>Possible values:<br>- "SPAN\_PER\_CHECKPOINT"<br>- "SPAN\_PER\_CHECKPOINT\_WITH\_TASKS"<br>- "CHILDREN\_SPANS\_PER\_TASK"<br>- "CHILDREN\_SPANS\_PER\_SUBTASK" |
| ##### traces.reporter.<name>.<parameter> [Anchor link for: traces reporter <name> <parameter>](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-reporter-%3Cname%3E-%3Cparameter%3E) | (none) | String | Configures the parameter <parameter> for the reporter named <name>. |
| ##### traces.reporter.<name>.factory.class [Anchor link for: traces reporter <name> factory class](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-reporter-%3Cname%3E-factory-class) | (none) | String | The reporter factory class to use for the reporter named <name>. |
| ##### traces.reporter.<name>.filter.excludes [Anchor link for: traces reporter <name> filter excludes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-reporter-%3Cname%3E-filter-excludes) |  | List<String> | The spans that should be excluded for the reporter named <name>. The format is identical to `filter.includes` |
| ##### traces.reporter.<name>.filter.includes [Anchor link for: traces reporter <name> filter includes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-reporter-%3Cname%3E-filter-includes) | "\*:\*:\*" | List<String> | The spans that should be included for the reporter named <name>. Filters are specified as a list, with each filter following this format:<br>`<scope>[:<name>[,<name>]]`<br>A span matches a filter if the scope pattern and at least one of the name patterns match.<br>- scope: Filters based on the logical scope.<br>  <br>  Specified as a pattern where `*` matches any sequence of characters and `.` separates scope components.<br>  <br>  <br>  <br>  For example:<br>  <br>   "`jobmanager.job`" matches any job-related spans on the JobManager,<br>  <br>   "`*.job`" matches all job-related spans and<br>  <br>   "`*.job.*`" matches all spans below the job-level (i.e., task/operator spans etc.).<br>  <br>- name: Filters based on the span name.<br>  <br>  Specified as a comma-separate list of patterns where `*` matches any sequence of characters.<br>  <br>  <br>  <br>  For example, "`*Records*,*Bytes*`" matches any span where the name contains `"Records" or "Bytes"`. |
| ##### traces.reporter.<name>.scope.delimiter [Anchor link for: traces reporter <name> scope delimiter](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-reporter-%3Cname%3E-scope-delimiter) | "." | String | The delimiter used to assemble the metric identifier for the reporter named <name>. |
| ##### traces.reporter.<name>.scope.variables.additional [Anchor link for: traces reporter <name> scope variables additional](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-reporter-%3Cname%3E-scope-variables-additional) |  | Map | The map of additional variables that should be included for the reporter named <name>. |
| ##### traces.reporter.<name>.scope.variables.excludes [Anchor link for: traces reporter <name> scope variables excludes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-reporter-%3Cname%3E-scope-variables-excludes) | "." | String | The set of variables that should be excluded for the reporter named <name>. Only applicable to tag-based reporters. |
| ##### traces.reporters [Anchor link for: traces reporters](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#traces-reporters) | (none) | String | An optional list of trace reporter names. If configured, only reporters whose name matches any of the names in the list will be started. Otherwise, all reporters that could be found in the configuration will be started. |

* * *

* * *

# History Server  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#history-server)

The history server keeps the information of completed jobs (graphs, runtimes, statistics). To enable it, you have to enable “job archiving” in the JobManager (`jobmanager.archive.fs.dir`).

See the [History Server Docs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/advanced/historyserver/) for details.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### historyserver.archive.clean-expired-jobs [Anchor link for: historyserver archive clean expired jobs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-archive-clean-expired-jobs) | false | Boolean | Whether HistoryServer should cleanup jobs that are no longer present \`historyserver.archive.fs.dir\`. |
| ##### historyserver.archive.fs.dir [Anchor link for: historyserver archive fs dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-archive-fs-dir) | (none) | String | Comma separated list of directories to fetch archived jobs from. The history server will monitor these directories for archived jobs. You can configure the JobManager to archive jobs to a directory via \`jobmanager.archive.fs.dir\`. |
| ##### historyserver.archive.fs.refresh-interval [Anchor link for: historyserver archive fs refresh interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-archive-fs-refresh-interval) | 10 s | Duration | Interval for refreshing the archived job directories. |
| ##### historyserver.archive.retained-jobs [Anchor link for: historyserver archive retained jobs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-archive-retained-jobs) | -1 | Integer | The maximum number of jobs to retain in each archive directory defined by `historyserver.archive.fs.dir`. <br>- If the option is not specified as a positive number without specifying `historyserver.archive.retained-ttl`, all of the jobs archives will be retained. <br>- If the option is specified as a positive number without specifying a value of `historyserver.archive.retained-ttl`, the jobs archive whose order index based modification time is equals to or less than the value will be retained. <br>- If this option is specified as a positive number together with the specified `historyserver.archive.retained-ttl` option, the job archive will be removed if its TTL has expired or the retained job count has been reached. <br>If set to `0` or less than `-1`, HistoryServer will throw an `IllegalConfigurationException`. <br>Note, when there are multiple history server instances, two recommended approaches when using this option are: <br>- Specify the option in only one HistoryServer instance to avoid errors caused by multiple instances simultaneously cleaning up remote files, <br>- Or you can keep the value of this configuration consistent across them. |
| ##### historyserver.archive.retained-ttl [Anchor link for: historyserver archive retained ttl](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-archive-retained-ttl) | (none) | Duration | The time-to-live duration to retain the jobs archived in each archive directory defined by `historyserver.archive.fs.dir`. <br>- If the option is not specified without specifying `historyserver.archive.retained-jobs`, all of the jobs archives will be retained. <br>- If the option is specified without specifying `historyserver.archive.retained-jobs`, the jobs archive whose modification time in the time-to-live duration will be retained. <br>- If this option is specified as a positive time duration together with the `historyserver.archive.retained-jobs` option, the job archive will be removed if its TTL has expired or the retained job count has been reached. <br>If set to equal to or less than `0` milliseconds, HistoryServer will throw an `IllegalConfigurationException`. <br>Note, when there are multiple history server instances, two recommended approaches when using this option are: <br>- Specify the option in only one HistoryServer instance to avoid errors caused by multiple instances simultaneously cleaning up remote files, <br>- Or you can keep the value of this configuration consistent across them. |
| ##### historyserver.log.jobmanager.url-pattern [Anchor link for: historyserver log jobmanager url pattern](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-log-jobmanager-url-pattern) | (none) | String | Pattern of the log URL of JobManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, \`<jobid>\`, to the id of job. Only http / https schemes are supported. |
| ##### historyserver.log.taskmanager.url-pattern [Anchor link for: historyserver log taskmanager url pattern](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-log-taskmanager-url-pattern) | (none) | String | Pattern of the log URL of TaskManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, \`<jobid>\` and \`<tmid>\`, to the id of job and TaskManager respectively. Only http / https schemes are supported. |
| ##### historyserver.web.address [Anchor link for: historyserver web address](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-web-address) | (none) | String | Address of the HistoryServer's web interface. |
| ##### historyserver.web.port [Anchor link for: historyserver web port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-web-port) | 8082 | Integer | Port of the HistoryServers's web interface. |
| ##### historyserver.web.refresh-interval [Anchor link for: historyserver web refresh interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-web-refresh-interval) | 10 s | Duration | The refresh interval for the HistoryServer web-frontend. |
| ##### historyserver.web.ssl.enabled [Anchor link for: historyserver web ssl enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-web-ssl-enabled) | false | Boolean | Enable HTTPs access to the HistoryServer web frontend. This is applicable only when the global SSL flag security.ssl.enabled is set to true. |
| ##### historyserver.web.tmpdir [Anchor link for: historyserver web tmpdir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#historyserver-web-tmpdir) | (none) | String | Local directory that is used by the history server REST API for temporary files. |

* * *

* * *

# Experimental  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#experimental)

_Options for experimental features in Flink._

# Client  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#client)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### client.heartbeat.interval [Anchor link for: client heartbeat interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#client-heartbeat-interval) | 30 s | Duration | Time interval for job client to report its heartbeat when 'execution.attached' and 'execution.shutdown-on-attached-exit' are both true. Cancel the job if timeout configured by 'client.heartbeat.timeout'. |
| ##### client.heartbeat.timeout [Anchor link for: client heartbeat timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#client-heartbeat-timeout) | 3 min | Duration | Cancel the job if the dispatcher hasn't received the client's heartbeat after timeout when 'execution.attached' and 'execution.shutdown-on-attached-exit' are both true. |
| ##### client.retry-period [Anchor link for: client retry period](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#client-retry-period) | 2 s | Duration | The interval (in ms) between consecutive retries of failed attempts to execute commands through the CLI or Flink's clients, wherever retry is supported (default 2sec). |
| ##### client.timeout [Anchor link for: client timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#client-timeout) | 1 min | Duration | Timeout on the client side. |

* * *

* * *

# User Artifact Management  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#user-artifact-management)

Flink is capable to upload and fetch local user artifacts in Application Mode. An artifact can be the actual job archive, a UDF that is packaged separately, etc.

1. Uploading local artifacts to a DFS is a Kubernetes specific feature, see the [Kubernetes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#kubernetes) section and look for `kubernetes.artifacts.*` prefixed options.
2. Fetching remote artifacts on the deployed application cluster is supported from DFS or an HTTP(S) endpoint.


> **Note:** Artifact Fetching is supported in Standalone Application Mode and Native Kubernetes Application Mode.


| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### user.artifacts.artifact-list [Anchor link for: user artifacts artifact list](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#user-artifacts-artifact-list) | (none) | List<String> | A semicolon-separated list of the additional artifacts to fetch for the job before setting up the application cluster. All given elements have to be valid URIs. Example: s3://sandbox-bucket/format.jar;http://sandbox-server:1234/udf.jar |
| ##### user.artifacts.base-dir [Anchor link for: user artifacts base dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#user-artifacts-base-dir) | "/opt/flink/artifacts" | String | The base dir to put the application job artifacts. |
| ##### user.artifacts.http-headers [Anchor link for: user artifacts http headers](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#user-artifacts-http-headers) | (none) | Map | Custom HTTP header(s) for the HTTP artifact fetcher. The header(s) will be applied when getting the application job artifacts. Expected format: headerKey1:headerValue1,headerKey2:headerValue2. |
| ##### user.artifacts.raw-http-enabled [Anchor link for: user artifacts raw http enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#user-artifacts-raw-http-enabled) | false | Boolean | Enables artifact fetching from raw HTTP endpoints. |

* * *

* * *

# Execution  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### execution.attached [Anchor link for: execution attached](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-attached) | false | Boolean | Specifies if the pipeline is submitted in attached or detached mode. |
| ##### execution.job-listeners [Anchor link for: execution job listeners](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-job-listeners) | (none) | List<String> | Custom JobListeners to be registered with the execution environment. The registered listeners cannot have constructors with arguments. |
| ##### execution.job-status-changed-listeners [Anchor link for: execution job status changed listeners](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-job-status-changed-listeners) | (none) | List<String> | When job is created or its status is changed, Flink will generate job event and notify job status changed listener. |
| ##### execution.program-config.enabled [Anchor link for: execution program config enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-program-config-enabled) | true | Boolean | Determines whether configurations in the user program are allowed. By default, configuration can be set both on a cluster-level (via options) or within the user program (i.e. programmatic via environment setters). If disabled, all configuration must be defined on a cluster-level and programmatic setters in the user program are prohibited.<br>Depending on your deployment mode failing the job might have different implications. Either your client that is trying to submit the job to an external cluster (session cluster deployment) throws the exception or the job manager (application mode deployment).<br>The 'execution.program-config.wildcards' option lists configuration keys that are allowed to be set in user programs regardless of this setting. |
| ##### execution.program-config.wildcards [Anchor link for: execution program config wildcards](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-program-config-wildcards) |  | List<String> | List of configuration keys that are allowed to be set in a user program regardless whether program configuration is enabled or not.<br>Currently changes that are not backed by the Configuration class are always allowed. |
| ##### execution.shutdown-on-application-finish [Anchor link for: execution shutdown on application finish](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-shutdown-on-application-finish) | true | Boolean | Whether a Flink Application cluster should shut down automatically after its application finishes (either successfully or as result of a failure). Has no effect for other deployment modes. |
| ##### execution.shutdown-on-attached-exit [Anchor link for: execution shutdown on attached exit](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-shutdown-on-attached-exit) | false | Boolean | If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C. |
| ##### execution.submit-failed-job-on-application-error [Anchor link for: execution submit failed job on application error](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-submit-failed-job-on-application-error) | false | Boolean | If a failed job should be submitted (in the application mode) when there is an error in the application driver before an actual job submission. This is intended for providing a clean way of reporting failures back to the user and is especially useful in combination with 'execution.shutdown-on-application-finish'. This option only works when the single job submission is enforced ('high-availability.type' is enabled). Please note that this is an experimental option and may be changed in the future. |
| ##### execution.target [Anchor link for: execution target](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-target) | (none) | String | The deployment target for the execution. This can take one of the following values when calling `bin/flink run`:<br>- remote<br>- local<br>- yarn-application<br>- yarn-session<br>- kubernetes-application<br>- kubernetes-session |
| ##### execution.terminate-application-on-any-job-terminated-exceptionally [Anchor link for: execution terminate application on any job terminated exceptiona](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-terminate-application-on-any-job-terminated-exceptiona) | true | Boolean | When it is set to true, the application will complete exceptionally if any job fails or is canceled. When it is set to false, the application will finish after all jobs reach terminal states. |

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### execution.batch-shuffle-mode [Anchor link for: execution batch shuffle mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-shuffle-mode) | ALL\_EXCHANGES\_BLOCKING | Enum | Defines how data is exchanged between tasks in batch 'execution.runtime-mode' if the shuffling behavior has not been set explicitly for an individual exchange.<br>With pipelined exchanges, upstream and downstream tasks run simultaneously. In order to achieve lower latency, a result record is immediately sent to and processed by the downstream task. Thus, the receiver back-pressures the sender. The streaming mode always uses this exchange.<br>With blocking exchanges, upstream and downstream tasks run in stages. Records are persisted to some storage between stages. Downstream tasks then fetch these records after the upstream tasks finished. Such an exchange reduces the resources required to execute the job as it does not need to run upstream and downstream tasks simultaneously.<br>With hybrid exchanges (experimental), downstream tasks can run anytime as long as upstream tasks start running. When given sufficient resources, it can reduce the overall job execution time by running tasks simultaneously. Otherwise, it also allows jobs to be executed with very little resources. It adapts to custom preferences between persisting less data and restarting less tasks on failures, by providing different spilling strategies.<br>Possible values:<br>- "ALL\_EXCHANGES\_PIPELINED": Upstream and downstream tasks run simultaneously. This leads to lower latency and more evenly distributed (but higher) resource usage across tasks.<br>- "ALL\_EXCHANGES\_BLOCKING": Upstream and downstream tasks run subsequently. This reduces the resource usage as downstream tasks are started after upstream tasks finished.<br>- "ALL\_EXCHANGES\_HYBRID\_FULL": Downstream can start running anytime, as long as the upstream has started. This adapts the resource usage to whatever is available. This type will spill all data to disk to support re-consume.<br>- "ALL\_EXCHANGES\_HYBRID\_SELECTIVE": Downstream can start running anytime, as long as the upstream has started. This adapts the resource usage to whatever is available. This type will selective spilling data to reduce disk writes as much as possible. |
| ##### execution.buffer-timeout.enabled [Anchor link for: execution buffer timeout enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-buffer-timeout-enabled) | true | Boolean | If disabled, the config execution.buffer-timeout.interval will not take effect and the flushing will be triggered only when the output buffer is full thus maximizing throughput |
| ##### execution.buffer-timeout.interval [Anchor link for: execution buffer timeout interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-buffer-timeout-interval) | 100 ms | Duration | The maximum time frequency (milliseconds) for the flushing of the output buffers. By default the output buffers flush frequently to provide low latency and to aid smooth developer experience. Setting the parameter can result in three logical modes:<br>- A positive value triggers flushing periodically by that interval<br>- 0 triggers flushing after every record thus minimizing latency<br>- If the config execution.buffer-timeout.enabled is false, trigger flushing only when the output buffer is full thus maximizing throughput |
| ##### execution.checkpointing.snapshot-compression [Anchor link for: execution checkpointing snapshot compression](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-snapshot-compression) | false | Boolean | Tells if we should use compression for the state snapshot data or not |
| ##### execution.runtime-mode [Anchor link for: execution runtime mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-runtime-mode) | STREAMING | Enum | Runtime execution mode of DataStream programs. Among other things, this controls task scheduling, network shuffle behavior, and time semantics.<br>Possible values:<br>- "STREAMING"<br>- "BATCH"<br>- "AUTOMATIC" |
| ##### execution.sort-keyed-partition.memory [Anchor link for: execution sort keyed partition memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-sort-keyed-partition-memory) | 128 mb | MemorySize | Sets the managed memory size for sort partition operator on KeyedPartitionWindowedStream.The memory size is only a weight hint. Thus, it will affect the operator's memory weight within a task, but the actual memory used depends on the running environment. |
| ##### execution.sort-partition.memory [Anchor link for: execution sort partition memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-sort-partition-memory) | 128 mb | MemorySize | Sets the managed memory size for sort partition operator in NonKeyedPartitionWindowedStream.The memory size is only a weight hint. Thus, it will affect the operator's memory weight within a task, but the actual memory used depends on the running environment. |

### Pipeline  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### pipeline.auto-generate-uids [Anchor link for: pipeline auto generate uids](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-auto-generate-uids) | true | Boolean | When auto-generated UIDs are disabled, users are forced to manually specify UIDs on DataStream applications.<br>It is highly recommended that users specify UIDs before deploying to production since they are used to match state in savepoints to operators in a job. Because auto-generated ID's are likely to change when modifying a job, specifying custom IDs allow an application to evolve over time without discarding state. |
| ##### pipeline.auto-watermark-interval [Anchor link for: pipeline auto watermark interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-auto-watermark-interval) | 200 ms | Duration | The interval of the automatic watermark emission. Watermarks are used throughout the streaming system to keep track of the progress of time. They are used, for example, for time based windowing. |
| ##### pipeline.cached-files [Anchor link for: pipeline cached files](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-cached-files) | (none) | List<String> | Files to be registered at the distributed cache under the given name. The files will be accessible from any user-defined function in the (distributed) runtime under a local path. Files may be local files (which will be distributed via BlobServer), or files in a distributed file system. The runtime will copy the files temporarily to a local cache, if needed.<br>Example:<br>`name:file1,path:'file:///tmp/file1';name:file2,path:'hdfs:///tmp/file2'` |
| ##### pipeline.classpaths [Anchor link for: pipeline classpaths](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-classpaths) | (none) | List<String> | A semicolon-separated list of the classpaths to package with the job jars to be sent to the cluster. These have to be valid URLs. |
| ##### pipeline.closure-cleaner-level [Anchor link for: pipeline closure cleaner level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-closure-cleaner-level) | RECURSIVE | Enum | Configures the mode in which the closure cleaner works.<br>Possible values:<br>- "NONE": Disables the closure cleaner completely.<br>- "TOP\_LEVEL": Cleans only the top-level class without recursing into fields.<br>- "RECURSIVE": Cleans all fields recursively. |
| ##### pipeline.force-avro [Anchor link for: pipeline force avro](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-force-avro) | false | Boolean | Forces Flink to use the Apache Avro serializer for POJOs.<br>Important: Make sure to include the `flink-avro` module. |
| ##### pipeline.force-kryo [Anchor link for: pipeline force kryo](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-force-kryo) | false | Boolean | If enabled, forces TypeExtractor to use Kryo serializer for POJOS even though we could analyze as POJO. In some cases this might be preferable. For example, when using interfaces with subclasses that cannot be analyzed as POJO. |
| ##### pipeline.force-kryo-avro [Anchor link for: pipeline force kryo avro](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-force-kryo-avro) | (none) | Boolean | Forces Flink to register avro classes in kryo serializer.<br>Important: Make sure to include the flink-avro module. Otherwise, nothing will be registered. For backward compatibility, the default value is empty to conform to the behavior of the older version. That is, always register avro with kryo, and if flink-avro is not in the class path, register a dummy serializer. In Flink-2.0, we will set the default value to true. |
| ##### pipeline.generic-types [Anchor link for: pipeline generic types](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-generic-types) | true | Boolean | If the use of generic types is disabled, Flink will throw an `UnsupportedOperationException` whenever it encounters a data type that would go through Kryo for serialization.<br>Disabling generic types can be helpful to eagerly find and eliminate the use of types that would go through Kryo serialization during runtime. Rather than checking types individually, using this option will throw exceptions eagerly in the places where generic types are used.<br>We recommend to use this option only during development and pre-production phases, not during actual production use. The application program and/or the input data may be such that new, previously unseen, types occur at some point. In that case, setting this option would cause the program to fail. |
| ##### pipeline.global-job-parameters [Anchor link for: pipeline global job parameters](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-global-job-parameters) | (none) | Map | Register a custom, serializable user configuration object. The configuration can be accessed in operators |
| ##### pipeline.jars [Anchor link for: pipeline jars](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-jars) | (none) | List<String> | A semicolon-separated list of the jars to package with the job jars to be sent to the cluster. These have to be valid paths. |
| ##### pipeline.jobvertex-parallelism-overrides [Anchor link for: pipeline jobvertex parallelism overrides](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-jobvertex-parallelism-overrides) |  | Map | A parallelism override map (jobVertexId -> parallelism) which will be used to update the parallelism of the corresponding job vertices of submitted JobGraphs. |
| ##### pipeline.max-parallelism [Anchor link for: pipeline max parallelism](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-max-parallelism) | -1 | Integer | The program-wide maximum parallelism used for operators which haven't specified a maximum parallelism. The maximum parallelism specifies the upper limit for dynamic scaling and the number of key groups used for partitioned state. Changing the value explicitly when recovery from original job will lead to state incompatibility. Must be less than or equal to 32768. |
| ##### pipeline.name [Anchor link for: pipeline name](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-name) | (none) | String | The job name used for printing and logging. |
| ##### pipeline.object-reuse [Anchor link for: pipeline object reuse](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-object-reuse) | false | Boolean | When enabled objects that Flink internally uses for deserialization and passing data to user-code functions will be reused. Keep in mind that this can lead to bugs when the user-code function of an operation is not aware of this behaviour. |
| ##### pipeline.operator-chaining.chain-operators-with-different-max-parallelism [Anchor link for: pipeline operator chaining chain operators with different max pa](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-operator-chaining-chain-operators-with-different-max-pa) | true | Boolean | Operators with different max parallelism can be chained together. Default behavior may prevent rescaling when the AdaptiveScheduler is used. |
| ##### pipeline.operator-chaining.enabled [Anchor link for: pipeline operator chaining enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-operator-chaining-enabled) | true | Boolean | Operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization. |
| ##### pipeline.serialization-config [Anchor link for: pipeline serialization config](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-serialization-config) | (none) | List<String> | List of pairs of class names and serializer configs to be used. There is a `type` field in the serializer config and each type has its own configuration. Note: only standard YAML config parser is supported, please use "config.yaml" as the config file. The fields involved are:<br>- `type`: the serializer type which could be "pojo", "kryo" or "typeinfo". If the serializer type is "pojo" or "kryo" without field `kryo-type`, it means the data type will use POJO or Kryo serializer directly.<br>- `kryo-type`: the Kryo serializer type which could be "default" or "registered". The Kryo serializer will use the serializer for the data type as default serializers when the kryo-type is "default", and register the data type and its serializer to Kryo serializer when the kryo-type is registered. When the field exists, there must be a field `class` to specify the serializer class name.<br>- `class`: the serializer class name for type "kryo" or "typeinfo". For "kryo", it should be a subclass of `com.esotericsoftware.kryo.Serializer`. For "typeinfo", it should be a subclass of `org.apache.flink.api.common.typeinfo.TypeInfoFactory`.<br>Example:<br>`[org.example.ExampleClass1: {type: pojo}, org.example.ExampleClass2: {type: kryo}, org.example.ExampleClass3: {type: kryo, kryo-type: default, class: org.example.Class3KryoSerializer}, org.example.ExampleClass4: {type: kryo, kryo-type: registered, class: org.example.Class4KryoSerializer}, org.example.ExampleClass5: {type: typeinfo, class: org.example.Class5TypeInfoFactory}]` |
| ##### pipeline.vertex-description-mode [Anchor link for: pipeline vertex description mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-vertex-description-mode) | TREE | Enum | The mode how we organize description of a job vertex.<br>Possible values:<br>- "TREE"<br>- "CASCADING" |
| ##### pipeline.vertex-name-include-index-prefix [Anchor link for: pipeline vertex name include index prefix](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-vertex-name-include-index-prefix) | false | Boolean | Whether name of vertex includes topological index or not. When it is true, the name will have a prefix of index of the vertex, like '\[vertex-0\]Source: source'. It is false by default |
| ##### pipeline.watermark-alignment.allow-unaligned-source-splits [Anchor link for: pipeline watermark alignment allow unaligned source splits](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pipeline-watermark-alignment-allow-unaligned-source-splits) | false | Boolean | If watermark alignment is used, sources with multiple splits will attempt to pause/resume split readers to avoid watermark drift of source splits. However, if split readers don't support pause/resume, an UnsupportedOperationException will be thrown when there is an attempt to pause/resume. To allow use of split readers that don't support pause/resume and, hence, to allow unaligned splits while still using watermark alignment, set this parameter to true. The default value is false. Note: This parameter may be removed in future releases. |

### Checkpointing  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#checkpointing)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### execution.checkpointing.aligned-checkpoint-timeout [Anchor link for: execution checkpointing aligned checkpoint timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-aligned-checkpoint-timeout) | 0 ms | Duration | Only relevant if `execution.checkpointing.unaligned.enabled` is enabled.<br>If timeout is 0, checkpoints will always start unaligned.<br>If timeout has a positive value, checkpoints will start aligned. If during checkpointing, checkpoint start delay exceeds this timeout, alignment will timeout and checkpoint barrier will start working as unaligned checkpoint. |
| ##### execution.checkpointing.checkpoints-after-tasks-finish [Anchor link for: execution checkpointing checkpoints after tasks finish](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-checkpoints-after-tasks-finish) | true | Boolean | Feature toggle for enabling checkpointing even if some of tasks have finished. Before you enable it, please take a look at [the important considerations](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/#checkpointing-with-parts-of-the-graph-finished) |
| ##### execution.checkpointing.cleaner.parallel-mode [Anchor link for: execution checkpointing cleaner parallel mode 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-cleaner-parallel-mode-1) | true | Boolean | Option whether to discard a checkpoint's states in parallel using the ExecutorService passed into the cleaner |
| ##### execution.checkpointing.create-subdir [Anchor link for: execution checkpointing create subdir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-create-subdir) | true | Boolean | Whether to create sub-directories named by job id under the '`execution.checkpointing.dir`' to store the data files and meta data of checkpoints. The default value is true to enable user could run several jobs with the same checkpoint directory at the same time. If this value is set to false, pay attention not to run several jobs with the same directory simultaneously. <br>WARNING: This is an advanced configuration. If set to false, users must ensure that no multiple jobs are run with the same checkpoint directory, and that no files exist other than those necessary for the restoration of the current job when starting a new job. |
| ##### execution.checkpointing.data-inline-threshold [Anchor link for: execution checkpointing data inline threshold](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-data-inline-threshold) | 20 kb | MemorySize | The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB. |
| ##### execution.checkpointing.dir [Anchor link for: execution checkpointing dir 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-dir-1) | (none) | String | The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers). If the 'execution.checkpointing.storage' is set to 'jobmanager', only the meta data of checkpoints will be stored in this directory. |
| ##### execution.checkpointing.externalized-checkpoint-retention [Anchor link for: execution checkpointing externalized checkpoint retention](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-externalized-checkpoint-retention) | NO\_EXTERNALIZED\_CHECKPOINTS | Enum | Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the owning job fails or is suspended (terminating with job status `JobStatus#FAILED` or `JobStatus#SUSPENDED`). In this case, you have to manually clean up the checkpoint state, both the meta data and actual program state.<br>The mode defines how an externalized checkpoint should be cleaned up on job cancellation. If you choose to retain externalized checkpoints on cancellation you have to handle checkpoint clean up manually when you cancel the job as well (terminating with job status `JobStatus#CANCELED`).<br>The target directory for externalized checkpoints is configured via `execution.checkpointing.dir`.<br>Possible values:<br>- "DELETE\_ON\_CANCELLATION": Checkpoint state is only kept when the owning job fails. It is deleted if the job is cancelled.<br>- "RETAIN\_ON\_CANCELLATION": Checkpoint state is kept when the owning job is cancelled or fails.<br>- "NO\_EXTERNALIZED\_CHECKPOINTS": Externalized checkpoints are disabled. |
| ##### execution.checkpointing.file-merging.across-checkpoint-boundary [Anchor link for: execution checkpointing file merging across checkpoint boundary](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-file-merging-across-checkpoint-boundary) | false | Boolean | Only relevant if `execution.checkpointing.file-merging.enabled` is enabled.<br>Whether to allow merging data of multiple checkpoints into one physical file. If this option is set to false, only merge files within checkpoint boundaries. Otherwise, it is possible for the logical files of different checkpoints to share the same physical file. |
| ##### execution.checkpointing.file-merging.enabled [Anchor link for: execution checkpointing file merging enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-file-merging-enabled) | false | Boolean | Whether to enable merging multiple checkpoint files into one, which will greatly reduce the number of small checkpoint files. This is an experimental feature under evaluation, make sure you're aware of the possible effects of enabling it. |
| ##### execution.checkpointing.file-merging.max-file-size [Anchor link for: execution checkpointing file merging max file size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-file-merging-max-file-size) | 32 mb | MemorySize | Max size of a physical file for merged checkpoints. |
| ##### execution.checkpointing.file-merging.max-space-amplification [Anchor link for: execution checkpointing file merging max space amplification](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-file-merging-max-space-amplification) | 2.0 | Float | Space amplification stands for the magnification of the occupied space compared to the amount of valid data. The more space amplification is, the more waste of space will be. This configs a space amplification above which a re-uploading for physical files will be triggered to reclaim space. Any value below 1f means disabling the space control. |
| ##### execution.checkpointing.file-merging.pool-blocking [Anchor link for: execution checkpointing file merging pool blocking](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-file-merging-pool-blocking) | false | Boolean | Whether to use Blocking or Non-Blocking pool for merging physical files. A Non-Blocking pool will always provide usable physical file without blocking. It may create many physical files if poll file frequently. When poll a small file from a Blocking pool, it may be blocked until the file is returned. |
| ##### execution.checkpointing.incremental [Anchor link for: execution checkpointing incremental 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-incremental-1) | false | Boolean | Option whether to create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option. |
| ##### execution.checkpointing.interval [Anchor link for: execution checkpointing interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-interval) | (none) | Duration | Gets the interval in which checkpoints are periodically scheduled.<br>This setting defines the base interval. Checkpoint triggering may be delayed by the settings `execution.checkpointing.max-concurrent-checkpoints`, `execution.checkpointing.min-pause` and `execution.checkpointing.interval-during-backlog` |
| ##### execution.checkpointing.interval-during-backlog [Anchor link for: execution checkpointing interval during backlog](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-interval-during-backlog) | (none) | Duration | If it is not null and any source reports isProcessingBacklog=true, it is the interval in which checkpoints are periodically scheduled.<br>Checkpoint triggering may be delayed by the settings `execution.checkpointing.max-concurrent-checkpoints` and `execution.checkpointing.min-pause`.<br>Note: if it is not null, the value must either be 0, which means the checkpoint is disabled during backlog, or be larger than or equal to execution.checkpointing.interval. |
| ##### execution.checkpointing.local-backup.dirs [Anchor link for: execution checkpointing local backup dirs 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-local-backup-dirs-1) | (none) | String | The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. If not configured it will default to <WORKING\_DIR>/localState. The <WORKING\_DIR> can be configured via `process.taskmanager.working-dir` |
| ##### execution.checkpointing.local-backup.enabled [Anchor link for: execution checkpointing local backup enabled 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-local-backup-enabled-1) | false | Boolean | This option configures local backup for the state backend, which indicates whether to make backup checkpoint on local disk. If not configured, fallback to execution.state-recovery.from-local. By default, local backup is deactivated. Local backup currently only covers keyed state backends (including both the EmbeddedRocksDBStateBackend and the HashMapStateBackend). |
| ##### execution.checkpointing.max-concurrent-checkpoints [Anchor link for: execution checkpointing max concurrent checkpoints](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-max-concurrent-checkpoints) | 1 | Integer | The maximum number of checkpoint attempts that may be in progress at the same time. If this value is n, then no checkpoints will be triggered while n checkpoint attempts are currently in flight. For the next checkpoint to be triggered, one checkpoint attempt would need to finish or expire. |
| ##### execution.checkpointing.min-pause [Anchor link for: execution checkpointing min pause](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-min-pause) | 0 ms | Duration | The minimal pause between checkpointing attempts. This setting defines how soon thecheckpoint coordinator may trigger another checkpoint after it becomes possible to triggeranother checkpoint with respect to the maximum number of concurrent checkpoints(see `execution.checkpointing.max-concurrent-checkpoints`).<br>If the maximum number of concurrent checkpoints is set to one, this setting makes effectively sure that a minimum amount of time passes where no checkpoint is in progress at all. |
| ##### execution.checkpointing.mode [Anchor link for: execution checkpointing mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-mode) | EXACTLY\_ONCE | Enum | The checkpointing mode (exactly-once vs. at-least-once).<br>Possible values:<br>- "EXACTLY\_ONCE"<br>- "AT\_LEAST\_ONCE" |
| ##### execution.checkpointing.num-retained [Anchor link for: execution checkpointing num retained 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-num-retained-1) | 1 | Integer | The maximum number of completed checkpoints to retain. |
| ##### execution.checkpointing.savepoint-dir [Anchor link for: execution checkpointing savepoint dir 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-savepoint-dir-1) | (none) | String | The default directory for savepoints. Used by the state backends that write savepoints to file systems (HashMapStateBackend, EmbeddedRocksDBStateBackend). |
| ##### execution.checkpointing.storage [Anchor link for: execution checkpointing storage 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-storage-1) | (none) | String | The checkpoint storage implementation to be used to checkpoint state.<br>The implementation can be specified either via their shortcut name, or via the class name of a `CheckpointStorageFactory`. If a factory is specified it is instantiated via its zero argument constructor and its `CheckpointStorageFactory#createFromConfig(ReadableConfig, ClassLoader)` method is called.<br>Recognized shortcut names are 'jobmanager' and 'filesystem'.<br>'execution.checkpointing.storage' and 'execution.checkpointing.dir' are usually combined to configure the checkpoint location. By default, the checkpoint meta data and actual program state will be stored in the JobManager's memory directly. When 'execution.checkpointing.storage' is set to 'jobmanager', if 'execution.checkpointing.dir' is configured, the meta data of checkpoints will be persisted to the path specified by 'execution.checkpointing.dir'. Otherwise, the meta data will be stored in the JobManager's memory. When 'execution.checkpointing.storage' is set to 'filesystem', a valid path must be configured to 'execution.checkpointing.dir', and the checkpoint meta data and actual program state will both be persisted to the path. |
| ##### execution.checkpointing.timeout [Anchor link for: execution checkpointing timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-timeout) | 10 min | Duration | The maximum time that a checkpoint may take before being discarded. |
| ##### execution.checkpointing.tolerable-failed-checkpoints [Anchor link for: execution checkpointing tolerable failed checkpoints](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-tolerable-failed-checkpoints) | 0 | Integer | The tolerable checkpoint consecutive failure number. If set to 0, that means we do not tolerance any checkpoint failure. This only applies to the following failure reasons: IOException on the Job Manager, failures in the async phase on the Task Managers and checkpoint expiration due to a timeout. Failures originating from the sync phase on the Task Managers are always forcing failover of an affected task. Other types of checkpoint failures (such as checkpoint being subsumed) are being ignored. |
| ##### execution.checkpointing.unaligned.enabled [Anchor link for: execution checkpointing unaligned enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-unaligned-enabled) | false | Boolean | Enables unaligned checkpoints, which greatly reduce checkpointing times under backpressure.<br>Unaligned checkpoints contain data stored in buffers as part of the checkpoint state, which allows checkpoint barriers to overtake these buffers. Thus, the checkpoint duration becomes independent of the current throughput as checkpoint barriers are effectively not embedded into the stream of data anymore.<br>Unaligned checkpoints can only be enabled if `execution.checkpointing.mode` is `EXACTLY_ONCE` and if `execution.checkpointing.max-concurrent-checkpoints` is 1 |
| ##### execution.checkpointing.unaligned.forced [Anchor link for: execution checkpointing unaligned forced](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-unaligned-forced) | false | Boolean | Forces unaligned checkpoints, particularly allowing them for iterative jobs. |
| ##### execution.checkpointing.unaligned.interruptible-timers.enabled [Anchor link for: execution checkpointing unaligned interruptible timers enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-unaligned-interruptible-timers-enabled) | false | Boolean | Allows unaligned checkpoints to skip timers that are currently being fired. For this feature to be enabled, it must be also supported by the operator. Currently this is supported by all TableStreamOperators and CepOperator. |
| ##### execution.checkpointing.unaligned.max-subtasks-per-channel-state-file [Anchor link for: execution checkpointing unaligned max subtasks per channel state](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-unaligned-max-subtasks-per-channel-state) | 5 | Integer | Defines the maximum number of subtasks that share the same channel state file. It can reduce the number of small files when enable unaligned checkpoint. Each subtask will create a new channel state file when this is configured to 1. |
| ##### execution.checkpointing.unaligned.recover-output-on-downstream.enabled [Anchor link for: execution checkpointing unaligned recover output on downstream e](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-unaligned-recover-output-on-downstream-e) | false | Boolean | Whether recovering output buffers of upstream task on downstream task directly when job restores from the unaligned checkpoint. |
| ##### execution.checkpointing.write-buffer-size [Anchor link for: execution checkpointing write buffer size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-write-buffer-size) | 4096 | Integer | The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'execution.checkpointing.data-inline-threshold'. |

### Recovery  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#recovery)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### execution.state-recovery.claim-mode [Anchor link for: execution state recovery claim mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-state-recovery-claim-mode) | NO\_CLAIM | Enum | Describes the mode how Flink should restore from the given savepoint or retained checkpoint.<br>Possible values:<br>- "CLAIM": Flink will take ownership of the given snapshot. It will clean the snapshot once it is subsumed by newer ones.<br>- "NO\_CLAIM": Flink will not claim ownership of the snapshot files. However it will make sure it does not depend on any artefacts from the restored snapshot. In order to do that, Flink will take the first checkpoint as a full one, which means it might reupload/duplicate files that are part of the restored checkpoint. |
| ##### execution.state-recovery.from-local [Anchor link for: execution state recovery from local](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-state-recovery-from-local) | false | Boolean | This option configures local recovery for the state backend, which indicates whether to recovery from local snapshot.By default, local recovery is deactivated. Local recovery currently only covers keyed state backends (including both the EmbeddedRocksDBStateBackend and the HashMapStateBackend)." |
| ##### execution.state-recovery.ignore-unclaimed-state [Anchor link for: execution state recovery ignore unclaimed state](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-state-recovery-ignore-unclaimed-state) | false | Boolean | Allow to skip savepoint state that cannot be restored. Allow this if you removed an operator from your pipeline after the savepoint was triggered. |
| ##### execution.state-recovery.path [Anchor link for: execution state recovery path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-state-recovery-path) | (none) | String | Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537). |
| ##### execution.state-recovery.without-channel-state.checkpoint-id [Anchor link for: execution state recovery without channel state checkpoint id](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-state-recovery-without-channel-state-checkpoint-id) | -1 | Long | Checkpoint id for which in-flight data should be ignored in case of the recovery from this checkpoint.<br>It is better to keep this value empty until there is explicit needs to restore from the specific checkpoint without in-flight data. |

* * *

* * *

# Debugging & Expert Tuning  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#debugging--expert-tuning)

> The options below here are meant for expert users and for fixing/debugging problems. Most setups should not need to configure these options.

### Class Loading  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#class-loading)

Flink dynamically loads the code for jobs submitted to a session cluster. In addition, Flink tries to hide many dependencies in the classpath from the application. This helps to reduce dependency conflicts between the application code and the dependencies in the classpath.

Please refer to the [Debugging Classloading Docs](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/debugging/debugging_classloading/) for details.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### classloader.check-leaked-classloader [Anchor link for: classloader check leaked classloader](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#classloader-check-leaked-classloader) | true | Boolean | Fails attempts at loading classes if the user classloader of a job is used after it has terminated.<br>This is usually caused by the classloader being leaked by lingering threads or misbehaving libraries, which may also result in the classloader being used by other jobs.<br>This check should only be disabled if such a leak prevents further jobs from running. |
| ##### classloader.fail-on-metaspace-oom-error [Anchor link for: classloader fail on metaspace oom error](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#classloader-fail-on-metaspace-oom-error) | true | Boolean | Fail Flink JVM processes if 'OutOfMemoryError: Metaspace' is thrown while trying to load a user code class. |
| ##### classloader.parent-first-patterns.additional [Anchor link for: classloader parent first patterns additional](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#classloader-parent-first-patterns-additional) |  | List<String> | A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. These patterns are appended to "classloader.parent-first-patterns.default". |
| ##### classloader.parent-first-patterns.default [Anchor link for: classloader parent first patterns default](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#classloader-parent-first-patterns-default) | "java.";"scala.";"org.apache.flink.";"com.esotericsoftware.kryo";"org.apache.hadoop.";"javax.annotation.";"org.xml";"javax.xml";"org.apache.xerces";"org.w3c";"org.rocksdb.";"org.slf4j";"org.apache.log4j";"org.apache.logging";"org.apache.commons.logging";"ch.qos.logback" | List<String> | A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. This setting should generally not be modified. To add another pattern we recommend to use "classloader.parent-first-patterns.additional" instead. |
| ##### classloader.resolve-order [Anchor link for: classloader resolve order](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#classloader-resolve-order) | "child-first" | String | Defines the class resolution strategy when loading classes from user code, meaning whether to first check the user code jar ("child-first") or the application classpath ("parent-first"). The default settings indicate to load classes first from the user code jar, which means that user code jars can include and load different dependencies than Flink uses (transitively). |

### Advanced Options for the debugging  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-options-for-the-debugging)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### jmx.server.port [Anchor link for: jmx server port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jmx-server-port) | (none) | String | The port range for the JMX server to start the registry. The port config can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234". <br>This option overrides metrics.reporter.\*.port option. |

### Advanced Checkpointing Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-checkpointing-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### execution.checkpointing.create-subdir [Anchor link for: execution checkpointing create subdir 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-create-subdir-1) | true | Boolean | Whether to create sub-directories named by job id under the '`execution.checkpointing.dir`' to store the data files and meta data of checkpoints. The default value is true to enable user could run several jobs with the same checkpoint directory at the same time. If this value is set to false, pay attention not to run several jobs with the same directory simultaneously. <br>WARNING: This is an advanced configuration. If set to false, users must ensure that no multiple jobs are run with the same checkpoint directory, and that no files exist other than those necessary for the restoration of the current job when starting a new job. |
| ##### execution.checkpointing.data-inline-threshold [Anchor link for: execution checkpointing data inline threshold 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-data-inline-threshold-1) | 20 kb | MemorySize | The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB. |
| ##### execution.checkpointing.write-buffer-size [Anchor link for: execution checkpointing write buffer size 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-checkpointing-write-buffer-size-1) | 4096 | Integer | The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'execution.checkpointing.data-inline-threshold'. |

### State Latency Tracking Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-latency-tracking-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.latency-track.history-size [Anchor link for: state latency track history size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-latency-track-history-size) | 128 | Integer | Defines the number of measured latencies to maintain at each state access operation. |
| ##### state.latency-track.keyed-state-enabled [Anchor link for: state latency track keyed state enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-latency-track-keyed-state-enabled) | false | Boolean | Whether to track latency of keyed state operations, e.g value state put/get/clear. |
| ##### state.latency-track.sample-interval [Anchor link for: state latency track sample interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-latency-track-sample-interval) | 100 | Integer | The sample interval of latency track once 'state.latency-track.keyed-state-enabled' is enabled. The default value is 100, which means we would track the latency every 100 access requests. |
| ##### state.latency-track.state-name-as-variable [Anchor link for: state latency track state name as variable](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-latency-track-state-name-as-variable) | true | Boolean | Whether to expose state name as a variable if tracking latency. |

### Advanced RocksDB State Backends Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-rocksdb-state-backends-options)

Advanced options to tune RocksDB and RocksDB checkpoints.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.rocksdb.checkpoint.transfer.thread.num [Anchor link for: state backend rocksdb checkpoint transfer thread num](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-checkpoint-transfer-thread-num) | 4 | Integer | The number of threads (per stateful operator) used to transfer (download and upload) files in RocksDBStateBackend.If negative, the common (TM) IO thread pool is used (see cluster.io-pool.size) |
| ##### state.backend.rocksdb.localdir [Anchor link for: state backend rocksdb localdir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-localdir) | (none) | String | The local directory (on the TaskManager) where RocksDB puts its files. Per default, it will be <WORKING\_DIR>/tmp. See `process.taskmanager.working-dir` for more details. |
| ##### state.backend.rocksdb.options-factory [Anchor link for: state backend rocksdb options factory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-options-factory) | (none) | String | The options factory class for users to add customized options in DBOptions and ColumnFamilyOptions for RocksDB. If set, the RocksDB state backend will load the class and apply configs to DBOptions and ColumnFamilyOptions after loading ones from 'RocksDBConfigurableOptions' and pre-defined options. |
| ##### state.backend.rocksdb.predefined-options [Anchor link for: state backend rocksdb predefined options](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-predefined-options) | "DEFAULT" | String | The predefined settings for RocksDB DBOptions and ColumnFamilyOptions by Flink community. Current supported candidate predefined-options are DEFAULT, SPINNING\_DISK\_OPTIMIZED, SPINNING\_DISK\_OPTIMIZED\_HIGH\_MEM or FLASH\_SSD\_OPTIMIZED. Note that user customized options and options from the RocksDBOptionsFactory are applied on top of these predefined ones. |

### Advanced ForSt State Backends Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-forst-state-backends-options)

Advanced options to tune ForSt and ForSt checkpoints.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.forst.cache.lru.access-before-promote [Anchor link for: state backend forst cache lru access before promote](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-cache-lru-access-before-promote) | 6 | Integer | When the number of accesses to a block in cold link reaches this value, the block will be promoted to the head of the LRU list and become a hot link. The evicted file in cache will be reloaded as well. The default value is '5'. |
| ##### state.backend.forst.cache.lru.promote-limit [Anchor link for: state backend forst cache lru promote limit](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-cache-lru-promote-limit) | 3 | Integer | When the number of eviction that a block in hot link is moved to cold link reaches this value, the block will be blocked from being promoted to the head of the LRU list. The default value is '3'. |
| ##### state.backend.forst.executor.inline-coordinator [Anchor link for: state backend forst executor inline coordinator](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-executor-inline-coordinator) | false | Boolean | Whether to let the task thread be the coordinator thread responsible for distributing requests. If set to 'true', the task thread will be responsible for distributing requests, otherwise, a dedicated coordinator thread will be used. The default value is 'false'. |
| ##### state.backend.forst.executor.inline-write [Anchor link for: state backend forst executor inline write](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-executor-inline-write) | true | Boolean | Whether to let write requests be executed within the coordinator thread. If set to 'true', write requests will be executed within the coordinator thread, otherwise, a dedicated write thread will be used. The default value is 'true'. |
| ##### state.backend.forst.local-dir [Anchor link for: state backend forst local dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-local-dir) | (none) | String | The local directory (on the TaskManager) where ForSt puts some metadata files. By default, it will be <WORKING\_DIR>/tmp. See `process.taskmanager.working-dir` for more details. |
| ##### state.backend.forst.memory.fixed-per-slot [Anchor link for: state backend forst memory fixed per slot](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-memory-fixed-per-slot) | (none) | MemorySize | The fixed total amount of memory per slot, shared among all ForSt instances.This option overrides the 'state.backend.forst.memory.managed' option. |
| ##### state.backend.forst.memory.fixed-per-tm [Anchor link for: state backend forst memory fixed per tm](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-memory-fixed-per-tm) | (none) | MemorySize | The fixed total amount of memory per Task Manager, shared among all ForSt instances. This is a cluster-level option. This option only takes effect if 'state.backend.forst.memory.managed' is set to false and 'state.backend.forst.memory.fixed-per-slot' is not configured. If so, then each ForSt column family state has its own memory caches (as controlled by the column family options). The relevant options for the shared resources (e.g. write-buffer-ratio) can be set on the same level (config.yaml). Note that this feature breaks resource isolation between the slots. |
| ##### state.backend.forst.memory.managed [Anchor link for: state backend forst memory managed](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-memory-managed) | true | Boolean | If set true, the ForSt state backend will automatically configure itself to use the managed memory budget of the task slot, and divide the memory over write buffers, indexes, block caches, etc. |
| ##### state.backend.forst.options-factory [Anchor link for: state backend forst options factory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-options-factory) | (none) | String | The options factory class for users to add customized options in DBOptions and ColumnFamilyOptions for ForSt. If set, the ForSt state backend will load the class and apply configs to DBOptions and ColumnFamilyOptions after loading ones from 'ForStConfigurableOptions' and pre-defined options. |

### State Changelog Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-options)

Please refer to [State Backends](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/#enabling-changelog) for information on
using State Changelog.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.changelog.enabled [Anchor link for: state changelog enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-enabled) | false | Boolean | Whether to enable state backend to write state changes to StateChangelog. If this config is not set explicitly, it means no preference for enabling the change log, and the value in lower config level will take effect. The default value 'false' here means if no value set (job or cluster), the change log will not be enabled. |
| ##### state.changelog.max-failures-allowed [Anchor link for: state changelog max failures allowed](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-max-failures-allowed) | 3 | Integer | Max number of consecutive materialization failures allowed. |
| ##### state.changelog.periodic-materialize.enabled [Anchor link for: state changelog periodic materialize enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-periodic-materialize-enabled) | true | Boolean | Defines whether to enable periodic materialization, all changelogs will not be truncated which may increase the space of checkpoint if disabled |
| ##### state.changelog.periodic-materialize.interval [Anchor link for: state changelog periodic materialize interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-periodic-materialize-interval) | 10 min | Duration | Defines the interval in milliseconds to perform periodic materialization for state backend. It only takes effect when state.changelog.periodic-materialize.enabled is true |
| ##### state.changelog.storage [Anchor link for: state changelog storage](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-storage) | "memory" | String | The storage to be used to store state changelog.<br>The implementation can be specified via their shortcut name.<br>The list of recognized shortcut names currently includes 'memory' and 'filesystem'. |

#### FileSystem-based Changelog options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#filesystem-based-changelog-options)

These settings take effect when the `state.changelog.storage` is set to `filesystem` (see [above](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#state-changelog-storage)).

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.changelog.dstl.dfs.base-path [Anchor link for: state changelog dstl dfs base path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-base-path) | (none) | String | Base path to store changelog files. |
| ##### state.changelog.dstl.dfs.batch.persist-delay [Anchor link for: state changelog dstl dfs batch persist delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-batch-persist-delay) | 10 ms | Duration | Delay before persisting changelog after receiving persist request (on checkpoint). Minimizes the number of files and requests if multiple operators (backends) or sub-tasks are using the same store. Correspondingly increases checkpoint time (async phase). |
| ##### state.changelog.dstl.dfs.batch.persist-size-threshold [Anchor link for: state changelog dstl dfs batch persist size threshold](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-batch-persist-size-threshold) | 10 mb | MemorySize | Size threshold for state changes that were requested to be persisted but are waiting for state.changelog.dstl.dfs.batch.persist-delay (from all operators). . Once reached, accumulated changes are persisted immediately. This is different from state.changelog.dstl.dfs.preemptive-persist-threshold as it happens AFTER the checkpoint and potentially for state changes of multiple operators. Must not exceed in-flight data limit (see below) |
| ##### state.changelog.dstl.dfs.compression.enabled [Anchor link for: state changelog dstl dfs compression enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-compression-enabled) | false | Boolean | Whether to enable compression when serializing changelog. |
| ##### state.changelog.dstl.dfs.discard.num-threads [Anchor link for: state changelog dstl dfs discard num threads](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-discard-num-threads) | 1 | Integer | Number of threads to use to discard changelog (e.g. pre-emptively uploaded unused state). |
| ##### state.changelog.dstl.dfs.download.local-cache.idle-timeout-ms [Anchor link for: state changelog dstl dfs download local cache idle timeout ms](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-download-local-cache-idle-timeout-ms) | 10 min | Duration | Maximum idle time for cache files of distributed changelog file, after which the cache files will be deleted. |
| ##### state.changelog.dstl.dfs.preemptive-persist-threshold [Anchor link for: state changelog dstl dfs preemptive persist threshold](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-preemptive-persist-threshold) | 5 mb | MemorySize | Size threshold for state changes of a single operator beyond which they are persisted pre-emptively without waiting for a checkpoint. Improves checkpointing time by allowing quasi-continuous uploading of state changes (as opposed to uploading all accumulated changes on checkpoint). |
| ##### state.changelog.dstl.dfs.upload.buffer-size [Anchor link for: state changelog dstl dfs upload buffer size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-upload-buffer-size) | 1 mb | MemorySize | Buffer size used when uploading change sets |
| ##### state.changelog.dstl.dfs.upload.max-attempts [Anchor link for: state changelog dstl dfs upload max attempts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-upload-max-attempts) | 3 | Integer | Maximum number of attempts (including the initial one) to perform a particular upload. Only takes effect if state.changelog.dstl.dfs.upload.retry-policy is fixed. |
| ##### state.changelog.dstl.dfs.upload.max-in-flight [Anchor link for: state changelog dstl dfs upload max in flight](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-upload-max-in-flight) | 100 mb | MemorySize | Max amount of data allowed to be in-flight. Upon reaching this limit the task will be back-pressured. I.e., snapshotting will block; normal processing will block if state.changelog.dstl.dfs.preemptive-persist-threshold is set and reached. The limit is applied to the total size of in-flight changes if multiple operators/backends are using the same changelog storage. Must be greater than or equal to state.changelog.dstl.dfs.batch.persist-size-threshold |
| ##### state.changelog.dstl.dfs.upload.next-attempt-delay [Anchor link for: state changelog dstl dfs upload next attempt delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-upload-next-attempt-delay) | 500 ms | Duration | Delay before the next attempt (if the failure was not caused by a timeout). |
| ##### state.changelog.dstl.dfs.upload.num-threads [Anchor link for: state changelog dstl dfs upload num threads](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-upload-num-threads) | 5 | Integer | Number of threads to use for upload. |
| ##### state.changelog.dstl.dfs.upload.retry-policy [Anchor link for: state changelog dstl dfs upload retry policy](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-upload-retry-policy) | "fixed" | String | Retry policy for the failed uploads (in particular, timed out). Valid values: none, fixed. |
| ##### state.changelog.dstl.dfs.upload.timeout [Anchor link for: state changelog dstl dfs upload timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-changelog-dstl-dfs-upload-timeout) | 1 s | Duration | Time threshold beyond which an upload is considered timed out. If a new attempt is made but this upload succeeds earlier then this upload result will be used. May improve upload times if tail latencies of upload requests are significantly high. Only takes effect if state.changelog.dstl.dfs.upload.retry-policy is fixed. Please note that timeout \* max\_attempts should be less than execution.checkpointing.timeout |

### RocksDB Configurable Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rocksdb-configurable-options)

These options give fine-grained control over the behavior and resources of ColumnFamilies.
With the introduction of `state.backend.rocksdb.memory.managed` and `state.backend.rocksdb.memory.fixed-per-slot` (Apache Flink 1.10), it should be only necessary to use the options here for advanced performance tuning. These options here can also be specified in the application program via `RocksDBStateBackend.setRocksDBOptions(RocksDBOptionsFactory)`.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.rocksdb.block.blocksize [Anchor link for: state backend rocksdb block blocksize](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-block-blocksize) | 4 kb | MemorySize | The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'. |
| ##### state.backend.rocksdb.block.cache-size [Anchor link for: state backend rocksdb block cache size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-block-cache-size) | 8 mb | MemorySize | The amount of the cache for data blocks in RocksDB. The default block-cache size is '8MB'. |
| ##### state.backend.rocksdb.block.metadata-blocksize [Anchor link for: state backend rocksdb block metadata blocksize](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-block-metadata-blocksize) | 4 kb | MemorySize | Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'. |
| ##### state.backend.rocksdb.bloom-filter.bits-per-key [Anchor link for: state backend rocksdb bloom filter bits per key](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-bloom-filter-bits-per-key) | 10.0 | Double | Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0. |
| ##### state.backend.rocksdb.bloom-filter.block-based-mode [Anchor link for: state backend rocksdb bloom filter block based mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-bloom-filter-block-based-mode) | false | Boolean | If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'. |
| ##### state.backend.rocksdb.compaction.filter.periodic-compaction-time [Anchor link for: state backend rocksdb compaction filter periodic compaction time](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-compaction-filter-periodic-compaction-time) | 30 d | Duration | Periodic compaction could speed up expired state entries cleanup, especially for state entries rarely accessed. Files older than this value will be picked up for compaction, and re-written to the same level as they were before. It makes sure a file goes through compaction filters periodically. 0 means turning off periodic compaction.The default value is '30days'. |
| ##### state.backend.rocksdb.compaction.filter.query-time-after-num-entries [Anchor link for: state backend rocksdb compaction filter query time after num ent](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-compaction-filter-query-time-after-num-ent) | 1000 | Long | Number of state entries to process by compaction filter before updating current timestamp. Updating the timestamp more often can improve cleanup speed, but it decreases compaction performance because it uses JNI calls from native code.The default value is '1000L'. |
| ##### state.backend.rocksdb.compaction.level.max-size-level-base [Anchor link for: state backend rocksdb compaction level max size level base](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-compaction-level-max-size-level-base) | 256 mb | MemorySize | The upper-bound of the total size of level base files in bytes. The default value is '256MB'. |
| ##### state.backend.rocksdb.compaction.level.target-file-size-base [Anchor link for: state backend rocksdb compaction level target file size base](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-compaction-level-target-file-size-base) | 64 mb | MemorySize | The target file size for compaction, which determines a level-1 file size. The default value is '64MB'. |
| ##### state.backend.rocksdb.compaction.level.use-dynamic-size [Anchor link for: state backend rocksdb compaction level use dynamic size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-compaction-level-use-dynamic-size) | false | Boolean | If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max\_bytes\_for\_level\_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to [RocksDB's doc.](https://github.com/facebook/rocksdb/wiki/Leveled-Compaction#level_compaction_dynamic_level_bytes-is-true) |
| ##### state.backend.rocksdb.compaction.style [Anchor link for: state backend rocksdb compaction style](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-compaction-style) | LEVEL | Enum | The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.<br>Possible values:<br>- "LEVEL"<br>- "UNIVERSAL"<br>- "FIFO"<br>- "NONE" |
| ##### state.backend.rocksdb.compression.per.level [Anchor link for: state backend rocksdb compression per level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-compression-per-level) | SNAPPY\_COMPRESSION | List<Enum> | A semicolon-separated list of Compression Type. Different levels can have different compression policies. In many cases, lower levels use fast compression algorithms, while higher levels with more data use slower but more effective compression algorithms. The N th element in the List corresponds to the compression type of the level N-1When `state.backend.rocksdb.compaction.level.use-dynamic-size` is true, compression\_per\_level\[0\] still determines L0, but other elements are based on the base level and may not match the level seen in the info log<br>Note: If the List size is smaller than the level number, the undefined lower level uses the last Compression Type in the List<br>Some commonly used compression algorithms for candidates include `NO_COMPRESSION` ,`SNAPPY_COMPRESSION` and `LZ4_COMPRESSION`<br>The default value is `SNAPPY_COMPRESSION`, which means that all data uses the Snappy compression algorithm.Likewise, if set to `NO_COMPRESSION` , means that all data is not compressed, which will achieve faster speed but will bring some space amplification.In addition, if we need to consider both spatial amplification and performance, we can also set it to '`NO_COMPRESSION`;`NO_COMPRESSION`;`LZ4_COMPRESSION`', which means that L0 and L1 data will not be compressed, and other data will be compressed using LZ4.<br>Possible values:<br>- "NO\_COMPRESSION"<br>- "SNAPPY\_COMPRESSION"<br>- "ZLIB\_COMPRESSION"<br>- "BZLIB2\_COMPRESSION"<br>- "LZ4\_COMPRESSION"<br>- "LZ4HC\_COMPRESSION"<br>- "XPRESS\_COMPRESSION"<br>- "ZSTD\_COMPRESSION"<br>- "DISABLE\_COMPRESSION\_OPTION" |
| ##### state.backend.rocksdb.files.open [Anchor link for: state backend rocksdb files open](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-files-open) | -1 | Integer | The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'. |
| ##### state.backend.rocksdb.incremental-restore-async-compact-after-rescale [Anchor link for: state backend rocksdb incremental restore async compact after re](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-incremental-restore-async-compact-after-re) | false | Boolean | If true, an async compaction of RocksDB is started after every restore after which we detect keys (including tombstones) in the database that are outside the key-groups range of the backend. |
| ##### state.backend.rocksdb.log.dir [Anchor link for: state backend rocksdb log dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-log-dir) | (none) | String | The directory for RocksDB's information logging files. If empty (Flink default setting), log files will be in the same directory as the Flink log. If non-empty, this directory will be used and the data directory's absolute path will be used as the prefix of the log file name. If setting this option as a non-existing location, e.g '/dev/null', RocksDB will then create the log under its own database folder as before. |
| ##### state.backend.rocksdb.log.file-num [Anchor link for: state backend rocksdb log file num](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-log-file-num) | 4 | Integer | The maximum number of files RocksDB should keep for information logging (Default setting: 4). |
| ##### state.backend.rocksdb.log.level [Anchor link for: state backend rocksdb log level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-log-level) | INFO\_LEVEL | Enum | The specified information logging level for RocksDB. If unset, Flink will use `INFO_LEVEL`.<br>Note: RocksDB info logs will not be written to the TaskManager logs and there is no rolling strategy, unless you configure `state.backend.rocksdb.log.dir`, `state.backend.rocksdb.log.max-file-size`, and `state.backend.rocksdb.log.file-num` accordingly. Without a rolling strategy, long-running tasks may lead to uncontrolled disk space usage if configured with increased log levels!<br>There is no need to modify the RocksDB log level, unless for troubleshooting RocksDB.<br>Possible values:<br>- "DEBUG\_LEVEL"<br>- "INFO\_LEVEL"<br>- "WARN\_LEVEL"<br>- "ERROR\_LEVEL"<br>- "FATAL\_LEVEL"<br>- "HEADER\_LEVEL"<br>- "NUM\_INFO\_LOG\_LEVELS" |
| ##### state.backend.rocksdb.log.max-file-size [Anchor link for: state backend rocksdb log max file size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-log-max-file-size) | 25 mb | MemorySize | The maximum size of RocksDB's file used for information logging. If the log files becomes larger than this, a new file will be created. If 0, all logs will be written to one log file. The default maximum file size is '25MB'. |
| ##### state.backend.rocksdb.rescaling.use-delete-files-in-range [Anchor link for: state backend rocksdb rescaling use delete files in range](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-rescaling-use-delete-files-in-range) | false | Boolean | If true, during rescaling, the deleteFilesInRange API will be invoked to clean up the useless files so that local disk space can be reclaimed more promptly. |
| ##### state.backend.rocksdb.restore-overlap-fraction-threshold [Anchor link for: state backend rocksdb restore overlap fraction threshold](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-restore-overlap-fraction-threshold) | 0.0 | Double | The threshold of overlap fraction between the handle's key-group range and target key-group range. When restore base DB, only the handle which overlap fraction greater than or equal to threshold has a chance to be an initial handle. The default value is 0.0, there is always a handle will be selected for initialization. |
| ##### state.backend.rocksdb.thread.num [Anchor link for: state backend rocksdb thread num](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-thread-num) | 2 | Integer | The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'. |
| ##### state.backend.rocksdb.use-bloom-filter [Anchor link for: state backend rocksdb use bloom filter](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-use-bloom-filter) | false | Boolean | If true, every newly created SST file will contain a Bloom filter. It is disabled by default. |
| ##### state.backend.rocksdb.use-ingest-db-restore-mode [Anchor link for: state backend rocksdb use ingest db restore mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-use-ingest-db-restore-mode) | false | Boolean | A recovery mode that directly clips and ingests multiple DBs during state recovery if the keys in the SST files does not exceed the declared key-group range. |
| ##### state.backend.rocksdb.write-batch-size [Anchor link for: state backend rocksdb write batch size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-write-batch-size) | 2 mb | MemorySize | The max size of the consumed memory for RocksDB batch write, will flush just based on item count if this config set to 0. |
| ##### state.backend.rocksdb.writebuffer.count [Anchor link for: state backend rocksdb writebuffer count](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-writebuffer-count) | 2 | Integer | The maximum number of write buffers that are built up in memory. The default value is '2'. |
| ##### state.backend.rocksdb.writebuffer.number-to-merge [Anchor link for: state backend rocksdb writebuffer number to merge](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-writebuffer-number-to-merge) | 1 | Integer | The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'. |
| ##### state.backend.rocksdb.writebuffer.size [Anchor link for: state backend rocksdb writebuffer size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-rocksdb-writebuffer-size) | 64 mb | MemorySize | The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'. |

### ForSt State Backend Configurable Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#forst-state-backend-configurable-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### state.backend.forst.block.blocksize [Anchor link for: state backend forst block blocksize](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-block-blocksize) | 4 kb | MemorySize | The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'. |
| ##### state.backend.forst.block.cache-size [Anchor link for: state backend forst block cache size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-block-cache-size) | 8 mb | MemorySize | The amount of the cache for data blocks in ForSt. The default block-cache size is '8MB'. |
| ##### state.backend.forst.block.metadata-blocksize [Anchor link for: state backend forst block metadata blocksize](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-block-metadata-blocksize) | 4 kb | MemorySize | Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'. |
| ##### state.backend.forst.bloom-filter.bits-per-key [Anchor link for: state backend forst bloom filter bits per key](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-bloom-filter-bits-per-key) | 10.0 | Double | Bits per key that the bloom filter will use, this only takes effect when the bloom filter is used. The default value is 10.0. |
| ##### state.backend.forst.bloom-filter.block-based-mode [Anchor link for: state backend forst bloom filter block based mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-bloom-filter-block-based-mode) | false | Boolean | If set 'true', ForSt will use block-based filter instead of full filter, this only takes effect when bloom filter is used. The default value is 'false'. |
| ##### state.backend.forst.compaction.filter.periodic-compaction-time [Anchor link for: state backend forst compaction filter periodic compaction time](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-compaction-filter-periodic-compaction-time) | 30 d | Duration | Periodic compaction could speed up expired state entries cleanup, especially for state entries rarely accessed. Files older than this value will be picked up for compaction, and re-written to the same level as they were before. It makes sure a file goes through compaction filters periodically. 0 means turning off periodic compaction.The default value is '30 d' (30 days). |
| ##### state.backend.forst.compaction.filter.query-time-after-num-entries [Anchor link for: state backend forst compaction filter query time after num entri](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-compaction-filter-query-time-after-num-entri) | 1000 | Long | Number of state entries to process by compaction filter before updating current timestamp. Updating the timestamp more often can improve cleanup speed, but it decreases compaction performance because it uses JNI calls from native code.The default value is '1000L'. |
| ##### state.backend.forst.compaction.level.max-size-level-base [Anchor link for: state backend forst compaction level max size level base](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-compaction-level-max-size-level-base) | 256 mb | MemorySize | The upper-bound of the total size of level base files in bytes. The default value is '256 mb'. |
| ##### state.backend.forst.compaction.level.target-file-size-base [Anchor link for: state backend forst compaction level target file size base](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-compaction-level-target-file-size-base) | 64 mb | MemorySize | The target file size for compaction, which determines a level-1 file size. The default value is '64 mb'. |
| ##### state.backend.forst.compaction.level.use-dynamic-size [Anchor link for: state backend forst compaction level use dynamic size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-compaction-level-use-dynamic-size) | false | Boolean | If true, ForSt will pick target size of each level dynamically. From an empty key-value store, ForSt would make last level the base level, which means merging L0 data into the last level, until it exceeds max\_bytes\_for\_level\_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to [RocksDB's doc.](https://github.com/facebook/rocksdb/wiki/Leveled-Compaction#level_compaction_dynamic_level_bytes-is-true) |
| ##### state.backend.forst.compaction.style [Anchor link for: state backend forst compaction style](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-compaction-style) | LEVEL | Enum | The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.<br>Possible values:<br>- "LEVEL"<br>- "UNIVERSAL"<br>- "FIFO"<br>- "NONE" |
| ##### state.backend.forst.compression.per.level [Anchor link for: state backend forst compression per level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-compression-per-level) | SNAPPY\_COMPRESSION | List<Enum> | A semicolon-separated list of Compression Type. Different levels can have different compression policies. In many cases, lower levels use fast compression algorithms, while higher levels with more data use slower but more effective compression algorithms. The N th element in the List corresponds to the compression type of the level N-1. When `state.backend.forst.compaction.level.use-dynamic-size` is true, compression\_per\_level\[0\] still determines L0, but other elements are based on the base level and may not match the level seen in the info log.<br>Note: If the List size is smaller than the level number, the undefined lower level uses the last Compression Type in the List.<br>Some commonly used compression algorithms for candidates include `NO_COMPRESSION` ,`SNAPPY_COMPRESSION` and `LZ4_COMPRESSION`.<br>The default value is `SNAPPY_COMPRESSION`, which means that all data uses the Snappy compression algorithm. Likewise, if set to `NO_COMPRESSION` , means that all data is not compressed, which will achieve faster speed but will bring some space amplification. In addition, if we need to consider both spatial amplification and performance, we can also set it to '`NO_COMPRESSION`;`NO_COMPRESSION`;`LZ4_COMPRESSION`', which means that L0 and L1 data will not be compressed, and other data will be compressed using LZ4.<br>Possible values:<br>- "NO\_COMPRESSION"<br>- "SNAPPY\_COMPRESSION"<br>- "ZLIB\_COMPRESSION"<br>- "BZLIB2\_COMPRESSION"<br>- "LZ4\_COMPRESSION"<br>- "LZ4HC\_COMPRESSION"<br>- "XPRESS\_COMPRESSION"<br>- "ZSTD\_COMPRESSION"<br>- "DISABLE\_COMPRESSION\_OPTION" |
| ##### state.backend.forst.files.open [Anchor link for: state backend forst files open](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-files-open) | -1 | Integer | The maximum number of open files (per stateful operator) that can be used by the ForSt, '-1' means no limit. The default value is '-1'. |
| ##### state.backend.forst.log.dir [Anchor link for: state backend forst log dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-log-dir) | (none) | String | The directory for ForSt's information logging files. If empty (Flink default setting), log files will be in the same directory as the Flink log. If non-empty, this directory will be used and the data directory's absolute path will be used as the prefix of the log file name. If setting this option as a non-existing location, e.g '/dev/null', ForSt will then create the log under its own database folder. |
| ##### state.backend.forst.log.file-num [Anchor link for: state backend forst log file num](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-log-file-num) | 4 | Integer | The maximum number of files ForSt should keep for information logging (Default setting: 4). |
| ##### state.backend.forst.log.level [Anchor link for: state backend forst log level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-log-level) | INFO\_LEVEL | Enum | The specified information logging level for ForSt. If unset, Flink will use `INFO_LEVEL`.<br>Note: ForSt info logs will not be written to the TaskManager logs and there is no rolling strategy, unless you configure `state.backend.forst.log.dir`, `state.backend.forst.log.max-file-size`, and `state.backend.forst.log.file-num` accordingly. Without a rolling strategy, long-running tasks may lead to uncontrolled disk space usage if configured with increased log levels!<br>There is no need to modify the ForSt log level, unless for troubleshooting ForSt.<br>Possible values:<br>- "DEBUG\_LEVEL"<br>- "INFO\_LEVEL"<br>- "WARN\_LEVEL"<br>- "ERROR\_LEVEL"<br>- "FATAL\_LEVEL"<br>- "HEADER\_LEVEL"<br>- "NUM\_INFO\_LOG\_LEVELS" |
| ##### state.backend.forst.log.max-file-size [Anchor link for: state backend forst log max file size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-log-max-file-size) | 25 mb | MemorySize | The maximum size of ForSt's file used for information logging. If the log files becomes larger than this, a new file will be created. If 0, all logs will be written to one log file. The default maximum file size is '25MB'. |
| ##### state.backend.forst.rescaling.use-delete-files-in-range [Anchor link for: state backend forst rescaling use delete files in range](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-rescaling-use-delete-files-in-range) | false | Boolean | If true, during rescaling, the deleteFilesInRange API will be invoked to clean up the useless key-values so that primary storage space can be reclaimed more promptly. |
| ##### state.backend.forst.restore-overlap-fraction-threshold [Anchor link for: state backend forst restore overlap fraction threshold](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-restore-overlap-fraction-threshold) | 0.0 | Double | The threshold of overlap fraction between the state handle's key-group range and target key-group range. When restore base DB, only the handle which overlap fraction greater than or equal to threshold has a chance to be an initial handle. The default value is 0.0, there is always a handle will be selected for initialization. |
| ##### state.backend.forst.thread.num [Anchor link for: state backend forst thread num](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-thread-num) | 2 | Integer | The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'. |
| ##### state.backend.forst.use-bloom-filter [Anchor link for: state backend forst use bloom filter](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-use-bloom-filter) | false | Boolean | Whether every newly created SST file will contain a Bloom filter. Default 'false'. |
| ##### state.backend.forst.use-ingest-db-restore-mode [Anchor link for: state backend forst use ingest db restore mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-use-ingest-db-restore-mode) | false | Boolean | A recovery mode that directly clips and ingests multiple DBs during state recovery if the keys in the SST files does not exceed the declared key-group range. |
| ##### state.backend.forst.write-batch-size [Anchor link for: state backend forst write batch size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-write-batch-size) | 2 mb | MemorySize | The max size of the consumed memory for ForSt batch write, will flush just based on item count if this config set to 0. |
| ##### state.backend.forst.writebuffer.count [Anchor link for: state backend forst writebuffer count](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-writebuffer-count) | 2 | Integer | The maximum number of write buffers that are built up in memory. The default value is '2'. |
| ##### state.backend.forst.writebuffer.number-to-merge [Anchor link for: state backend forst writebuffer number to merge](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-writebuffer-number-to-merge) | 1 | Integer | The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'. |
| ##### state.backend.forst.writebuffer.size [Anchor link for: state backend forst writebuffer size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#state-backend-forst-writebuffer-size) | 64 mb | MemorySize | The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'. |

### Advanced Fault Tolerance Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-fault-tolerance-options)

_These parameters can help with problems related to failover and to components erroneously considering each other as failed._

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### cluster.io-pool.size [Anchor link for: cluster io pool size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-io-pool-size) | (none) | Integer | The size of the IO executor pool used by the cluster to execute blocking IO operations (Master as well as TaskManager processes). By default it will use 4 \* the number of CPU cores (hardware contexts) that the cluster process has access to. Increasing the pool size allows to run more IO operations concurrently. |
| ##### cluster.registration.error-delay [Anchor link for: cluster registration error delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-registration-error-delay) | 10 s | Duration | The pause made after an registration attempt caused an exception (other than timeout). |
| ##### cluster.registration.initial-timeout [Anchor link for: cluster registration initial timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-registration-initial-timeout) | 100 ms | Duration | Initial registration timeout between cluster components. |
| ##### cluster.registration.max-timeout [Anchor link for: cluster registration max timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-registration-max-timeout) | 30 s | Duration | Maximum registration timeout between cluster components. |
| ##### cluster.registration.refused-registration-delay [Anchor link for: cluster registration refused registration delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-registration-refused-registration-delay) | 30 s | Duration | The pause made after the registration attempt was refused. |
| ##### cluster.services.shutdown-timeout [Anchor link for: cluster services shutdown timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-services-shutdown-timeout) | 30 s | Duration | The shutdown timeout for cluster services like executors. |
| ##### heartbeat.interval [Anchor link for: heartbeat interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#heartbeat-interval) | 10 s | Duration | Time interval between heartbeat RPC requests from the sender to the receiver side. |
| ##### heartbeat.rpc-failure-threshold [Anchor link for: heartbeat rpc failure threshold](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#heartbeat-rpc-failure-threshold) | 2 | Integer | The number of consecutive failed heartbeat RPCs until a heartbeat target is marked as unreachable. Failed heartbeat RPCs can be used to detect dead targets faster because they no longer receive the RPCs. The detection time is `heartbeat.interval` \\* `heartbeat.rpc-failure-threshold`. In environments with a flaky network, setting this value too low can produce false positives. In this case, we recommend to increase this value, but not higher than `heartbeat.timeout` / `heartbeat.interval`. The mechanism can be disabled by setting this option to `-1` |
| ##### heartbeat.timeout [Anchor link for: heartbeat timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#heartbeat-timeout) | 50 s | Duration | Timeout for requesting and receiving heartbeats for both sender and receiver sides. |
| ##### jobmanager.execution.failover-strategy [Anchor link for: jobmanager execution failover strategy](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-execution-failover-strategy) | "region" | String | This option specifies how the job computation recovers from task failures. Accepted values are:<br>- 'full': Restarts all tasks to recover the job.<br>- 'region': Restarts all tasks that could be affected by the task failure. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery/#restart-pipelined-region-failover-strategy). |

### Advanced Cluster Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-cluster-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### cluster.id [Anchor link for: cluster id](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-id) | "00000000000000000000000000000000" | String | The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Currently, this option is primarily used to determine the archive path, and may also be used as the fixed application/job id (if not specified by the user) in high availability mode.The expected format is \[0-9a-fA-F\]{32}, e.g. fd72014d4c864993a2e5a9287b4a9c5d.<br>If this option is not configured but `high-availability.cluster-id` is configured, the value will be derived from the value of `high-availability.cluster-id`. |
| ##### cluster.intercept-user-system-exit [Anchor link for: cluster intercept user system exit](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-intercept-user-system-exit) | DISABLED | Enum | Flag to check user code exiting system by terminating JVM (e.g., System.exit()). Note that this configuration option can interfere with `cluster.processes.halt-on-fatal-error`: In intercepted user-code, a call to System.exit() will not cause the JVM to halt, when `THROW` is configured.<br>Possible values:<br>- "DISABLED": Flink is not monitoring or intercepting calls to System.exit()<br>- "LOG": Log exit attempt with stack trace but still allowing exit to be performed<br>- "THROW": Throw exception when exit is attempted disallowing JVM termination |
| ##### cluster.processes.halt-on-fatal-error [Anchor link for: cluster processes halt on fatal error](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-processes-halt-on-fatal-error) | false | Boolean | Whether processes should halt on fatal errors instead of performing a graceful shutdown. In some environments (e.g. Java 8 with the G1 garbage collector), a regular graceful shutdown can lead to a JVM deadlock. See [FLINK-16510](https://issues.apache.org/jira/browse/FLINK-16510) for details. |
| ##### cluster.thread-dump.stacktrace-max-depth [Anchor link for: cluster thread dump stacktrace max depth](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-thread-dump-stacktrace-max-depth) | 50 | Integer | The maximum stacktrace depth of TaskManager and JobManager's thread dump web-frontend displayed. |
| ##### cluster.uncaught-exception-handling [Anchor link for: cluster uncaught exception handling](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#cluster-uncaught-exception-handling) | LOG | Enum | Defines whether cluster will handle any uncaught exceptions by just logging them (LOG mode), or by failing job (FAIL mode)<br>Possible values:<br>- "LOG"<br>- "FAIL" |
| ##### process.jobmanager.working-dir [Anchor link for: process jobmanager working dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#process-jobmanager-working-dir) | (none) | String | Working directory for Flink JobManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to `process.working-dir`. |
| ##### process.taskmanager.working-dir [Anchor link for: process taskmanager working dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#process-taskmanager-working-dir) | (none) | String | Working directory for Flink TaskManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to `process.working-dir`. |
| ##### process.working-dir [Anchor link for: process working dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#process-working-dir) | io.tmp.dirs | String | Local working directory for Flink processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to a randomly picked temporary directory defined via `io.tmp.dirs`. |

### Advanced JobManager Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-jobmanager-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### jobmanager.future-pool.size [Anchor link for: jobmanager future pool size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-future-pool-size) | (none) | Integer | The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores. |
| ##### jobmanager.io-pool.size [Anchor link for: jobmanager io pool size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-io-pool-size) | (none) | Integer | The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores. |

### Advanced Scheduling Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-scheduling-options)

_These parameters can help with fine-tuning scheduling for specific situations._

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### execution.batch.adaptive.auto-parallelism.avg-data-volume-per-task [Anchor link for: execution batch adaptive auto parallelism avg data volume per ta](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-adaptive-auto-parallelism-avg-data-volume-per-ta) | 16 mb | MemorySize | The average size of data volume to expect each task instance to process if `jobmanager.scheduler` has been set to `AdaptiveBatch`. Note that when data skew occurs or the decided parallelism reaches the `execution.batch.adaptive.auto-parallelism.max-parallelism` (due to too much data), the data actually processed by some tasks may far exceed this value. |
| ##### execution.batch.adaptive.auto-parallelism.default-source-parallelism [Anchor link for: execution batch adaptive auto parallelism default source paralle](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-adaptive-auto-parallelism-default-source-paralle) | (none) | Integer | The default parallelism of source vertices or the upper bound of source parallelism to set adaptively if `jobmanager.scheduler` has been set to `AdaptiveBatch`. Note that `execution.batch.adaptive.auto-parallelism.max-parallelism` will be used if this configuration is not configured. If `execution.batch.adaptive.auto-parallelism.max-parallelism` is not set either, then the default parallelism set via `parallelism.default` will be used instead. |
| ##### execution.batch.adaptive.auto-parallelism.enabled [Anchor link for: execution batch adaptive auto parallelism enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-adaptive-auto-parallelism-enabled) | true | Boolean | If true, Flink will automatically decide the parallelism of operators in batch jobs. |
| ##### execution.batch.adaptive.auto-parallelism.max-parallelism [Anchor link for: execution batch adaptive auto parallelism max parallelism](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-adaptive-auto-parallelism-max-parallelism) | 128 | Integer | The upper bound of allowed parallelism to set adaptively if `jobmanager.scheduler` has been set to `AdaptiveBatch` |
| ##### execution.batch.adaptive.auto-parallelism.min-parallelism [Anchor link for: execution batch adaptive auto parallelism min parallelism](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-adaptive-auto-parallelism-min-parallelism) | 1 | Integer | The lower bound of allowed parallelism to set adaptively if `jobmanager.scheduler` has been set to `AdaptiveBatch` |
| ##### execution.batch.job-recovery.enabled [Anchor link for: execution batch job recovery enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-job-recovery-enabled) | false | Boolean | A flag to enable or disable the job recovery. If enabled, batch jobs can resume with previously generated intermediate results after job master restarts due to failures, thereby preserving the progress. |
| ##### execution.batch.job-recovery.previous-worker.recovery.timeout [Anchor link for: execution batch job recovery previous worker recovery timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-job-recovery-previous-worker-recovery-timeout) | 30 s | Duration | The timeout for a new job master to wait for the previous worker to reconnect.A reconnected worker will transmit the details of its produced intermediate results to the new job master, enabling the job master to reuse these results. |
| ##### execution.batch.job-recovery.snapshot.min-pause [Anchor link for: execution batch job recovery snapshot min pause](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-job-recovery-snapshot-min-pause) | 3 min | Duration | The minimal pause between snapshots taken by operator coordinator or other components. It is used to avoid performance degradation due to excessive snapshot frequency. |
| ##### execution.batch.speculative.block-slow-node-duration [Anchor link for: execution batch speculative block slow node duration](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-speculative-block-slow-node-duration) | 1 min | Duration | Controls how long an detected slow node should be blocked for. |
| ##### execution.batch.speculative.enabled [Anchor link for: execution batch speculative enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-speculative-enabled) | false | Boolean | Controls whether to enable speculative execution. |
| ##### execution.batch.speculative.max-concurrent-executions [Anchor link for: execution batch speculative max concurrent executions](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#execution-batch-speculative-max-concurrent-executions) | 2 | Integer | Controls the maximum number of execution attempts of each operator that can execute concurrently, including the original one and speculative ones. |
| ##### job-event.store.write-buffer.flush-interval [Anchor link for: job event store write buffer flush interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#job-event-store-write-buffer-flush-interval) | 1 s | Duration | The flush interval of JobEventStore write buffers. Buffer contents will be flushed to external file system regularly with regard to this value. |
| ##### job-event.store.write-buffer.size [Anchor link for: job event store write buffer size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#job-event-store-write-buffer-size) | 1 mb | MemorySize | The size of the write buffer of JobEventStore. The content will be flushed to external file system once the buffer is full |
| ##### jobmanager.adaptive-scheduler.executing.cooldown-after-rescaling [Anchor link for: jobmanager adaptive scheduler executing cooldown after rescaling](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-executing-cooldown-after-rescaling) | 30 s | Duration | Determines the minimum time between scaling operations. |
| ##### jobmanager.adaptive-scheduler.executing.resource-stabilization-timeout [Anchor link for: jobmanager adaptive scheduler executing resource stabilization t](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-executing-resource-stabilization-t) | 1 min | Duration | Defines the duration the JobManager delays the scaling operation after a resource change if only sufficient resources are available. The scaling operation is performed immediately if the resources have changed and the desired resources are available. The timeout begins as soon as either the available resources or the job's resource requirements are changed.<br>The resource requirements of a running job can be changed using the [REST API endpoint](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-jobid-resource-requirements-1). |
| ##### jobmanager.adaptive-scheduler.prefer-minimal-taskmanagers [Anchor link for: jobmanager adaptive scheduler prefer minimal taskmanagers](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-prefer-minimal-taskmanagers) | true | Boolean | This parameter defines whether the adaptive scheduler prioritizes using the minimum number of `TaskManagers` when scheduling tasks.<br>Note, this parameter is suitable if `execution.state-recovery.from-local` is not enabled. More details about this configuration are available at [FLINK-33977](https://issues.apache.org/jira/browse/FLINK-33977). |
| ##### jobmanager.adaptive-scheduler.rescale-trigger.max-checkpoint-failures [Anchor link for: jobmanager adaptive scheduler rescale trigger max checkpoint fai](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-rescale-trigger-max-checkpoint-fai) | 2 | Integer | The number of consecutive failed checkpoints that will trigger rescaling even in the absence of a completed checkpoint. |
| ##### jobmanager.adaptive-scheduler.rescale-trigger.max-delay [Anchor link for: jobmanager adaptive scheduler rescale trigger max delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-rescale-trigger-max-delay) | (none) | Duration | The maximum time the JobManager will wait with evaluating previously observed events for rescaling (default: 0ms if checkpointing is disabled and the checkpointing interval multiplied by the by-1-incremented parameter value of jobmanager.adaptive-scheduler.rescale-trigger.max-checkpoint-failures if checkpointing is enabled). |
| ##### jobmanager.adaptive-scheduler.submission.resource-stabilization-timeout [Anchor link for: jobmanager adaptive scheduler submission resource stabilization](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-submission-resource-stabilization) | 10 s | Duration | The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available during job submission. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.<br>If `scheduler-mode` is configured to `REACTIVE`, this configuration value will default to 0, so that jobs are starting immediately with the available resources. |
| ##### jobmanager.adaptive-scheduler.submission.resource-wait-timeout [Anchor link for: jobmanager adaptive scheduler submission resource wait timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-submission-resource-wait-timeout) | 5 min | Duration | The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.<br>Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).<br>Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.<br>If `scheduler-mode` is configured to `REACTIVE`, this configuration value will default to a negative value to disable the resource timeout. |
| ##### jobmanager.partition.hybrid.partition-data-consume-constraint [Anchor link for: jobmanager partition hybrid partition data consume constraint](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-partition-hybrid-partition-data-consume-constraint) | (none) | Enum | Controls the constraint that hybrid partition data can be consumed. Note that this option is allowed only when `jobmanager.scheduler` has been set to `AdaptiveBatch`. Accepted values are:<br>- '`ALL_PRODUCERS_FINISHED`': hybrid partition data can be consumed only when all producers are finished.<br>- '`ONLY_FINISHED_PRODUCERS`': hybrid partition data can be consumed when its producer is finished.<br>- '`UNFINISHED_PRODUCERS`': hybrid partition data can be consumed even if its producer is un-finished.<br>Possible values:<br>- "ALL\_PRODUCERS\_FINISHED"<br>- "ONLY\_FINISHED\_PRODUCERS"<br>- "UNFINISHED\_PRODUCERS" |
| ##### jobmanager.scheduler [Anchor link for: jobmanager scheduler](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-scheduler) | Default | Enum | Determines which scheduler implementation is used to schedule tasks. If this option is not explicitly set, batch jobs will use the 'AdaptiveBatch' scheduler as the default, while streaming jobs will default to the 'Default' scheduler. <br>Possible values:<br>- "Default": Default scheduler<br>- "Adaptive": Adaptive scheduler. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling#adaptive-scheduler).<br>- "AdaptiveBatch": Adaptive batch scheduler. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling#adaptive-batch-scheduler). |
| ##### scheduler-mode [Anchor link for: scheduler mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#scheduler-mode) | (none) | Enum | Determines the mode of the scheduler. Note that `scheduler-mode`=`REACTIVE` is only supported by standalone application deployments, not by active resource managers (YARN, Kubernetes) or session clusters.<br>Possible values:<br>- "REACTIVE" |
| ##### slot.idle.timeout [Anchor link for: slot idle timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slot-idle-timeout) | 50 s | Duration | The timeout for a idle slot in Slot Pool. |
| ##### slot.request.max-interval [Anchor link for: slot request max interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slot-request-max-interval) | 20 ms | Duration | The max interval duration for slots request. |
| ##### slot.request.timeout [Anchor link for: slot request timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slot-request-timeout) | 5 min | Duration | The timeout for requesting a slot from Slot Pool. |
| ##### slotmanager.max-total-resource.cpu [Anchor link for: slotmanager max total resource cpu](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-max-total-resource-cpu) | (none) | Double | Maximum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. |
| ##### slotmanager.max-total-resource.memory [Anchor link for: slotmanager max total resource memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-max-total-resource-memory) | (none) | MemorySize | Maximum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. |
| ##### slotmanager.min-total-resource.cpu [Anchor link for: slotmanager min total resource cpu](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-min-total-resource-cpu) | (none) | Double | Minimum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.min'. |
| ##### slotmanager.min-total-resource.memory [Anchor link for: slotmanager min total resource memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-min-total-resource-memory) | (none) | MemorySize | Minimum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.min'. |
| ##### slotmanager.number-of-slots.max [Anchor link for: slotmanager number of slots max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-number-of-slots-max) | infinite | Integer | Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink. |
| ##### slotmanager.number-of-slots.min [Anchor link for: slotmanager number of slots min](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-number-of-slots-min) | 0 | Integer | Defines the minimum number of slots that the Flink cluster allocates. This configuration option is meant for cluster to initialize certain workers in best efforts when starting. This can be used to speed up a job startup process. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink. |
| ##### slow-task-detector.check-interval [Anchor link for: slow task detector check interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slow-task-detector-check-interval) | 1 s | Duration | The interval to check slow tasks. |
| ##### slow-task-detector.execution-time.baseline-lower-bound [Anchor link for: slow task detector execution time baseline lower bound](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slow-task-detector-execution-time-baseline-lower-bound) | 1 min | Duration | The lower bound of slow task detection baseline. |
| ##### slow-task-detector.execution-time.baseline-multiplier [Anchor link for: slow task detector execution time baseline multiplier](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slow-task-detector-execution-time-baseline-multiplier) | 1.5 | Double | The multiplier to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N\*R finished tasks' execution time. The baseline will be T\*M, where M is the multiplier of the baseline. Note that the execution time will be weighted with the task's input bytes to ensure the accuracy of the detection if data skew occurs. |
| ##### slow-task-detector.execution-time.baseline-ratio [Anchor link for: slow task detector execution time baseline ratio](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slow-task-detector-execution-time-baseline-ratio) | 0.75 | Double | The finished execution ratio threshold to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N\*R finished tasks' execution time. The baseline will be T\*M, where M is the multiplier of the baseline. Note that the execution time will be weighted with the task's input bytes to ensure the accuracy of the detection if data skew occurs. |
| ##### taskmanager.load-balance.mode [Anchor link for: taskmanager load balance mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-load-balance-mode) | NONE | Enum | Mode for the load-balance allocation strategy across all available `TaskManagers`.<br>- The `SLOTS` mode tries to spread out the slots evenly across all available `TaskManagers`.<br>- The `MIN_RESOURCES` mode tries to allocate slots on minimum number of available `TaskManagers`.<br>- The `TASKS` mode tries to schedule evenly tasks across all available `TaskManagers` with in the job scope based the number of tasks.<br>- The `NONE` mode is the default mode without any specified strategy.<br>Possible values:<br>- "NONE"<br>- "SLOTS"<br>- "MIN\_RESOURCES"<br>- "TASKS" |

### Advanced High-availability Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-high-availability-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### high-availability.jobmanager.port [Anchor link for: high availability jobmanager port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-jobmanager-port) | "0" | String | The port (range) used by the Flink Master for its RPC connections in highly-available setups. In highly-available setups, this value is used instead of 'jobmanager.rpc.port'.A value of '0' means that a random free port is chosen. TaskManagers discover this port through the high-availability services (leader election), so a random port or a port range works without requiring any additional means of service discovery. |

### Advanced High-availability ZooKeeper Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-high-availability-zookeeper-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### high-availability.zookeeper.client.acl [Anchor link for: high availability zookeeper client acl](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-client-acl) | "open" | String | Defines the ACL (open\|creator) to be configured on ZK node. The configuration value can be set to “creator” if the ZooKeeper server configuration has the “authProvider” property mapped to use SASLAuthenticationProvider and the cluster is configured to run in secure mode (Kerberos). |
| ##### high-availability.zookeeper.client.connection-timeout [Anchor link for: high availability zookeeper client connection timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-client-connection-timeout) | 15 s | Duration | Defines the connection timeout for ZooKeeper. |
| ##### high-availability.zookeeper.client.ensemble-tracker [Anchor link for: high availability zookeeper client ensemble tracker](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-client-ensemble-tracker) | true | Boolean | Defines whether Curator should enable ensemble tracker. This can be useful in certain scenarios in which CuratorFramework is accessing to ZK clusters via load balancer or Virtual IPs. Default Curator EnsembleTracking logic watches CuratorEventType.GET\_CONFIG events and changes ZooKeeper connection string. It is not desired behaviour when ZooKeeper is running under the Virtual IPs. Under certain configurations EnsembleTracking can lead to setting of ZooKeeper connection string with unresolvable hostnames. |
| ##### high-availability.zookeeper.client.max-retry-attempts [Anchor link for: high availability zookeeper client max retry attempts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-client-max-retry-attempts) | 3 | Integer | Defines the number of connection retries before the client gives up. |
| ##### high-availability.zookeeper.client.max-retry-wait [Anchor link for: high availability zookeeper client max retry wait](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-client-max-retry-wait) | 1 min | Duration | Defines the maximum retry wait time in milliseconds for each attempt. This caps the exponential backoff to prevent excessively long waits between retries. |
| ##### high-availability.zookeeper.client.retry-wait [Anchor link for: high availability zookeeper client retry wait](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-client-retry-wait) | 5 s | Duration | Defines the pause between consecutive retries. |
| ##### high-availability.zookeeper.client.session-timeout [Anchor link for: high availability zookeeper client session timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-client-session-timeout) | 1 min | Duration | Defines the session timeout for the ZooKeeper session. |
| ##### high-availability.zookeeper.client.tolerate-suspended-connections [Anchor link for: high availability zookeeper client tolerate suspended connection](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-client-tolerate-suspended-connection) | false | Boolean | Defines whether a suspended ZooKeeper connection will be treated as an error that causes the leader information to be invalidated or not. In case you set this option to `true`, Flink will wait until a ZooKeeper connection is marked as lost before it revokes the leadership of components. This has the effect that Flink is more resilient against temporary connection instabilities at the cost of running more likely into timing issues with ZooKeeper. |
| ##### high-availability.zookeeper.path.execution-plans [Anchor link for: high availability zookeeper path execution plans](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-zookeeper-path-execution-plans) | "/execution-plans" | String | ZooKeeper root path (ZNode) for execution plans |

### Advanced High-availability Kubernetes Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-high-availability-kubernetes-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### high-availability.kubernetes.leader-election.lease-duration [Anchor link for: high availability kubernetes leader election lease duration](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-kubernetes-leader-election-lease-duration) | 15 s | Duration | Define the lease duration for the Kubernetes leader election. The leader will continuously renew its lease time to indicate its existence. And the followers will do a lease checking against the current time. "renewTime + leaseDuration > now" means the leader is alive. |
| ##### high-availability.kubernetes.leader-election.renew-deadline [Anchor link for: high availability kubernetes leader election renew deadline](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-kubernetes-leader-election-renew-deadline) | 15 s | Duration | Defines the deadline duration when the leader tries to renew the lease. The leader will give up its leadership if it cannot successfully renew the lease in the given time. |
| ##### high-availability.kubernetes.leader-election.retry-period [Anchor link for: high availability kubernetes leader election retry period](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#high-availability-kubernetes-leader-election-retry-period) | 5 s | Duration | Defines the pause duration between consecutive retries. All the contenders, including the current leader and all other followers, periodically try to acquire/renew the leadership if possible at this interval. |

### Advanced SSL Security Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-ssl-security-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### security.ssl.internal.close-notify-flush-timeout [Anchor link for: security ssl internal close notify flush timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-close-notify-flush-timeout) | -1 | Integer | The timeout (in ms) for flushing the \`close\_notify\` that was triggered by closing a channel. If the \`close\_notify\` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default) |
| ##### security.ssl.internal.handshake-timeout [Anchor link for: security ssl internal handshake timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-handshake-timeout) | -1 | Integer | The timeout (in ms) during SSL handshake. (-1 = use system default) |
| ##### security.ssl.internal.session-cache-size [Anchor link for: security ssl internal session cache size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-session-cache-size) | -1 | Integer | The size of the cache used for storing SSL session objects. According to [here](https://github.com/netty/netty/issues/832), you should always set this to an appropriate number to not run into a bug with stalling IO threads during garbage collection. (-1 = use system default). |
| ##### security.ssl.internal.session-timeout [Anchor link for: security ssl internal session timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-internal-session-timeout) | -1 | Integer | The timeout (in ms) for the cached SSL session objects. (-1 = use system default) |
| ##### security.ssl.provider [Anchor link for: security ssl provider](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#security-ssl-provider) | "JDK" | String | The SSL engine provider to use for the ssl transport:<br>- `JDK`: default Java-based SSL engine<br>- `OPENSSL`: openSSL-based SSL engine using system libraries<br>`OPENSSL` is based on [netty-tcnative](http://netty.io/wiki/forked-tomcat-native.html#wiki-h2-4) and comes in two flavours:<br>- dynamically linked: This will use your system's openSSL libraries (if compatible) and requires `opt/flink-shaded-netty-tcnative-dynamic-*.jar` to be copied to `lib/`<br>- statically linked: Due to potential licensing issues with openSSL (see [LEGAL-393](https://issues.apache.org/jira/browse/LEGAL-393)), we cannot ship pre-built libraries. However, you can build the required library yourself and put it into `lib/`:<br>  <br>  `git clone https://github.com/apache/flink-shaded.git && cd flink-shaded && mvn clean package -Pinclude-netty-tcnative-static -pl flink-shaded-netty-tcnative-static` |

### Advanced Options for the REST endpoint and Client  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-options-for-the-rest-endpoint-and-client)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### rest.async.store-duration [Anchor link for: rest async store duration](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-async-store-duration) | 5 min | Duration | Maximum duration that the result of an async operation is stored. Once elapsed the result of the operation can no longer be retrieved. |
| ##### rest.await-leader-timeout [Anchor link for: rest await leader timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-await-leader-timeout) | 30 s | Duration | The time that the client waits for the leader address, e.g., Dispatcher or WebMonitorEndpoint |
| ##### rest.cache.checkpoint-statistics.size [Anchor link for: rest cache checkpoint statistics size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-cache-checkpoint-statistics-size) | 1000 | Integer | Maximum number of entries in the checkpoint statistics cache. |
| ##### rest.cache.checkpoint-statistics.timeout [Anchor link for: rest cache checkpoint statistics timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-cache-checkpoint-statistics-timeout) | 3 s | Duration | Duration from write after which cached checkpoints statistics are cleaned up. For backwards compatibility, if no value is configured, `web.refresh-interval` will be used instead. |
| ##### rest.client.max-content-length [Anchor link for: rest client max content length](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-client-max-content-length) | 104857600 | Integer | The maximum content length in bytes that the client will handle. |
| ##### rest.connection-timeout [Anchor link for: rest connection timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-connection-timeout) | 15 s | Duration | The maximum time for the client to establish a TCP connection. |
| ##### rest.flamegraph.cleanup-interval [Anchor link for: rest flamegraph cleanup interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-flamegraph-cleanup-interval) | 10 min | Duration | Time after which cached stats are cleaned up if not accessed. It can be specified using notation: "100 s", "10 m". |
| ##### rest.flamegraph.delay-between-samples [Anchor link for: rest flamegraph delay between samples](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-flamegraph-delay-between-samples) | 50 ms | Duration | Delay between individual stack trace samples taken for building a FlameGraph. It can be specified using notation: "100 ms", "1 s". |
| ##### rest.flamegraph.enabled [Anchor link for: rest flamegraph enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-flamegraph-enabled) | false | Boolean | Enables the experimental flame graph feature. |
| ##### rest.flamegraph.num-samples [Anchor link for: rest flamegraph num samples](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-flamegraph-num-samples) | 100 | Integer | Number of samples to take to build a FlameGraph. |
| ##### rest.flamegraph.refresh-interval [Anchor link for: rest flamegraph refresh interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-flamegraph-refresh-interval) | 1 min | Duration | Time after which available stats are deprecated and need to be refreshed (by resampling). It can be specified using notation: "30 s", "1 m". |
| ##### rest.flamegraph.stack-depth [Anchor link for: rest flamegraph stack depth](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-flamegraph-stack-depth) | 100 | Integer | Maximum depth of stack traces used to create FlameGraphs. |
| ##### rest.idleness-timeout [Anchor link for: rest idleness timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-idleness-timeout) | 5 min | Duration | The maximum time for a connection to stay idle before failing. |
| ##### rest.profiling.dir [Anchor link for: rest profiling dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-profiling-dir) | System.getProperty("java.io.tmpdir") | String | Profiling result storing directory. |
| ##### rest.profiling.duration-max [Anchor link for: rest profiling duration max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-profiling-duration-max) | 5 min | Duration | Maximum profiling duration for each profiling request. Any profiling request's duration exceeding this value will not be accepted. |
| ##### rest.profiling.enabled [Anchor link for: rest profiling enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-profiling-enabled) | false | Boolean | Enables the experimental profiler feature. |
| ##### rest.profiling.history-size [Anchor link for: rest profiling history size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-profiling-history-size) | 10 | Integer | Maximum profiling history instance to be maintained for JobManager or each TaskManager. The oldest instance will be removed on a rolling basis when the history size exceeds this value. |
| ##### rest.retry.delay [Anchor link for: rest retry delay](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-retry-delay) | 3 s | Duration | The time that the client waits between retries (See also \`rest.retry.max-attempts\`). |
| ##### rest.retry.max-attempts [Anchor link for: rest retry max attempts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-retry-max-attempts) | 20 | Integer | The number of retries the client will attempt if a retryable operations fails. |
| ##### rest.server.max-content-length [Anchor link for: rest server max content length](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-server-max-content-length) | 104857600 | Integer | The maximum content length in bytes that the server will handle. |
| ##### rest.server.numThreads [Anchor link for: rest server numthreads](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-server-numthreads) | 4 | Integer | The number of threads for the asynchronous processing of requests. |
| ##### rest.server.thread-priority [Anchor link for: rest server thread priority](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rest-server-thread-priority) | 5 | Integer | Thread priority of the REST server's executor for processing asynchronous requests. Lowering the thread priority will give Flink's main components more CPU time whereas increasing will allocate more time for the REST server's processing. |

### Advanced Options for Flink Web UI  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#advanced-options-for-flink-web-ui)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### web.access-control-allow-origin [Anchor link for: web access control allow origin](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-access-control-allow-origin) | "\*" | String | Access-Control-Allow-Origin header for all responses from the web-frontend. |
| ##### web.cancel.enable [Anchor link for: web cancel enable](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-cancel-enable) | true | Boolean | Flag indicating whether jobs can be canceled from the web-frontend. |
| ##### web.checkpoints.history [Anchor link for: web checkpoints history](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-checkpoints-history) | 10 | Integer | Number of checkpoints to remember for recent history. |
| ##### web.exception-history-size [Anchor link for: web exception history size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-exception-history-size) | 16 | Integer | The maximum number of failures collected by the exception history per job. |
| ##### web.history [Anchor link for: web history](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-history) | 5 | Integer | Number of archived jobs for the JobManager. |
| ##### web.log.path [Anchor link for: web log path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-log-path) | (none) | String | Path to the log file (may be in /log for standalone but under log directory when using YARN). |
| ##### web.refresh-interval [Anchor link for: web refresh interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-refresh-interval) | 3 s | Duration | Refresh interval for the web-frontend. |
| ##### web.rescale.enable [Anchor link for: web rescale enable](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-rescale-enable) | true | Boolean | Flag indicating whether jobs can be rescaled from the web-frontend. |
| ##### web.submit.enable [Anchor link for: web submit enable](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-submit-enable) | true | Boolean | Flag indicating whether jobs can be uploaded and run from the web-frontend. |
| ##### web.timeout [Anchor link for: web timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-timeout) | 10 min | Duration | Timeout for asynchronous operations by the web monitor. |
| ##### web.tmpdir [Anchor link for: web tmpdir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-tmpdir) | System.getProperty("java.io.tmpdir") | String | Local directory that is used by the REST API for temporary files. |
| ##### web.upload.dir [Anchor link for: web upload dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-upload-dir) | (none) | String | Local directory that is used by the REST API for storing uploaded jars. If not specified a dynamic directory will be created under `web.tmpdir`. |

### Full JobManager Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#full-jobmanager-options)

**JobManager**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### completed-application-store.cache-size [Anchor link for: completed application store cache size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#completed-application-store-cache-size) | 52428800 | Long | The cache size in bytes which is used to keep completed applications in memory. |
| ##### completed-application-store.expiration-time [Anchor link for: completed application store expiration time](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#completed-application-store-expiration-time) | 1 h | Duration | The time after which a completed application expires and is purged from the store. |
| ##### completed-application-store.max-capacity [Anchor link for: completed application store max capacity](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#completed-application-store-max-capacity) | infinite | Integer | The max number of completed applications that can be kept in the store. NOTICE: if memory store keeps too many applications in session cluster, it may cause FullGC or OOM in jm. |
| ##### completed-application-store.type [Anchor link for: completed application store type](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#completed-application-store-type) | File | Enum | Determines which store implementation is used in session cluster. Accepted values are:<br>- 'File': the file store keeps the completed applications in files<br>- 'Memory': the memory store keeps the completed applications in memory. You may need to limit the `completed-application-store.max-capacity` to mitigate FullGC or OOM when there are too many applications<br>Possible values:<br>- "File"<br>- "Memory" |
| ##### jobmanager.adaptive-scheduler.executing.cooldown-after-rescaling [Anchor link for: jobmanager adaptive scheduler executing cooldown after rescaling 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-executing-cooldown-after-rescaling-1) | 30 s | Duration | Determines the minimum time between scaling operations. |
| ##### jobmanager.adaptive-scheduler.executing.resource-stabilization-timeout [Anchor link for: jobmanager adaptive scheduler executing resource stabilization t 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-executing-resource-stabilization-t-1) | 1 min | Duration | Defines the duration the JobManager delays the scaling operation after a resource change if only sufficient resources are available. The scaling operation is performed immediately if the resources have changed and the desired resources are available. The timeout begins as soon as either the available resources or the job's resource requirements are changed.<br>The resource requirements of a running job can be changed using the [REST API endpoint](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-jobid-resource-requirements-1). |
| ##### jobmanager.adaptive-scheduler.prefer-minimal-taskmanagers [Anchor link for: jobmanager adaptive scheduler prefer minimal taskmanagers 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-prefer-minimal-taskmanagers-1) | true | Boolean | This parameter defines whether the adaptive scheduler prioritizes using the minimum number of `TaskManagers` when scheduling tasks.<br>Note, this parameter is suitable if `execution.state-recovery.from-local` is not enabled. More details about this configuration are available at [FLINK-33977](https://issues.apache.org/jira/browse/FLINK-33977). |
| ##### jobmanager.adaptive-scheduler.rescale-trigger.max-checkpoint-failures [Anchor link for: jobmanager adaptive scheduler rescale trigger max checkpoint fai 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-rescale-trigger-max-checkpoint-fai-1) | 2 | Integer | The number of consecutive failed checkpoints that will trigger rescaling even in the absence of a completed checkpoint. |
| ##### jobmanager.adaptive-scheduler.rescale-trigger.max-delay [Anchor link for: jobmanager adaptive scheduler rescale trigger max delay 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-rescale-trigger-max-delay-1) | (none) | Duration | The maximum time the JobManager will wait with evaluating previously observed events for rescaling (default: 0ms if checkpointing is disabled and the checkpointing interval multiplied by the by-1-incremented parameter value of jobmanager.adaptive-scheduler.rescale-trigger.max-checkpoint-failures if checkpointing is enabled). |
| ##### jobmanager.adaptive-scheduler.submission.resource-stabilization-timeout [Anchor link for: jobmanager adaptive scheduler submission resource stabilization 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-submission-resource-stabilization-1) | 10 s | Duration | The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available during job submission. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.<br>If `scheduler-mode` is configured to `REACTIVE`, this configuration value will default to 0, so that jobs are starting immediately with the available resources. |
| ##### jobmanager.adaptive-scheduler.submission.resource-wait-timeout [Anchor link for: jobmanager adaptive scheduler submission resource wait timeout 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-adaptive-scheduler-submission-resource-wait-timeout-1) | 5 min | Duration | The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.<br>Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).<br>Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.<br>If `scheduler-mode` is configured to `REACTIVE`, this configuration value will default to a negative value to disable the resource timeout. |
| ##### jobmanager.archive.fs.dir [Anchor link for: jobmanager archive fs dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-archive-fs-dir) | (none) | String | Directory for JobManager to store the archives of completed jobs. |
| ##### jobmanager.bind-host [Anchor link for: jobmanager bind host 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-bind-host-1) | (none) | String | The local address of the network interface that the job manager binds to. If not configured, '0.0.0.0' will be used. |
| ##### jobmanager.execution.attempts-history-size [Anchor link for: jobmanager execution attempts history size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-execution-attempts-history-size) | 16 | Integer | The maximum number of historical execution attempts kept in history. |
| ##### jobmanager.execution.failover-strategy [Anchor link for: jobmanager execution failover strategy 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-execution-failover-strategy-1) | "region" | String | This option specifies how the job computation recovers from task failures. Accepted values are:<br>- 'full': Restarts all tasks to recover the job.<br>- 'region': Restarts all tasks that could be affected by the task failure. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery/#restart-pipelined-region-failover-strategy). |
| ##### jobmanager.failure-enrichers [Anchor link for: jobmanager failure enrichers](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-failure-enrichers) | (none) | String | An optional list of failure enricher names. If empty, NO failure enrichers will be started. If configured, only enrichers whose name matches any of the names in the list will be started. |
| ##### jobmanager.future-pool.size [Anchor link for: jobmanager future pool size 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-future-pool-size-1) | (none) | Integer | The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores. |
| ##### jobmanager.io-pool.size [Anchor link for: jobmanager io pool size 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-io-pool-size-1) | (none) | Integer | The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores. |
| ##### jobmanager.partition.hybrid.partition-data-consume-constraint [Anchor link for: jobmanager partition hybrid partition data consume constraint 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-partition-hybrid-partition-data-consume-constraint-1) | (none) | Enum | Controls the constraint that hybrid partition data can be consumed. Note that this option is allowed only when `jobmanager.scheduler` has been set to `AdaptiveBatch`. Accepted values are:<br>- '`ALL_PRODUCERS_FINISHED`': hybrid partition data can be consumed only when all producers are finished.<br>- '`ONLY_FINISHED_PRODUCERS`': hybrid partition data can be consumed when its producer is finished.<br>- '`UNFINISHED_PRODUCERS`': hybrid partition data can be consumed even if its producer is un-finished.<br>Possible values:<br>- "ALL\_PRODUCERS\_FINISHED"<br>- "ONLY\_FINISHED\_PRODUCERS"<br>- "UNFINISHED\_PRODUCERS" |
| ##### jobmanager.resource-id [Anchor link for: jobmanager resource id](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-resource-id) | (none) | String | The JobManager's ResourceID. If not configured, the ResourceID will be generated randomly. |
| ##### jobmanager.retrieve-taskmanager-hostname [Anchor link for: jobmanager retrieve taskmanager hostname](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-retrieve-taskmanager-hostname) | true | Boolean | Flag indicating whether JobManager would retrieve canonical host name of TaskManager during registration. If the option is set to "false", TaskManager registration with JobManager could be faster, since no reverse DNS lookup is performed. However, local input split assignment (such as for HDFS files) may be impacted. |
| ##### jobmanager.rpc.address [Anchor link for: jobmanager rpc address 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-rpc-address-1) | (none) | String | The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. |
| ##### jobmanager.rpc.bind-port [Anchor link for: jobmanager rpc bind port 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-rpc-bind-port-1) | (none) | Integer | The local RPC port that the JobManager binds to. If not configured, the external port (configured by 'jobmanager.rpc.port') will be used. |
| ##### jobmanager.rpc.port [Anchor link for: jobmanager rpc port 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-rpc-port-1) | 6123 | Integer | The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. |
| ##### jobmanager.scheduler [Anchor link for: jobmanager scheduler 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jobmanager-scheduler-1) | Default | Enum | Determines which scheduler implementation is used to schedule tasks. If this option is not explicitly set, batch jobs will use the 'AdaptiveBatch' scheduler as the default, while streaming jobs will default to the 'Default' scheduler. <br>Possible values:<br>- "Default": Default scheduler<br>- "Adaptive": Adaptive scheduler. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling#adaptive-scheduler).<br>- "AdaptiveBatch": Adaptive batch scheduler. More details can be found [here](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling#adaptive-batch-scheduler). |
| ##### web.exception-history-size [Anchor link for: web exception history size 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#web-exception-history-size-1) | 16 | Integer | The maximum number of failures collected by the exception history per job. |

**Blob Server**

The Blob Server is a component in the JobManager. It is used for distribution of objects that are too large to be attached to a RPC message and that benefit from caching (like Jar files or large serialized code objects).

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### blob.client.connect.timeout [Anchor link for: blob client connect timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-client-connect-timeout) | 0 | Integer | The connection timeout in milliseconds for the blob client. |
| ##### blob.client.socket.timeout [Anchor link for: blob client socket timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-client-socket-timeout) | 300000 | Integer | The socket timeout in milliseconds for the blob client. |
| ##### blob.fetch.backlog [Anchor link for: blob fetch backlog](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-fetch-backlog) | 1000 | Integer | The config parameter defining the desired backlog of BLOB fetches on the JobManager.Note that the operating system usually enforces an upper limit on the backlog size based on the `SOMAXCONN` setting. |
| ##### blob.fetch.num-concurrent [Anchor link for: blob fetch num concurrent](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-fetch-num-concurrent) | 50 | Integer | The config parameter defining the maximum number of concurrent BLOB fetches that the JobManager serves. |
| ##### blob.fetch.retries [Anchor link for: blob fetch retries](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-fetch-retries) | 5 | Integer | The config parameter defining number of retires for failed BLOB fetches. |
| ##### blob.offload.minsize [Anchor link for: blob offload minsize](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-offload-minsize) | 1048576 | Integer | The minimum size for messages to be offloaded to the BlobServer. |
| ##### blob.server.port [Anchor link for: blob server port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-server-port) | "0" | String | The config parameter defining the server port of the blob service. |
| ##### blob.service.cleanup.interval [Anchor link for: blob service cleanup interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-service-cleanup-interval) | 3600 | Long | Cleanup interval of the blob caches at the task managers (in seconds). |
| ##### blob.service.ssl.enabled [Anchor link for: blob service ssl enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-service-ssl-enabled) | true | Boolean | Flag to override ssl support for the blob service transport. |
| ##### blob.storage.directory [Anchor link for: blob storage directory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#blob-storage-directory) | (none) | String | The config parameter defining the local storage directory to be used by the blob server. If not configured, then it will default to <WORKING\_DIR>/blobStorage. |

**ResourceManager**

These configuration keys control basic Resource Manager behavior, independent of the used resource orchestration management framework (YARN, etc.)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### resourcemanager.job.timeout [Anchor link for: resourcemanager job timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resourcemanager-job-timeout) | 5 min | Duration | Timeout for jobs which don't have a job manager as leader assigned. |
| ##### resourcemanager.previous-worker.recovery.timeout [Anchor link for: resourcemanager previous worker recovery timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resourcemanager-previous-worker-recovery-timeout) | 0 ms | Duration | Timeout for resource manager to recover all the previous attempts workers. If exceeded, resource manager will handle new resource requests by requesting new workers. If you would like to reuse the previous workers as much as possible, you should configure a longer timeout time to wait for previous workers to register. |
| ##### resourcemanager.rpc.port [Anchor link for: resourcemanager rpc port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resourcemanager-rpc-port) | 0 | Integer | Defines the network port to connect to for communication with the resource manager. By default, the port of the JobManager, because the same ActorSystem is used. Its not possible to use this configuration key to define port ranges. |
| ##### resourcemanager.standalone.start-up-time [Anchor link for: resourcemanager standalone start up time](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resourcemanager-standalone-start-up-time) | (none) | Duration | Time of the start-up period of a standalone cluster. During this time, resource manager of the standalone cluster expects new task executors to be registered, and will not fail slot requests that can not be satisfied by any current registered slots. After this time, it will fail pending and new coming requests immediately that can not be satisfied by registered slots. If not set, `slot.request.timeout` will be used by default. |
| ##### resourcemanager.start-worker.max-failure-rate [Anchor link for: resourcemanager start worker max failure rate](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resourcemanager-start-worker-max-failure-rate) | 10.0 | Double | The maximum number of start worker failures (Native Kubernetes / Yarn) per minute before pausing requesting new workers. Once the threshold is reached, subsequent worker requests will be postponed to after a configured retry interval ('resourcemanager.start-worker.retry-interval'). |
| ##### resourcemanager.start-worker.retry-interval [Anchor link for: resourcemanager start worker retry interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resourcemanager-start-worker-retry-interval) | 3 s | Duration | The time to wait before requesting new workers (Native Kubernetes / Yarn) once the max failure rate of starting workers ('resourcemanager.start-worker.max-failure-rate') is reached. |
| ##### resourcemanager.taskmanager-registration.timeout [Anchor link for: resourcemanager taskmanager registration timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resourcemanager-taskmanager-registration-timeout) | 5 min | Duration | Timeout for TaskManagers to register at the active resource managers. If exceeded, active resource manager will release and try to re-request the resource for the worker. If not configured, fallback to 'taskmanager.registration.timeout'. |
| ##### resourcemanager.taskmanager-timeout [Anchor link for: resourcemanager taskmanager timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#resourcemanager-taskmanager-timeout) | 30 s | Duration | The timeout for an idle task manager to be released. |
| ##### slotmanager.max-total-resource.cpu [Anchor link for: slotmanager max total resource cpu 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-max-total-resource-cpu-1) | (none) | Double | Maximum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. |
| ##### slotmanager.max-total-resource.memory [Anchor link for: slotmanager max total resource memory 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-max-total-resource-memory-1) | (none) | MemorySize | Maximum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. |
| ##### slotmanager.min-total-resource.cpu [Anchor link for: slotmanager min total resource cpu 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-min-total-resource-cpu-1) | (none) | Double | Minimum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.min'. |
| ##### slotmanager.min-total-resource.memory [Anchor link for: slotmanager min total resource memory 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-min-total-resource-memory-1) | (none) | MemorySize | Minimum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.min'. |
| ##### slotmanager.number-of-slots.max [Anchor link for: slotmanager number of slots max 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-number-of-slots-max-1) | infinite | Integer | Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink. |
| ##### slotmanager.number-of-slots.min [Anchor link for: slotmanager number of slots min 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-number-of-slots-min-1) | 0 | Integer | Defines the minimum number of slots that the Flink cluster allocates. This configuration option is meant for cluster to initialize certain workers in best efforts when starting. This can be used to speed up a job startup process. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink. |
| ##### slotmanager.redundant-taskmanager-num [Anchor link for: slotmanager redundant taskmanager num](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#slotmanager-redundant-taskmanager-num) | 0 | Integer | The number of redundant task managers. Redundant task managers are extra task managers started by Flink, in order to speed up job recovery in case of failures due to task manager lost. Note that this feature is available only to the active deployments (native K8s, Yarn).For fine-grained resource requirement, Redundant resources will be reserved, but it is possible that we have many small pieces of free resources form multiple TMs, which added up larger than the desired redundant resources, but each piece is too small to match the resource requirement of tasks from the failed worker. |

### Full TaskManagerOptions  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#full-taskmanageroptions)

Please refer to the [network memory tuning guide](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/network_mem_tuning/) for details on how to use the `taskmanager.network.memory.buffer-debloat.*` configuration.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### minicluster.number-of-taskmanagers [Anchor link for: minicluster number of taskmanagers](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#minicluster-number-of-taskmanagers) | 1 | Integer | The number of task managers of MiniCluster. |
| ##### task.cancellation.interval [Anchor link for: task cancellation interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#task-cancellation-interval) | 30 s | Duration | Time interval between two successive task cancellation attempts. |
| ##### task.cancellation.timeout [Anchor link for: task cancellation timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#task-cancellation-timeout) | 3 min | Duration | Timeout after which a task cancellation times out and leads to a fatal TaskManager error. A value of 0 deactivates the watch dog. Notice that a task cancellation is different from both a task failure and a clean shutdown. Task cancellation timeout only applies to task cancellation and does not apply to task closing/clean-up caused by a task failure or a clean shutdown. |
| ##### task.cancellation.timers.timeout [Anchor link for: task cancellation timers timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#task-cancellation-timers-timeout) | 7500 ms | Duration | Time we wait for the timers to finish all pending timer threads when the stream task is cancelled. |
| ##### taskmanager.bind-host [Anchor link for: taskmanager bind host 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-bind-host-1) | (none) | String | The local address of the network interface that the task manager binds to. If not configured, '0.0.0.0' will be used. |
| ##### taskmanager.collect-sink.port [Anchor link for: taskmanager collect sink port 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-collect-sink-port-1) | 0 | Integer | The port used for the client to retrieve query results from the TaskManager. The default value is 0, which corresponds to a random port assignment. |
| ##### taskmanager.data.bind-port [Anchor link for: taskmanager data bind port 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-data-bind-port-1) | (none) | String | The task manager's bind port used for data exchange operations. Also accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. If not configured, 'taskmanager.data.port' will be used. |
| ##### taskmanager.data.port [Anchor link for: taskmanager data port 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-data-port-1) | 0 | Integer | The task manager’s external port used for data exchange operations. |
| ##### taskmanager.data.ssl.enabled [Anchor link for: taskmanager data ssl enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-data-ssl-enabled) | true | Boolean | Enable SSL support for the taskmanager data transport. This is applicable only when the global flag for internal SSL (security.ssl.internal.enabled) is set to true |
| ##### taskmanager.debug.memory.log [Anchor link for: taskmanager debug memory log](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-debug-memory-log) | false | Boolean | Flag indicating whether to start a thread, which repeatedly logs the memory usage of the JVM. |
| ##### taskmanager.debug.memory.log-interval [Anchor link for: taskmanager debug memory log interval](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-debug-memory-log-interval) | 5 s | Duration | The interval for the log thread to log the current memory usage. |
| ##### taskmanager.host [Anchor link for: taskmanager host 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-host-1) | (none) | String | The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file. |
| ##### taskmanager.jvm-exit-on-oom [Anchor link for: taskmanager jvm exit on oom](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-jvm-exit-on-oom) | false | Boolean | Whether to kill the TaskManager when the task thread throws an OutOfMemoryError. |
| ##### taskmanager.load-balance.mode [Anchor link for: taskmanager load balance mode 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-load-balance-mode-1) | NONE | Enum | Mode for the load-balance allocation strategy across all available `TaskManagers`.<br>- The `SLOTS` mode tries to spread out the slots evenly across all available `TaskManagers`.<br>- The `MIN_RESOURCES` mode tries to allocate slots on minimum number of available `TaskManagers`.<br>- The `TASKS` mode tries to schedule evenly tasks across all available `TaskManagers` with in the job scope based the number of tasks.<br>- The `NONE` mode is the default mode without any specified strategy.<br>Possible values:<br>- "NONE"<br>- "SLOTS"<br>- "MIN\_RESOURCES"<br>- "TASKS" |
| ##### taskmanager.log.path [Anchor link for: taskmanager log path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-log-path) | System.getProperty("log.file") | String | The path to the log file of the task manager. |
| ##### taskmanager.memory.min-segment-size [Anchor link for: taskmanager memory min segment size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-min-segment-size) | 256 bytes | MemorySize | Minimum possible size of memory buffers used by the network stack and the memory manager. ex. can be used for automatic buffer size adjustment. |
| ##### taskmanager.memory.segment-size [Anchor link for: taskmanager memory segment size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-segment-size) | 32 kb | MemorySize | Size of memory buffers used by the network stack and the memory manager. |
| ##### taskmanager.memory.starting-segment-size [Anchor link for: taskmanager memory starting segment size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-memory-starting-segment-size) | 1 kb | MemorySize | Starting size of memory buffers used by the network stack and the memory manager, when using automatic buffer size adjustment. |
| ##### taskmanager.network.bind-policy [Anchor link for: taskmanager network bind policy](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-bind-policy) | "ip" | String | The automatic address binding policy used by the TaskManager if "taskmanager.host" is not set. The value should be one of the following:<br>- "name" - uses hostname as binding address<br>- "ip" - uses host's ip address as binding address |
| ##### taskmanager.numberOfTaskSlots [Anchor link for: taskmanager numberoftaskslots](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-numberoftaskslots) | 1 | Integer | The number of parallel operator or user function instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a function or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or function instances. This value is typically proportional to the number of physical CPU cores that the TaskManager's machine has (e.g., equal to the number of cores, or half the number of cores). |
| ##### taskmanager.registration.timeout [Anchor link for: taskmanager registration timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-registration-timeout) | 5 min | Duration | Defines the timeout for the TaskManager registration. If the duration is exceeded without a successful registration, then the TaskManager terminates. |
| ##### taskmanager.resource-id [Anchor link for: taskmanager resource id](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-resource-id) | (none) | String | The TaskManager's ResourceID. If not configured, the ResourceID will be generated with the "RpcAddress:RpcPort" and a 6-character random string. Notice that this option is not valid in Yarn and Native Kubernetes mode. |
| ##### taskmanager.rpc.bind-port [Anchor link for: taskmanager rpc bind port 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-rpc-bind-port-1) | (none) | Integer | The local RPC port that the TaskManager binds to. If not configured, the external port (configured by 'taskmanager.rpc.port') will be used. |
| ##### taskmanager.rpc.port [Anchor link for: taskmanager rpc port 1](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-rpc-port-1) | "0" | String | The external RPC port where the TaskManager is exposed. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine. |
| ##### taskmanager.runtime.fs-timeout [Anchor link for: taskmanager runtime fs timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-runtime-fs-timeout) | 0 ms | Duration | The timeout for filesystem stream opening. A value of 0 indicates infinite waiting. |
| ##### taskmanager.slot.timeout [Anchor link for: taskmanager slot timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-slot-timeout) | 10 s | Duration | Timeout used for identifying inactive slots. The TaskManager will free the slot if it does not become active within the given amount of time. Inactive slots can be caused by an out-dated slot request. If no value is configured, then it will fall back to `pekko.ask.timeout`. |
| ##### taskmanager.system-out.log.cache-upper-size [Anchor link for: taskmanager system out log cache upper size](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-system-out-log-cache-upper-size) | 100 kb | MemorySize | The cache upper size when Flink caches current line context of `System.out` or `System.err` when `taskmanager.system-out.mode` is LOG. |
| ##### taskmanager.system-out.log.thread-name.enabled [Anchor link for: taskmanager system out log thread name enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-system-out-log-thread-name-enabled) | false | Boolean | Whether to log the thread name when `taskmanager.system-out.mode` is LOG. |
| ##### taskmanager.system-out.mode [Anchor link for: taskmanager system out mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-system-out-mode) | DEFAULT | Enum | Redirection mode of `System.out` and `System.err` for all `TaskManagers`.<br>- `DEFAULT`: `TaskManagers` don't redirect the `System.out` and `System.err`, it's the default value.<br>- `LOG`: `TaskManagers` redirect `System.out` and `System.err` to LOG.info and LOG.error.<br>- `IGNORE`: `TaskManagers` ignore `System.out` and `System.err` directly.<br>Possible values:<br>- "DEFAULT"<br>- "LOG"<br>- "IGNORE" |

**Data Transport Network Stack**

These options are for the network stack that handles the streaming and batch data exchanges between TaskManagers.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### taskmanager.network.compression.codec [Anchor link for: taskmanager network compression codec](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-compression-codec) | LZ4 | Enum | The codec to be used when compressing shuffle data. If it is "NONE", compression is disable. If it is not "NONE", only "LZ4", "LZO" and "ZSTD" are supported now. Through tpc-ds test of these three algorithms, the results show that "LZ4" algorithm has the highest compression and decompression speed, but the compression ratio is the lowest. "ZSTD" has the highest compression ratio, but the compression and decompression speed is the slowest, and LZO is between the two. Also note that this option is experimental and might be changed in the future.<br>Possible values:<br>- "NONE"<br>- "LZ4"<br>- "LZO"<br>- "ZSTD" |
| ##### taskmanager.network.detailed-metrics [Anchor link for: taskmanager network detailed metrics](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-detailed-metrics) | false | Boolean | Boolean flag to enable/disable more detailed metrics about inbound/outbound network queue lengths. |
| ##### taskmanager.network.hybrid-shuffle.external-remote-tier-factory.class [Anchor link for: taskmanager network hybrid shuffle external remote tier factory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-hybrid-shuffle-external-remote-tier-factory) | (none) | String | The option configures the class that is responsible for creating an external remote tier factory for hybrid shuffle. If configured, the hybrid shuffle will only initialize the specified remote tier according to the given class name. Currently, since the tier interfaces are not yet public and are still actively evolving, it is recommended that users do not independently implement the external remote tier until the tier interfaces are stabilized. |
| ##### taskmanager.network.hybrid-shuffle.remote.path [Anchor link for: taskmanager network hybrid shuffle remote path](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-hybrid-shuffle-remote-path) | (none) | String | The option is used to configure the base path of remote storage for hybrid shuffle. The shuffle data will be stored in remote storage when the disk space is not enough. Note: If this option is not configured the remote storage will be disabled. |
| ##### taskmanager.network.memory.buffer-debloat.enabled [Anchor link for: taskmanager network memory buffer debloat enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-memory-buffer-debloat-enabled) | false | Boolean | The switch of the automatic buffered debloating feature. If enabled the amount of in-flight data will be adjusted automatically accordingly to the measured throughput. |
| ##### taskmanager.network.memory.buffer-debloat.period [Anchor link for: taskmanager network memory buffer debloat period](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-memory-buffer-debloat-period) | 200 ms | Duration | The minimum period of time after which the buffer size will be debloated if required. The low value provides a fast reaction to the load fluctuation but can influence the performance. |
| ##### taskmanager.network.memory.buffer-debloat.samples [Anchor link for: taskmanager network memory buffer debloat samples](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-memory-buffer-debloat-samples) | 20 | Integer | The number of the last buffer size values that will be taken for the correct calculation of the new one. |
| ##### taskmanager.network.memory.buffer-debloat.target [Anchor link for: taskmanager network memory buffer debloat target](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-memory-buffer-debloat-target) | 1 s | Duration | The target total time after which buffered in-flight data should be fully consumed. This configuration option will be used, in combination with the measured throughput, to adjust the amount of in-flight data. |
| ##### taskmanager.network.memory.buffer-debloat.threshold-percentages [Anchor link for: taskmanager network memory buffer debloat threshold percentages](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-memory-buffer-debloat-threshold-percentages) | 25 | Integer | The minimum difference in percentage between the newly calculated buffer size and the old one to announce the new value. Can be used to avoid constant back and forth small adjustments. |
| ##### taskmanager.network.memory.read-buffer.required-per-gate.max [Anchor link for: taskmanager network memory read buffer required per gate max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-memory-read-buffer-required-per-gate-max) | (none) | Integer | The maximum number of network read buffers that are required by an input gate. (An input gate is responsible for reading data from all subtasks of an upstream task.) The number of buffers needed by an input gate is dynamically calculated in runtime, depending on various factors (e.g., the parallelism of the upstream task). Among the calculated number of needed buffers, the part below this configured value is required, while the excess part, if any, is optional. A task will fail if the required buffers cannot be obtained in runtime. A task will not fail due to not obtaining optional buffers, but may suffer a performance reduction. If not explicitly configured, the default value is Integer.MAX\_VALUE for streaming workloads, and 1000 for batch workloads. If explicitly configured, the configured value should be at least 1. |
| ##### taskmanager.network.netty.client.connectTimeoutSec [Anchor link for: taskmanager network netty client connecttimeoutsec](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-netty-client-connecttimeoutsec) | 120 | Integer | The Netty client connection timeout. |
| ##### taskmanager.network.netty.client.tcp.keepCount [Anchor link for: taskmanager network netty client tcp keepcount](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-netty-client-tcp-keepcount) | (none) | Integer | The maximum number of keepalive probes TCP should send before Netty client dropping the connection. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298. |
| ##### taskmanager.network.netty.client.tcp.keepIdleSec [Anchor link for: taskmanager network netty client tcp keepidlesec](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-netty-client-tcp-keepidlesec) | (none) | Integer | The time (in seconds) the connection needs to remain idle before TCP starts sending keepalive probes. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298. |
| ##### taskmanager.network.netty.client.tcp.keepIntervalSec [Anchor link for: taskmanager network netty client tcp keepintervalsec](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-netty-client-tcp-keepintervalsec) | (none) | Integer | The time (in seconds) between individual keepalive probes. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298. |
| ##### taskmanager.network.partition-request-timeout [Anchor link for: taskmanager network partition request timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-partition-request-timeout) | 10 s | Duration | Timeout for an individual partition request of remote input channels. The partition request will finally fail if the total wait time exceeds twice the value of `taskmanager.network.request-backoff.max`. |
| ##### taskmanager.network.request-backoff.initial [Anchor link for: taskmanager network request backoff initial](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-request-backoff-initial) | 100 | Integer | Minimum backoff in milliseconds for partition requests of local input channels. |
| ##### taskmanager.network.request-backoff.max [Anchor link for: taskmanager network request backoff max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-request-backoff-max) | 10000 | Integer | Maximum backoff in milliseconds for partition requests of local input channels. |
| ##### taskmanager.network.retries [Anchor link for: taskmanager network retries](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-retries) | 0 | Integer | The number of retry attempts for network communication. Currently it's only used for establishing input/output channel connections |
| ##### taskmanager.network.sort-shuffle.min-buffers [Anchor link for: taskmanager network sort shuffle min buffers](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-sort-shuffle-min-buffers) | 512 | Integer | Minimum number of network buffers required per blocking result partition for sort-shuffle. For production usage, it is suggested to increase this config value to at least 2048 (64M memory if the default 32K memory segment size is used) to improve the data compression ratio and reduce the small network packets. Usually, several hundreds of megabytes memory is enough for large scale batch jobs. Note: you may also need to increase the size of total network memory to avoid the 'insufficient number of network buffers' error if you are increasing this config value. |
| ##### taskmanager.network.tcp-connection.enable-reuse-across-jobs [Anchor link for: taskmanager network tcp connection enable reuse across jobs](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-network-tcp-connection-enable-reuse-across-jobs) | true | Boolean | Whether to reuse tcp connections across multi jobs. If set to true, tcp connections will not be released after job finishes. The subsequent jobs will be free from the overhead of the connection re-establish. However, this may lead to an increase in the total number of connections on your machine. When it reaches the upper limit, you can set it to false to release idle connections. |

### RPC / Pekko  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#rpc--pekko)

Flink uses Pekko for RPC between components (JobManager/TaskManager/ResourceManager).
Flink does not use Pekko for data transport.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### pekko.ask.callstack [Anchor link for: pekko ask callstack](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-ask-callstack) | true | Boolean | If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint. |
| ##### pekko.ask.timeout [Anchor link for: pekko ask timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-ask-timeout) | 10 s | Duration | Timeout used for all futures and blocking Pekko calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d). |
| ##### pekko.client-socket-worker-pool.pool-size-factor [Anchor link for: pekko client socket worker pool pool size factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-client-socket-worker-pool-pool-size-factor) | 1.0 | Double | The pool size factor is used to determine thread pool size using the following formula: ceil(available processors \* factor). Resulting size is then bounded by the pool-size-min and pool-size-max values. |
| ##### pekko.client-socket-worker-pool.pool-size-max [Anchor link for: pekko client socket worker pool pool size max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-client-socket-worker-pool-pool-size-max) | 2 | Integer | Max number of threads to cap factor-based number to. |
| ##### pekko.client-socket-worker-pool.pool-size-min [Anchor link for: pekko client socket worker pool pool size min](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-client-socket-worker-pool-pool-size-min) | 1 | Integer | Min number of threads to cap factor-based number to. |
| ##### pekko.fork-join-executor.parallelism-factor [Anchor link for: pekko fork join executor parallelism factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-fork-join-executor-parallelism-factor) | 2.0 | Double | The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors \* factor). Resulting size is then bounded by the parallelism-min and parallelism-max values. |
| ##### pekko.fork-join-executor.parallelism-max [Anchor link for: pekko fork join executor parallelism max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-fork-join-executor-parallelism-max) | 64 | Integer | Max number of threads to cap factor-based parallelism number to. |
| ##### pekko.fork-join-executor.parallelism-min [Anchor link for: pekko fork join executor parallelism min](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-fork-join-executor-parallelism-min) | 8 | Integer | Min number of threads to cap factor-based parallelism number to. |
| ##### pekko.framesize [Anchor link for: pekko framesize](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-framesize) | "10485760b" | String | Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier. |
| ##### pekko.jvm-exit-on-fatal-error [Anchor link for: pekko jvm exit on fatal error](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-jvm-exit-on-fatal-error) | true | Boolean | Exit JVM on fatal Pekko errors. |
| ##### pekko.log.lifecycle.events [Anchor link for: pekko log lifecycle events](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-log-lifecycle-events) | false | Boolean | Turns on the Pekko’s remote logging of events. Set this value to 'true' in case of debugging. |
| ##### pekko.lookup.timeout [Anchor link for: pekko lookup timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-lookup-timeout) | 10 s | Duration | Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d). |
| ##### pekko.remote-fork-join-executor.parallelism-factor [Anchor link for: pekko remote fork join executor parallelism factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-remote-fork-join-executor-parallelism-factor) | 2.0 | Double | The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors \* factor). Resulting size is then bounded by the parallelism-min and parallelism-max values. |
| ##### pekko.remote-fork-join-executor.parallelism-max [Anchor link for: pekko remote fork join executor parallelism max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-remote-fork-join-executor-parallelism-max) | 16 | Integer | Max number of threads to cap factor-based parallelism number to. |
| ##### pekko.remote-fork-join-executor.parallelism-min [Anchor link for: pekko remote fork join executor parallelism min](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-remote-fork-join-executor-parallelism-min) | 8 | Integer | Min number of threads to cap factor-based parallelism number to. |
| ##### pekko.retry-gate-closed-for [Anchor link for: pekko retry gate closed for](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-retry-gate-closed-for) | 50 | Long | Milliseconds a gate should be closed for after a remote connection was disconnected. |
| ##### pekko.server-socket-worker-pool.pool-size-factor [Anchor link for: pekko server socket worker pool pool size factor](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-server-socket-worker-pool-pool-size-factor) | 1.0 | Double | The pool size factor is used to determine thread pool size using the following formula: ceil(available processors \* factor). Resulting size is then bounded by the pool-size-min and pool-size-max values. |
| ##### pekko.server-socket-worker-pool.pool-size-max [Anchor link for: pekko server socket worker pool pool size max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-server-socket-worker-pool-pool-size-max) | 2 | Integer | Max number of threads to cap factor-based number to. |
| ##### pekko.server-socket-worker-pool.pool-size-min [Anchor link for: pekko server socket worker pool pool size min](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-server-socket-worker-pool-pool-size-min) | 1 | Integer | Min number of threads to cap factor-based number to. |
| ##### pekko.ssl.enabled [Anchor link for: pekko ssl enabled](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-ssl-enabled) | true | Boolean | Turns on SSL for Pekko’s remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true. |
| ##### pekko.startup-timeout [Anchor link for: pekko startup timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-startup-timeout) | (none) | Duration | Timeout after which the startup of a remote component is considered being failed. |
| ##### pekko.tcp.timeout [Anchor link for: pekko tcp timeout](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-tcp-timeout) | 20 s | Duration | Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value. |
| ##### pekko.throughput [Anchor link for: pekko throughput](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#pekko-throughput) | 15 | Integer | Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness. |

* * *

* * *

# JVM and Logging Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#jvm-and-logging-options)

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### env.hadoop.conf.dir [Anchor link for: env hadoop conf dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-hadoop-conf-dir) | (none) | String | Path to hadoop configuration directory. It is required to read HDFS and/or YARN configuration. You can also set it via environment variable. |
| ##### env.hbase.conf.dir [Anchor link for: env hbase conf dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-hbase-conf-dir) | (none) | String | Path to hbase configuration directory. It is required to read HBASE configuration. You can also set it via environment variable. |
| ##### env.java.default-opts.all [Anchor link for: env java default opts all](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-default-opts-all) | (none) | String | A string of default JVM options to prepend to `env.java.opts.all`. This is intended to be set by administrators. |
| ##### env.java.default-opts.jobmanager [Anchor link for: env java default opts jobmanager](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-default-opts-jobmanager) | (none) | String | A string of default JVM options to prepend to `env.java.opts.jobmanager`. This is intended to be set by administrators. |
| ##### env.java.default-opts.taskmanager [Anchor link for: env java default opts taskmanager](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-default-opts-taskmanager) | (none) | String | A string of default JVM options to prepend to `env.java.opts.taskmanager`. This is intended to be set by administrators. |
| ##### env.java.home [Anchor link for: env java home](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-home) | (none) | String | Location where Java is installed. If not specified, Flink will use your default Java installation. |
| ##### env.java.opts.all [Anchor link for: env java opts all](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-opts-all) | (none) | String | Java options to start the JVM of all Flink processes with. |
| ##### env.java.opts.client [Anchor link for: env java opts client](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-opts-client) | (none) | String | Java options to start the JVM of the Flink Client with. |
| ##### env.java.opts.historyserver [Anchor link for: env java opts historyserver](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-opts-historyserver) | (none) | String | Java options to start the JVM of the HistoryServer with. |
| ##### env.java.opts.jobmanager [Anchor link for: env java opts jobmanager](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-opts-jobmanager) | (none) | String | Java options to start the JVM of the JobManager with. |
| ##### env.java.opts.sql-gateway [Anchor link for: env java opts sql gateway](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-opts-sql-gateway) | (none) | String | Java options to start the JVM of the Flink SQL Gateway with. |
| ##### env.java.opts.taskmanager [Anchor link for: env java opts taskmanager](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-java-opts-taskmanager) | (none) | String | Java options to start the JVM of the TaskManager with. |
| ##### env.log.dir [Anchor link for: env log dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-log-dir) | (none) | String | Defines the directory where the Flink logs are saved. It has to be an absolute path. (Defaults to the log directory under Flink’s home) |
| ##### env.log.level [Anchor link for: env log level](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-log-level) | "INFO" | String | Defines the level of the root logger. |
| ##### env.log.max [Anchor link for: env log max](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-log-max) | 10 | Integer | The maximum number of old log files to keep. |
| ##### env.pid.dir [Anchor link for: env pid dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-pid-dir) | "/tmp" | String | Defines the directory where the flink-<host>-<process>.pid files are saved. |
| ##### env.ssh.opts [Anchor link for: env ssh opts](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-ssh-opts) | (none) | String | Additional command line options passed to SSH clients when starting or stopping JobManager, TaskManager, and Zookeeper services (start-cluster.sh, stop-cluster.sh, start-zookeeper-quorum.sh, stop-zookeeper-quorum.sh). |
| ##### env.stdout-err.redirect-to-file [Anchor link for: env stdout err redirect to file](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-stdout-err-redirect-to-file) | false | Boolean | Whether redirect stdout and stderr to files when running foreground. If enabled, logs won't append the console too. Note that redirected files do not support rolling rotate. |
| ##### env.yarn.conf.dir [Anchor link for: env yarn conf dir](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#env-yarn-conf-dir) | (none) | String | Path to yarn configuration directory. It is required to run flink on YARN. You can also set it via environment variable. |

# Forwarding Environment Variables  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#forwarding-environment-variables)

You can configure environment variables to be set on the JobManager and TaskManager processes started on Yarn.

- `containerized.master.env.`: Prefix for passing custom environment variables to Flink’s JobManager process.
For example for passing LD\_LIBRARY\_PATH as an env variable to the JobManager, set containerized.master.env.LD\_LIBRARY\_PATH: “/usr/lib/native”
in the config.yaml.

- `containerized.taskmanager.env.`: Similar to the above, this configuration prefix allows setting custom environment variables for the workers (TaskManagers).


* * *

* * *

# Deprecated Options  [\#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#deprecated-options)

These options relate to parts of Flink that are not actively developed any more.
These options may be removed in a future release.

**Optimizer**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### compiler.delimited-informat.max-line-samples [Anchor link for: compiler delimited informat max line samples](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#compiler-delimited-informat-max-line-samples) | 10 | Integer | The maximum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters. |
| ##### compiler.delimited-informat.max-sample-len [Anchor link for: compiler delimited informat max sample len](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#compiler-delimited-informat-max-sample-len) | 2097152 | Integer | The maximal length of a line sample that the compiler takes for delimited inputs. If the length of a single sample exceeds this value (possible because of misconfiguration of the parser), the sampling aborts. This value can be overridden for a specific input with the input format’s parameters. |
| ##### compiler.delimited-informat.min-line-samples [Anchor link for: compiler delimited informat min line samples](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#compiler-delimited-informat-min-line-samples) | 2 | Integer | The minimum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters |

**Runtime Algorithms**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### taskmanager.runtime.hashjoin-bloom-filters [Anchor link for: taskmanager runtime hashjoin bloom filters](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-runtime-hashjoin-bloom-filters) | false | Boolean | Flag to activate/deactivate bloom filters in the hybrid hash join implementation. In cases where the hash join needs to spill to disk (datasets larger than the reserved fraction of memory), these bloom filters can greatly reduce the number of spilled records, at the cost some CPU cycles. |
| ##### taskmanager.runtime.large-record-handler [Anchor link for: taskmanager runtime large record handler](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-runtime-large-record-handler) | false | Boolean | Whether to use the LargeRecordHandler when spilling. If a record will not fit into the sorting buffer. The record will be spilled on disk and the sorting will continue with only the key. The record itself will be read afterwards when merging. |
| ##### taskmanager.runtime.max-fan [Anchor link for: taskmanager runtime max fan](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-runtime-max-fan) | 128 | Integer | The maximal fan-in for external merge joins and fan-out for spilling hash tables. Limits the number of file handles per operator, but may cause intermediate merging/partitioning, if set too small. |
| ##### taskmanager.runtime.sort-spilling-threshold [Anchor link for: taskmanager runtime sort spilling threshold](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#taskmanager-runtime-sort-spilling-threshold) | 0.8 | Float | A sort operation starts spilling when this fraction of its memory budget is full. |

**File Sinks**

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### fs.output.always-create-directory [Anchor link for: fs output always create directory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#fs-output-always-create-directory) | false | Boolean | File writers running with a parallelism larger than one create a directory for the output file path and put the different result files (one per parallel writer task) into that directory. If this option is set to "true", writers with a parallelism of 1 will also create a directory and place a single result file into it. If the option is set to "false", the writer will directly create the file directly at the output path, without creating a containing directory. |
| ##### fs.overwrite-files [Anchor link for: fs overwrite files](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/\#fs-overwrite-files) | false | Boolean | Specifies whether file output writers should overwrite existing files by default. Set to "true" to overwrite by default,"false" otherwise. |