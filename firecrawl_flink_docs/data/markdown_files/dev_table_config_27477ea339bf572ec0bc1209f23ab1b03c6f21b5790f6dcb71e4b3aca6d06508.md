# Configuration  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#configuration)

By default, the Table & SQL API is preconfigured for producing accurate results with acceptable
performance.

Depending on the requirements of a table program, it might be necessary to adjust
certain parameters for optimization. For example, unbounded streaming programs may need to ensure
that the required state size is capped (see [streaming concepts](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/concepts/overview/)).

### Overview  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#overview)

When instantiating a `TableEnvironment`, `EnvironmentSettings` can be used to pass the desired
configuration for the current session, by passing a `Configuration` object to the
`EnvironmentSettings`.

Additionally, in every table environment, the `TableConfig` offers options for configuring the
current session.

For common or important configuration options, the `TableConfig` provides getters and setters methods
with detailed inline documentation.

For more advanced configuration, users can directly access the underlying key-value map. The following
sections list all available options that can be used to adjust Flink Table & SQL API programs.

Attention Because options are read at different point in time
when performing operations, it is recommended to set configuration options early after instantiating a
table environment.

Java

```java
// instantiate table environment
Configuration configuration = new Configuration();
// set low-level key-value options
configuration.setString("table.exec.mini-batch.enabled", "true");
configuration.setString("table.exec.mini-batch.allow-latency", "5 s");
configuration.setString("table.exec.mini-batch.size", "5000");
EnvironmentSettings settings = EnvironmentSettings.newInstance()
        .inStreamingMode().withConfiguration(configuration).build();
TableEnvironment tEnv = TableEnvironment.create(settings);

// access flink configuration after table environment instantiation
TableConfig tableConfig = tEnv.getConfig();
// set low-level key-value options
tableConfig.set("table.exec.mini-batch.enabled", "true");
tableConfig.set("table.exec.mini-batch.allow-latency", "5 s");
tableConfig.set("table.exec.mini-batch.size", "5000");
```

Scala

```scala
// instantiate table environment
val configuration = new Configuration;
// set low-level key-value options
configuration.setString("table.exec.mini-batch.enabled", "true")
configuration.setString("table.exec.mini-batch.allow-latency", "5 s")
configuration.setString("table.exec.mini-batch.size", "5000")
val settings = EnvironmentSettings.newInstance
  .inStreamingMode.withConfiguration(configuration).build
val tEnv: TableEnvironment = TableEnvironment.create(settings)

// access flink configuration after table environment instantiation
val tableConfig = tEnv.getConfig()
// set low-level key-value options
tableConfig.set("table.exec.mini-batch.enabled", "true")
tableConfig.set("table.exec.mini-batch.allow-latency", "5 s")
tableConfig.set("table.exec.mini-batch.size", "5000")
```

Python

```python
# instantiate table environment
configuration = Configuration()
configuration.set("table.exec.mini-batch.enabled", "true")
configuration.set("table.exec.mini-batch.allow-latency", "5 s")
configuration.set("table.exec.mini-batch.size", "5000")
settings = EnvironmentSettings.new_instance() \
...     .in_streaming_mode() \
...     .with_configuration(configuration) \
...     .build()

t_env = TableEnvironment.create(settings)

# access flink configuration after table environment instantiation
table_config = t_env.get_config()
# set low-level key-value options
table_config.set("table.exec.mini-batch.enabled", "true")
table_config.set("table.exec.mini-batch.allow-latency", "5 s")
table_config.set("table.exec.mini-batch.size", "5000")
```

SQL CLI

```
Flink SQL> SET 'table.exec.mini-batch.enabled' = 'true';
Flink SQL> SET 'table.exec.mini-batch.allow-latency' = '5s';
Flink SQL> SET 'table.exec.mini-batch.size' = '5000';
```

> **Note:** All of the following configuration options can also be set globally in
> [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file) and can be later
> on overridden in the application, through `EnvironmentSettings`, before instantiating
> the `TableEnvironment`, or through the `TableConfig` of the `TableEnvironment`.

### Execution Options  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#execution-options)

The following options can be used to tune the performance of the query execution.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### table.exec.async-lookup.buffer-capacity [Anchor link for: table exec async lookup buffer capacity](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-lookup-buffer-capacity)<br>BatchStreaming | 100 | Integer | The max number of async i/o operation that the async lookup join can trigger. |
| ##### table.exec.async-lookup.key-ordered-enabled [Anchor link for: table exec async lookup key ordered enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-lookup-key-ordered-enabled)<br>BatchStreaming | false | Boolean | When true, async lookup joins would follow the upsert key order in cdc streams. If there is no defined upsert key, then the total record is considered as the upsert key. Setting this for insert-only streams has no effect because record in insert-only streams is independent and does not affect the state of previous records. Besides, since records in insert-only streams typically do not involve a primary key then no upsertKey can be derived. This makes them be unordered processed even if key ordered enabled. |
| ##### table.exec.async-lookup.output-mode [Anchor link for: table exec async lookup output mode](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-lookup-output-mode)<br>BatchStreaming | ORDERED | Enum | Output mode for asynchronous operations which will convert to {@see AsyncDataStream.OutputMode}, ORDERED by default. If set to ALLOW\_UNORDERED, will attempt to use {@see AsyncDataStream.OutputMode.UNORDERED} when it does not affect the correctness of the result, otherwise ORDERED will be still used.<br>Possible values:<br>- "ORDERED"<br>- "ALLOW\_UNORDERED" |
| ##### table.exec.async-lookup.timeout [Anchor link for: table exec async lookup timeout](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-lookup-timeout)<br>BatchStreaming | 3 min | Duration | The async timeout for the asynchronous operation to complete. |
| ##### table.exec.async-ml-predict.max-concurrent-operations [Anchor link for: table exec async ml predict max concurrent operations](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-ml-predict-max-concurrent-operations)<br>BatchStreaming | 10 | Integer | The max number of async i/o operation that the async ml predict can trigger. |
| ##### table.exec.async-ml-predict.output-mode [Anchor link for: table exec async ml predict output mode](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-ml-predict-output-mode)<br>BatchStreaming | ORDERED | Enum | Output mode for async ML predict, which describes whether or not the the output should attempt to be ordered or not. The supported options are: ALLOW\_UNORDERED means the operator emit the result when execution finishes. The planner will attempt use ALLOW\_UNORDERED whn it doesn't affect the correctness of the results.<br>ORDERED ensures that the operator emits the result in the same order as the data enters it. This is the default.<br>Possible values:<br>- "ORDERED"<br>- "ALLOW\_UNORDERED" |
| ##### table.exec.async-ml-predict.timeout [Anchor link for: table exec async ml predict timeout](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-ml-predict-timeout)<br>BatchStreaming | 3 min | Duration | The async timeout for the asynchronous operation to complete. If the deadline fails, a timeout exception will be thrown to indicate the error. |
| ##### table.exec.async-scalar.max-attempts [Anchor link for: table exec async scalar max attempts](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-scalar-max-attempts)<br>Streaming | 3 | Integer | The max number of async retry attempts to make before task execution is failed. |
| ##### table.exec.async-scalar.max-concurrent-operations [Anchor link for: table exec async scalar max concurrent operations](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-scalar-max-concurrent-operations)<br>Streaming | 10 | Integer | The max number of async i/o operation that the async scalar function can trigger. |
| ##### table.exec.async-scalar.retry-delay [Anchor link for: table exec async scalar retry delay](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-scalar-retry-delay)<br>Streaming | 100 ms | Duration | The delay to wait before trying again. |
| ##### table.exec.async-scalar.retry-strategy [Anchor link for: table exec async scalar retry strategy](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-scalar-retry-strategy)<br>Streaming | FIXED\_DELAY | Enum | Restart strategy which will be used, FIXED\_DELAY by default.<br>Possible values:<br>- "NO\_RETRY"<br>- "FIXED\_DELAY" |
| ##### table.exec.async-scalar.timeout [Anchor link for: table exec async scalar timeout](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-scalar-timeout)<br>Streaming | 3 min | Duration | The async timeout for the asynchronous operation to complete. |
| ##### table.exec.async-state.enabled [Anchor link for: table exec async state enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-state-enabled)<br>Streaming | false | Boolean | Set whether to use the SQL/Table operators based on the asynchronous state api. Default value is false. |
| ##### table.exec.async-table.max-concurrent-operations [Anchor link for: table exec async table max concurrent operations](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-table-max-concurrent-operations)<br>Streaming | 10 | Integer | The max number of concurrent async i/o operations that the async table function can trigger. |
| ##### table.exec.async-table.max-retries [Anchor link for: table exec async table max retries](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-table-max-retries)<br>Streaming | 3 | Integer | The max number of async retry attempts to make before task execution is failed. |
| ##### table.exec.async-table.retry-delay [Anchor link for: table exec async table retry delay](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-table-retry-delay)<br>Streaming | 100 ms | Duration | The delay to wait before trying again. |
| ##### table.exec.async-table.retry-strategy [Anchor link for: table exec async table retry strategy](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-table-retry-strategy)<br>Streaming | FIXED\_DELAY | Enum | Restart strategy which will be used, FIXED\_DELAY by default.<br>Possible values:<br>- "NO\_RETRY"<br>- "FIXED\_DELAY" |
| ##### table.exec.async-table.timeout [Anchor link for: table exec async table timeout](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-table-timeout)<br>Streaming | 3 min | Duration | The async timeout for the asynchronous operation to complete, including any retries which may occur. |
| ##### table.exec.async-vector-search.max-concurrent-operations [Anchor link for: table exec async vector search max concurrent operations](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-vector-search-max-concurrent-operations)<br>BatchStreaming | 10 | Integer | The max number of async i/o operation that the async vector search can trigger. |
| ##### table.exec.async-vector-search.output-mode [Anchor link for: table exec async vector search output mode](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-vector-search-output-mode)<br>BatchStreaming | ORDERED | Enum | Output mode for async vector search, which describes whether or not the output should attempt to be ordered or not.<br>The supported options are:<br>ALLOW\_UNORDERED means the operator emits the result when execution finishes. The planner will attempt to use ALLOW\_UNORDERED when it doesn't affect the correctness of the results.<br>ORDERED means that the operator emits the result in the same order as the data enters it. This is the default.<br>Possible values:<br>- "ORDERED"<br>- "ALLOW\_UNORDERED" |
| ##### table.exec.async-vector-search.timeout [Anchor link for: table exec async vector search timeout](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-async-vector-search-timeout)<br>BatchStreaming | 3 min | Duration | The total time which can pass before the invocation (including retries) is considered timed out and task execution is failed. |
| ##### table.exec.deduplicate.insert-update-after-sensitive-enabled [Anchor link for: table exec deduplicate insert update after sensitive enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-deduplicate-insert-update-after-sensitive-enabled)<br>Streaming | true | Boolean | Set whether the job (especially the sinks) is sensitive to INSERT messages and UPDATE\_AFTER messages. If false, Flink may, sometimes (e.g. deduplication for last row), send UPDATE\_AFTER instead of INSERT for the first row. If true, Flink will guarantee to send INSERT for the first row, in that case there will be additional overhead. Default is true. |
| ##### table.exec.deduplicate.mini-batch.compact-changes-enabled [Anchor link for: table exec deduplicate mini batch compact changes enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-deduplicate-mini-batch-compact-changes-enabled)<br>Streaming | false | Boolean | Set whether to compact the changes sent downstream in row-time mini-batch. If true, Flink will compact changes and send only the latest change downstream. Note that if the downstream needs the details of versioned data, this optimization cannot be applied. If false, Flink will send all changes to downstream just like when the mini-batch is not enabled. |
| ##### table.exec.delta-join.cache-enabled [Anchor link for: table exec delta join cache enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-delta-join-cache-enabled)<br>Streaming | true | Boolean | Whether to enable the cache of delta join. If enabled, the delta join caches the records from remote dim table. Default is true. |
| ##### table.exec.delta-join.left.cache-size [Anchor link for: table exec delta join left cache size](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-delta-join-left-cache-size)<br>Streaming | 10000 | Long | The cache size used to cache the lookup results of the left table in delta join. This value must be positive when enabling cache. Default is 10000. |
| ##### table.exec.delta-join.right.cache-size [Anchor link for: table exec delta join right cache size](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-delta-join-right-cache-size)<br>Streaming | 10000 | Long | The cache size used to cache the lookup results of the right table in delta join. This value must be positive when enabling cache. Default is 10000. |
| ##### table.exec.disabled-operators [Anchor link for: table exec disabled operators](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-disabled-operators)<br>Batch | (none) | String | Mainly for testing. A comma-separated list of operator names, each name represents a kind of disabled operator.<br>Operators that can be disabled include "NestedLoopJoin", "ShuffleHashJoin", "BroadcastHashJoin", "SortMergeJoin", "HashAgg", "SortAgg".<br>By default no operator is disabled. |
| ##### table.exec.interval-join.min-cleanup-interval [Anchor link for: table exec interval join min cleanup interval](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-interval-join-min-cleanup-interval)<br>Streaming | 0 ms | Duration | Specifies a minimum time interval for how long cleanup unmatched records in the interval join operator. Before Flink 1.18, the default value of this param was the half of interval duration. Note: Set this option greater than 0 will cause unmatched records in outer joins to be output later than watermark, leading to possible discarding of these records by downstream watermark-dependent operators, such as window operators. The default value is 0, which means it will clean up unmatched records immediately. |
| ##### table.exec.legacy-cast-behaviour [Anchor link for: table exec legacy cast behaviour](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-legacy-cast-behaviour)<br>BatchStreaming | DISABLED | Enum | Determines whether CAST will operate following the legacy behaviour or the new one that introduces various fixes and improvements.<br>Possible values:<br>- "ENABLED": CAST will operate following the legacy behaviour.<br>- "DISABLED": CAST will operate following the new correct behaviour. |
| ##### table.exec.local-hash-agg.adaptive.distinct-value-rate-threshold [Anchor link for: table exec local hash agg adaptive distinct value rate threshold](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-local-hash-agg-adaptive-distinct-value-rate-threshold)<br>Batch | 0.5 | Double | The distinct value rate can be defined as the number of local aggregation results for the sampled data divided by the sampling threshold (see table.exec.local-hash-agg.adaptive.sampling-threshold). If the computed result is lower than the given configuration value, the remaining input records proceed to do local aggregation, otherwise the remaining input records are subjected to simple projection which calculation cost is less than local aggregation. The default value is 0.5. |
| ##### table.exec.local-hash-agg.adaptive.enabled [Anchor link for: table exec local hash agg adaptive enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-local-hash-agg-adaptive-enabled)<br>Batch | true | Boolean | Whether to enable adaptive local hash aggregation. Adaptive local hash aggregation is an optimization of local hash aggregation, which can adaptively determine whether to continue to do local hash aggregation according to the distinct value rate of sampling data. If distinct value rate bigger than defined threshold (see parameter: table.exec.local-hash-agg.adaptive.distinct-value-rate-threshold), we will stop aggregating and just send the input data to the downstream after a simple projection. Otherwise, we will continue to do aggregation. Adaptive local hash aggregation only works in batch mode. Default value of this parameter is true. |
| ##### table.exec.local-hash-agg.adaptive.sampling-threshold [Anchor link for: table exec local hash agg adaptive sampling threshold](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-local-hash-agg-adaptive-sampling-threshold)<br>Batch | 500000 | Long | If adaptive local hash aggregation is enabled, this value defines how many records will be used as sampled data to calculate distinct value rate (see parameter: table.exec.local-hash-agg.adaptive.distinct-value-rate-threshold) for the local aggregate. The higher the sampling threshold, the more accurate the distinct value rate is. But as the sampling threshold increases, local aggregation is meaningless when the distinct values rate is low. The default value is 500000. |
| ##### table.exec.mini-batch.allow-latency [Anchor link for: table exec mini batch allow latency](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-mini-batch-allow-latency)<br>Streaming | 0 ms | Duration | The maximum latency can be used for MiniBatch to buffer input records. MiniBatch is an optimization to buffer input records to reduce state access. MiniBatch is triggered with the allowed latency interval and when the maximum number of buffered records reached. NOTE: If table.exec.mini-batch.enabled is set true, its value must be greater than zero. |
| ##### table.exec.mini-batch.enabled [Anchor link for: table exec mini batch enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-mini-batch-enabled)<br>Streaming | false | Boolean | Specifies whether to enable MiniBatch optimization. MiniBatch is an optimization to buffer input records to reduce state access. This is disabled by default. To enable this, users should set this config to true. NOTE: If mini-batch is enabled, 'table.exec.mini-batch.allow-latency' and 'table.exec.mini-batch.size' must be set. |
| ##### table.exec.mini-batch.size [Anchor link for: table exec mini batch size](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-mini-batch-size)<br>Streaming | -1 | Long | The maximum number of input records can be buffered for MiniBatch. MiniBatch is an optimization to buffer input records to reduce state access. MiniBatch is triggered with the allowed latency interval and when the maximum number of buffered records reached. NOTE: MiniBatch only works for non-windowed aggregations currently. If table.exec.mini-batch.enabled is set true, its value must be positive. |
| ##### table.exec.operator-fusion-codegen.enabled [Anchor link for: table exec operator fusion codegen enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-operator-fusion-codegen-enabled)<br>BatchStreaming | false | Boolean | If true, multiple physical operators will be compiled into a single operator by planner which can improve the performance. |
| ##### table.exec.rank.topn-cache-size [Anchor link for: table exec rank topn cache size](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-rank-topn-cache-size)<br>Streaming | 10000 | Long | Rank operators have a cache which caches partial state contents to reduce state access. Cache size is the number of records in each ranking task. |
| ##### table.exec.resource.default-parallelism [Anchor link for: table exec resource default parallelism](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-resource-default-parallelism)<br>BatchStreaming | -1 | Integer | Sets default parallelism for all operators (such as aggregate, join, filter) to run with parallel instances. This config has a higher priority than parallelism of StreamExecutionEnvironment (actually, this config overrides the parallelism of StreamExecutionEnvironment). A value of -1 indicates that no default parallelism is set, then it will fallback to use the parallelism of StreamExecutionEnvironment. |
| ##### table.exec.simplify-operator-name-enabled [Anchor link for: table exec simplify operator name enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-simplify-operator-name-enabled)<br>BatchStreaming | true | Boolean | When it is true, the optimizer will simplify the operator name with id and type of ExecNode and keep detail in description. Default value is true. |
| ##### table.exec.sink.keyed-shuffle [Anchor link for: table exec sink keyed shuffle](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-keyed-shuffle)<br>Streaming | AUTO | Enum | In order to minimize the distributed disorder problem when writing data into table with primary keys that many users suffers. FLINK will auto add a keyed shuffle by default when the sink parallelism differs from upstream operator and sink parallelism is not 1. This works only when the upstream ensures the multi-records' order on the primary key, if not, the added shuffle can not solve the problem (In this situation, a more proper way is to consider the deduplicate operation for the source firstly or use an upsert source with primary key definition which truly reflect the records evolution).<br>By default, the keyed shuffle will be added when the sink's parallelism differs from upstream operator. You can set to no shuffle(NONE) or force shuffle(FORCE).<br>Possible values:<br>- "NONE"<br>- "AUTO"<br>- "FORCE" |
| ##### table.exec.sink.nested-constraint-enforcer [Anchor link for: table exec sink nested constraint enforcer](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-nested-constraint-enforcer)<br>BatchStreaming | IGNORE | Enum | Determines if constraints should be enforced for nested fields. Beware that enforcing constraints for nested fields adds computational overhead especially when iterating through collections<br>Possible values:<br>- "IGNORE": Don't perform check on nested types in ROWS/ARRAYS/MAPS<br>- "ROWS": Perform checks on nested types in ROWS.<br>- "ROWS\_AND\_COLLECTIONS": Perform checks on nested types in ROWS/ARRAYS/MAPS. Be aware that the more checks the more performance impact. Especially checking types in ARRAYS/MAPS can be expensive. |
| ##### table.exec.sink.not-null-enforcer [Anchor link for: table exec sink not null enforcer](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-not-null-enforcer)<br>BatchStreaming | ERROR | Enum | Determines how Flink enforces NOT NULL column constraints when inserting null values.<br>Possible values:<br>- "ERROR": Throw a runtime exception when writing null values into NOT NULL column.<br>- "DROP": Drop records silently if a null value would have to be inserted into a NOT NULL column. |
| ##### table.exec.sink.rowtime-inserter [Anchor link for: table exec sink rowtime inserter](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-rowtime-inserter)<br>Streaming | ENABLED | Enum | Some sink implementations require a single rowtime attribute in the input that can be inserted into the underlying stream record. This option allows disabling the timestamp insertion and avoids errors around multiple time attributes being present in the query schema.<br>Possible values:<br>- "ENABLED": Insert a rowtime attribute (if available) into the underlying stream record. This requires at most one time attribute in the input for the sink.<br>- "DISABLED": Do not insert the rowtime attribute into the underlying stream record. |
| ##### table.exec.sink.type-length-enforcer [Anchor link for: table exec sink type length enforcer](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-type-length-enforcer)<br>BatchStreaming | IGNORE | Enum | Determines whether values for columns with CHAR(<length>)/VARCHAR(<length>)/BINARY(<length>)/VARBINARY(<length>) types will be trimmed or padded (only for CHAR(<length>)/BINARY(<length>)), so that their length will match the one defined by the length of their respective CHAR/VARCHAR/BINARY/VARBINARY column type.<br>Possible values:<br>- "IGNORE": Don't apply any trimming and padding, and instead ignore the CHAR/VARCHAR/BINARY/VARBINARY length directive.<br>- "TRIM\_PAD": Trim and pad string and binary values to match the length defined by the CHAR/VARCHAR/BINARY/VARBINARY length.<br>- "ERROR": Throw a runtime exception when writing data into a CHAR/VARCHAR/BINARY/VARBINARY column which does not match the length constraint |
| ##### table.exec.sink.upsert-materialize [Anchor link for: table exec sink upsert materialize](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-upsert-materialize)<br>Streaming | AUTO | Enum | Because of the disorder of ChangeLog data caused by Shuffle in distributed system, the data received by Sink may not be the order of global upsert. So add upsert materialize operator before upsert sink. It receives the upstream changelog records and generate an upsert view for the downstream.<br>By default, the materialize operator will be added when a distributed disorder occurs on unique keys. You can also choose no materialization(NONE) or force materialization(FORCE).<br>Possible values:<br>- "NONE"<br>- "AUTO"<br>- "FORCE" |
| ##### table.exec.sink.upsert-materialize-strategy.adaptive.threshold.high [Anchor link for: table exec sink upsert materialize strategy adaptive threshold h](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-upsert-materialize-strategy-adaptive-threshold-h)<br>Streaming | (none) | Long | When using strategy=ADAPTIVE, defines the number of entries per key when the implementation is changed from VALUE to MAP. If not specified, Flink uses state-backend specific defaults (400 for hashmap state backend and 50 for RocksDB and the rest). |
| ##### table.exec.sink.upsert-materialize-strategy.adaptive.threshold.low [Anchor link for: table exec sink upsert materialize strategy adaptive threshold l](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-upsert-materialize-strategy-adaptive-threshold-l)<br>Streaming | (none) | Long | When using strategy=ADAPTIVE, defines the number of entries per key when the implementation is changed from MAP to VALUE. If not specified, Flink uses state-backend specific defaults (300 for hashmap state backend and 40 for RocksDB and the rest). |
| ##### table.exec.sink.upsert-materialize-strategy.type [Anchor link for: table exec sink upsert materialize strategy type](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sink-upsert-materialize-strategy-type)<br>Streaming | LEGACY | Enum | Which strategy of SinkUpsertMaterializer to use. Supported strategies:<br>LEGACY: Simple implementation based on ValueState<List> (the original implementation).<br>MAP: SequencedMultiSetState implementation based on a combination of several MapState maintaining ordering and fast lookup properties.<br>VALUE: Similar to LEGACY, but compatible with MAP and therefore allows to switch to ADAPTIVE.<br>ADAPTIVE: Alternate between MAP and VALUE depending on the number of entries for the given key starting with VALUE and switching to MAP upon reaching threshold.high value (and back to VALUE, when reaching low).<br>Possible values:<br>- "LEGACY"<br>- "MAP"<br>- "VALUE"<br>- "ADAPTIVE" |
| ##### table.exec.sort.async-merge-enabled [Anchor link for: table exec sort async merge enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sort-async-merge-enabled)<br>Batch | true | Boolean | Whether to asynchronously merge sorted spill files. |
| ##### table.exec.sort.default-limit [Anchor link for: table exec sort default limit](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sort-default-limit)<br>Batch | -1 | Integer | Default limit when user don't set a limit after order by. -1 indicates that this configuration is ignored. |
| ##### table.exec.sort.max-num-file-handles [Anchor link for: table exec sort max num file handles](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-sort-max-num-file-handles)<br>Batch | 128 | Integer | The maximal fan-in for external merge sort. It limits the number of file handles per operator. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading. |
| ##### table.exec.source.cdc-events-duplicate [Anchor link for: table exec source cdc events duplicate](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-source-cdc-events-duplicate)<br>Streaming | false | Boolean | Indicates whether the CDC (Change Data Capture) sources in the job will produce duplicate change events that requires the framework to deduplicate and get consistent result. CDC source refers to the source that produces full change events, including INSERT/UPDATE\_BEFORE/UPDATE\_AFTER/DELETE, for example Kafka source with Debezium format. The value of this configuration is false by default.<br>However, it's a common case that there are duplicate change events. Because usually the CDC tools (e.g. Debezium) work in at-least-once delivery when failover happens. Thus, in the abnormal situations Debezium may deliver duplicate change events to Kafka and Flink will get the duplicate events. This may cause Flink query to get wrong results or unexpected exceptions.<br>Therefore, it is recommended to turn on this configuration if your CDC tool is at-least-once delivery. Enabling this configuration requires to define PRIMARY KEY on the CDC sources. The primary key will be used to deduplicate change events and generate normalized changelog stream at the cost of an additional stateful operator. |
| ##### table.exec.source.idle-timeout [Anchor link for: table exec source idle timeout](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-source-idle-timeout)<br>Streaming | 0 ms | Duration | When a source do not receive any elements for the timeout time, it will be marked as temporarily idle. This allows downstream tasks to advance their watermarks without the need to wait for watermarks from this source while it is idle. Default value is 0, which means detecting source idleness is not enabled. |
| ##### table.exec.spill-compression.block-size [Anchor link for: table exec spill compression block size](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-spill-compression-block-size)<br>Batch | 64 kb | MemorySize | The memory size used to do compress when spilling data. The larger the memory, the higher the compression ratio, but more memory resource will be consumed by the job. |
| ##### table.exec.spill-compression.enabled [Anchor link for: table exec spill compression enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-spill-compression-enabled)<br>Batch | true | Boolean | Whether to compress spilled data. Currently we only support compress spilled data for sort and hash-agg and hash-join operators. |
| ##### table.exec.state.ttl [Anchor link for: table exec state ttl](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-state-ttl)<br>Streaming | 0 ms | Duration | Specifies a minimum time interval for how long idle state (i.e. state which was not updated), will be retained. State will never be cleared until it was idle for less than the minimum time, and will be cleared at some time after it was idle. Default is never clean-up the state. NOTE: Cleaning up state requires additional overhead for bookkeeping. Default value is 0, which means that it will never clean up state. |
| ##### table.exec.uid.format [Anchor link for: table exec uid format](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-uid-format)<br>Streaming | "<id>\_<transformation>" | String | Defines the format pattern for generating the UID of an ExecNode streaming transformation. The pattern can be defined globally or per-ExecNode in the compiled plan. Supported arguments are: <id> (from static counter), <type> (e.g. 'stream-exec-sink'), <version>, and <transformation> (e.g. 'constraint-validator' for a sink). In Flink 1.15.x the pattern was wrongly defined as '<id>\_<type>\_<version>\_<transformation>' which would prevent migrations in the future. |
| ##### table.exec.uid.generation [Anchor link for: table exec uid generation](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-uid-generation)<br>Streaming | PLAN\_ONLY | Enum | In order to remap state to operators during a restore, it is required that the pipeline's streaming transformations get a UID assigned.<br>The planner can generate and assign explicit UIDs. If no UIDs have been set by the planner, the UIDs will be auto-generated by lower layers that can take the complete topology into account for uniqueness of the IDs. See the DataStream API for more information.<br>This configuration option is for experts only and the default should be sufficient for most use cases. By default, only pipelines created from a persisted compiled plan will get UIDs assigned explicitly. Thus, these pipelines can be arbitrarily moved around within the same topology without affecting the stable UIDs.<br>Possible values:<br>- "PLAN\_ONLY": Sets UIDs on streaming transformations if and only if the pipeline definition comes from a compiled plan. Pipelines that have been constructed in the API without a compilation step will not set an explicit UID as it might not be stable across multiple translations.<br>- "ALWAYS": Always sets UIDs on streaming transformations. This strategy is for experts only! Pipelines that have been constructed in the API without a compilation step might not be able to be restored properly. The UID generation depends on previously declared pipelines (potentially across jobs if the same JVM is used). Thus, a stable environment must be ensured. Pipeline definitions that come from a compiled plan are safe to use.<br>- "DISABLED": No explicit UIDs will be set. |
| ##### table.exec.unbounded-over.version [Anchor link for: table exec unbounded over version](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-unbounded-over-version)<br>Streaming | 2 | Integer | Which version of the unbounded over aggregation to use: 1 - legacy version 2 - version with improved performance |
| ##### table.exec.window-agg.buffer-size-limit [Anchor link for: table exec window agg buffer size limit](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-exec-window-agg-buffer-size-limit)<br>Batch | 100000 | Integer | Sets the window elements buffer size limit used in group window agg operator. |

### Optimizer Options  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#optimizer-options)

The following options can be used to adjust the behavior of the query optimizer to get a better execution plan.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### table.optimizer.adaptive-broadcast-join.strategy [Anchor link for: table optimizer adaptive broadcast join strategy](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-adaptive-broadcast-join-strategy)<br>Batch | auto | Enum | Flink will perform broadcast hash join optimization when the runtime statistics on one side of a join operator is less than the threshold \`table.optimizer.join.broadcast-threshold\`. The value of this configuration option decides when Flink should perform this optimization. AUTO means Flink will automatically choose the timing for optimization, RUNTIME\_ONLY means broadcast hash join optimization is only performed at runtime, and NONE means the optimization is only carried out at compile time.<br>Possible values:<br>- "auto": Flink will automatically choose the timing for optimization<br>- "runtime\_only": Broadcast hash join optimization is only performed at runtime.<br>- "none": Broadcast hash join optimization is only carried out at compile time. |
| ##### table.optimizer.agg-phase-strategy [Anchor link for: table optimizer agg phase strategy](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-agg-phase-strategy)<br>BatchStreaming | AUTO | Enum | Strategy for aggregate phase. Only AUTO, TWO\_PHASE or ONE\_PHASE can be set.<br>AUTO: No special enforcer for aggregate stage. Whether to choose two stage aggregate or one stage aggregate depends on cost. <br>TWO\_PHASE: Enforce to use two stage aggregate which has localAggregate and globalAggregate. Note that if aggregate call does not support optimize into two phase, we will still use one stage aggregate.<br>ONE\_PHASE: Enforce to use one stage aggregate which only has CompleteGlobalAggregate.<br>Possible values:<br>- "AUTO"<br>- "ONE\_PHASE"<br>- "TWO\_PHASE" |
| ##### table.optimizer.bushy-join-reorder-threshold [Anchor link for: table optimizer bushy join reorder threshold](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-bushy-join-reorder-threshold)<br>BatchStreaming | 12 | Integer | The maximum number of joined nodes allowed in the bushy join reorder algorithm, otherwise the left-deep join reorder algorithm will be used. The search space of bushy join reorder algorithm will increase with the increase of this threshold value, so this threshold is not recommended to be set too large. The default value is 12. |
| ##### table.optimizer.delta-join.strategy [Anchor link for: table optimizer delta join strategy](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-delta-join-strategy)<br>Streaming | AUTO | Enum | Strategy for optimizing the delta-join. Only AUTO, FORCE or NONE can be set. Default it AUTO.<br>Possible values:<br>- "AUTO": Optimizer will try to use delta join first. If it fails, it will fallback to the regular join.<br>- "FORCE": Use the delta join. If it fails, an exception will be thrown.<br>- "NONE": Don't try to use delta join. |
| ##### table.optimizer.distinct-agg.split.bucket-num [Anchor link for: table optimizer distinct agg split bucket num](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-distinct-agg-split-bucket-num)<br>Streaming | 1024 | Integer | Configure the number of buckets when splitting distinct aggregation. The number is used in the first level aggregation to calculate a bucket key 'hash\_code(distinct\_key) % BUCKET\_NUM' which is used as an additional group key after splitting. |
| ##### table.optimizer.distinct-agg.split.enabled [Anchor link for: table optimizer distinct agg split enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-distinct-agg-split-enabled)<br>Streaming | false | Boolean | Tells the optimizer whether to split distinct aggregation (e.g. COUNT(DISTINCT col), SUM(DISTINCT col)) into two level. The first aggregation is shuffled by an additional key which is calculated using the hashcode of distinct\_key and number of buckets. This optimization is very useful when there is data skew in distinct aggregation and gives the ability to scale-up the job. Default is false. |
| ##### table.optimizer.dynamic-filtering.enabled [Anchor link for: table optimizer dynamic filtering enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-dynamic-filtering-enabled)<br>BatchStreaming | true | Boolean | When it is true, the optimizer will try to push dynamic filtering into scan table source, the irrelevant partitions or input data will be filtered to reduce scan I/O in runtime. |
| ##### table.optimizer.incremental-agg-enabled [Anchor link for: table optimizer incremental agg enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-incremental-agg-enabled)<br>Streaming | true | Boolean | When both local aggregation and distinct aggregation splitting are enabled, a distinct aggregation will be optimized into four aggregations, i.e., local-agg1, global-agg1, local-agg2, and global-agg2. We can combine global-agg1 and local-agg2 into a single operator (we call it incremental agg because it receives incremental accumulators and outputs incremental results). In this way, we can reduce some state overhead and resources. Default is enabled. |
| ##### table.optimizer.join-reorder-enabled [Anchor link for: table optimizer join reorder enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-join-reorder-enabled)<br>BatchStreaming | false | Boolean | Enables join reorder in optimizer. Default is disabled. |
| ##### table.optimizer.join.broadcast-threshold [Anchor link for: table optimizer join broadcast threshold](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-join-broadcast-threshold)<br>Batch | 1048576 | Long | Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 to disable broadcasting. |
| ##### table.optimizer.multi-join.enabled [Anchor link for: table optimizer multi join enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-multi-join-enabled)<br>Streaming | false | Boolean | Enables a multi-way join operator for a chain of streaming joins. This operator processes multiple inputs at once, reducing the state size considerably by avoiding intermediate results. It supports regular INNER and LEFT joins.<br>Note: This is an experimental feature and not recommended for production just yet. The operator's internal implementation and state layout is subject to changes due to ongoing relevant optimizations. These might break savepoint compatibility across Flink versions and the goal is to have a stable version in the next release. |
| ##### table.optimizer.multiple-input-enabled [Anchor link for: table optimizer multiple input enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-multiple-input-enabled)<br>Batch | true | Boolean | When it is true, the optimizer will merge the operators with pipelined shuffling into a multiple input operator to reduce shuffling and improve performance. Default value is true. |
| ##### table.optimizer.non-deterministic-update.strategy [Anchor link for: table optimizer non deterministic update strategy](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-non-deterministic-update-strategy)<br>Streaming | IGNORE | Enum | When it is \`TRY\_RESOLVE\`, the optimizer tries to resolve the correctness issue caused by 'Non-Deterministic Updates' (NDU) in a changelog pipeline. Changelog may contain kinds of message types: Insert (I), Delete (D), Update\_Before (UB), Update\_After (UA). There's no NDU problem in an insert only changelog pipeline. For updates, there are three main NDU problems:<br>1\. Non-deterministic functions, include scalar, table, aggregate functions, both builtin and custom ones.<br>2\. LookupJoin on an evolving source<br>3\. Cdc-source carries metadata fields which are system columns, not belongs to the entity data itself.<br>For the first step, the optimizer automatically enables the materialization for No.2(LookupJoin) if needed, and gives the detailed error message for No.1(Non-deterministic functions) and No.3(Cdc-source with metadata) which is relatively easier to solve by changing the SQL.<br>Default value is \`IGNORE\`, the optimizer does no changes.<br>Possible values:<br>- "TRY\_RESOLVE"<br>- "IGNORE" |
| ##### table.optimizer.ptf.max-tables [Anchor link for: table optimizer ptf max tables](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-ptf-max-tables)<br>Streaming | 20 | Integer | The maximum number of table arguments for a Process Table Function (PTF). In theory, a PTF can accept an arbitrary number of input tables. In practice, however, each input requires reserving network buffers, which impacts memory usage. For this reason, the number of input tables is limited to 20. |
| ##### table.optimizer.reuse-optimize-block-with-digest-enabled [Anchor link for: table optimizer reuse optimize block with digest enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-reuse-optimize-block-with-digest-enabled)<br>BatchStreaming | false | Boolean | When true, the optimizer will try to find out duplicated sub-plans by digest to build optimize blocks (a.k.a. common sub-graphs). Each optimize block will be optimized independently. |
| ##### table.optimizer.reuse-sink-enabled [Anchor link for: table optimizer reuse sink enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-reuse-sink-enabled)<br>BatchStreaming | true | Boolean | When it is true, the optimizer will try to find out duplicated table sinks and reuse them. This works only when table.optimizer.reuse-sub-plan-enabled is true. |
| ##### table.optimizer.reuse-source-enabled [Anchor link for: table optimizer reuse source enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-reuse-source-enabled)<br>BatchStreaming | true | Boolean | When it is true, the optimizer will try to find out duplicated table sources and reuse them. This works only when table.optimizer.reuse-sub-plan-enabled is true. |
| ##### table.optimizer.reuse-sub-plan-enabled [Anchor link for: table optimizer reuse sub plan enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-reuse-sub-plan-enabled)<br>BatchStreaming | true | Boolean | When it is true, the optimizer will try to find out duplicated sub-plans and reuse them. |
| ##### table.optimizer.runtime-filter.enabled [Anchor link for: table optimizer runtime filter enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-runtime-filter-enabled)<br>Batch | false | Boolean | A flag to enable or disable the runtime filter. When it is true, the optimizer will try to inject a runtime filter for eligible join. |
| ##### table.optimizer.runtime-filter.max-build-data-size [Anchor link for: table optimizer runtime filter max build data size](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-runtime-filter-max-build-data-size)<br>Batch | 150 mb | MemorySize | Max data volume threshold of the runtime filter build side. Estimated data volume needs to be under this value to try to inject runtime filter. |
| ##### table.optimizer.runtime-filter.min-filter-ratio [Anchor link for: table optimizer runtime filter min filter ratio](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-runtime-filter-min-filter-ratio)<br>Batch | 0.5 | Double | Min filter ratio threshold of the runtime filter. Estimated filter ratio needs to be over this value to try to inject runtime filter. |
| ##### table.optimizer.runtime-filter.min-probe-data-size [Anchor link for: table optimizer runtime filter min probe data size](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-runtime-filter-min-probe-data-size)<br>Batch | 10 gb | MemorySize | Min data volume threshold of the runtime filter probe side. Estimated data volume needs to be over this value to try to inject runtime filter.This value should be larger than `table.optimizer.runtime-filter.max-build-data-size`. |
| ##### table.optimizer.skewed-join-optimization.skewed-factor [Anchor link for: table optimizer skewed join optimization skewed factor](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-skewed-join-optimization-skewed-factor)<br>Batch | 4.0 | Double | When a join operator instance encounters input data that exceeds N times the median size of other concurrent join operator instances, it is considered skewed (where N represents this skewed-factor). In such cases, Flink may automatically split the skewed data into multiple parts to ensure a more balanced data distribution, unless the data volume is below the skewed threshold(defined using table.optimizer.skewed-join-optimization.skewed-threshold). |
| ##### table.optimizer.skewed-join-optimization.skewed-threshold [Anchor link for: table optimizer skewed join optimization skewed threshold](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-skewed-join-optimization-skewed-threshold)<br>Batch | 256 mb | MemorySize | When a join operator instance encounters input data that exceeds N times the median size of other concurrent join operator instances, it is considered skewed (where N represents the table.optimizer.skewed-join-optimization.skewed-factor). In such cases, Flink may automatically split the skewed data into multiple parts to ensure a more balanced data distribution, unless the data volume is below this skewed threshold. |
| ##### table.optimizer.skewed-join-optimization.strategy [Anchor link for: table optimizer skewed join optimization strategy](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-skewed-join-optimization-strategy)<br>Batch | auto | Enum | Flink will handle skew in shuffled joins (sort-merge and hash) at runtime by splitting data according to the skewed join key. The value of this configuration determines how Flink performs this optimization. AUTO means Flink will automatically apply this optimization, FORCED means Flink will enforce this optimization even if it introduces extra hash shuffle, and NONE means this optimization will not be executed.<br>Possible values:<br>- "auto": Flink will automatically perform this optimization.<br>- "forced": Flink will perform this optimization even if it introduces extra hash shuffling.<br>- "none": Skewed join optimization will not be performed. |
| ##### table.optimizer.source.report-statistics-enabled [Anchor link for: table optimizer source report statistics enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-source-report-statistics-enabled)<br>BatchStreaming | true | Boolean | When it is true, the optimizer will collect and use the statistics from source connectors if the source extends from SupportsStatisticReport and the statistics from catalog is UNKNOWN.Default value is true. |
| ##### table.optimizer.sql2rel.project-merge.enabled [Anchor link for: table optimizer sql2rel project merge enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-sql2rel-project-merge-enabled)<br>BatchStreaming | false | Boolean | If set to true, it will merge projects when converting SqlNode to RelNode.<br>Note: it is not recommended to turn on unless you are aware of possible side effects, such as causing the output of certain non-deterministic expressions to not meet expectations(see FLINK-20887). |
| ##### table.optimizer.union-all-as-breakpoint-enabled [Anchor link for: table optimizer union all as breakpoint enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-optimizer-union-all-as-breakpoint-enabled)<br>BatchStreaming | true | Boolean | When true, the optimizer will breakup the graph at union-all node when it's a breakpoint. When false, the optimizer will skip the union-all node even it's a breakpoint, and will try find the breakpoint in its inputs. |

### Table Options  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-options)

The following options can be used to adjust the behavior of the table planner.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### table.builtin-catalog-name [Anchor link for: table builtin catalog name](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-builtin-catalog-name)<br>BatchStreaming | "default\_catalog" | String | The name of the initial catalog to be created when instantiating a TableEnvironment. |
| ##### table.builtin-database-name [Anchor link for: table builtin database name](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-builtin-database-name)<br>BatchStreaming | "default\_database" | String | The name of the default database in the initial catalog to be created when instantiating TableEnvironment. |
| ##### table.catalog-modification.listeners [Anchor link for: table catalog modification listeners](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-catalog-modification-listeners)<br>BatchStreaming | (none) | List<String> | A (semicolon-separated) list of factories that creates listener for catalog modification which will be notified in catalog manager after it performs database and table ddl operations successfully. |
| ##### table.column-expansion-strategy [Anchor link for: table column expansion strategy](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-column-expansion-strategy)<br>BatchStreaming |  | List<Enum> | Configures the default expansion behavior of 'SELECT \*'. By default, all top-level columns of the table's schema are selected and nested fields are retained.<br>Possible values:<br>- "EXCLUDE\_ALIASED\_VIRTUAL\_METADATA\_COLUMNS": Excludes virtual metadata columns that reference a metadata key via an alias. For example, a column declared as 'c METADATA VIRTUAL FROM k' is not selected by default if the strategy is applied.<br>- "EXCLUDE\_DEFAULT\_VIRTUAL\_METADATA\_COLUMNS": Excludes virtual metadata columns that directly reference a metadata key. For example, a column declared as 'k METADATA VIRTUAL' is not selected by default if the strategy is applied. |
| ##### table.display.max-column-width [Anchor link for: table display max column width](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-display-max-column-width)<br>BatchStreaming | 30 | Integer | When printing the query results to the client console, this parameter determines the number of characters shown on screen before truncating. This only applies to columns with variable-length types (e.g. CHAR, VARCHAR, STRING) in the streaming mode. Fixed-length types are printed in the batch mode using a deterministic column width. |
| ##### table.dml-sync [Anchor link for: table dml sync](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-dml-sync)<br>BatchStreaming | false | Boolean | Specifies if the DML job (i.e. the insert operation) is executed asynchronously or synchronously. By default, the execution is async, so you can submit multiple DML jobs at the same time. If set this option to true, the insert operation will wait for the job to finish. |
| ##### table.dynamic-table-options.enabled [Anchor link for: table dynamic table options enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-dynamic-table-options-enabled)<br>BatchStreaming | true | Boolean | Enable or disable the OPTIONS hint used to specify table options dynamically, if disabled, an exception would be thrown if any OPTIONS hint is specified |
| ##### table.generated-code.max-length [Anchor link for: table generated code max length](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-generated-code-max-length)<br>BatchStreaming | 4000 | Integer | Specifies a threshold where generated code will be split into sub-function calls. Java has a maximum method length of 64 KB. This setting allows for finer granularity if necessary. Default value is 4000 instead of 64KB as by default JIT refuses to work on methods with more than 8K byte code. |
| ##### table.legacy-nested-row-nullability [Anchor link for: table legacy nested row nullability](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-legacy-nested-row-nullability)<br>BatchStreaming | false | Boolean | Before Flink 2.2, row types defined in SQL e.g. \`SELECT CAST(f AS ROW<i NOT NULL>)\` did ignore the \`NOT NULL\` constraint. This was more aligned with the SQL standard but caused many type inconsistencies and cryptic error message when working on nested data. For example, it prevented using rows in computed columns or join keys. The new behavior takes the nullability into consideration. |
| ##### table.local-time-zone [Anchor link for: table local time zone](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-local-time-zone)<br>BatchStreaming | "default" | String | The local time zone defines current session time zone id. It is used when converting to/from <code>TIMESTAMP WITH LOCAL TIME ZONE</code>. Internally, timestamps with local time zone are always represented in the UTC time zone. However, when converting to data types that don't include a time zone (e.g. TIMESTAMP, TIME, or simply STRING), the session time zone is used during conversion. The input of option is either a full name such as "America/Los\_Angeles", or a custom timezone id such as "GMT-08:00". |
| ##### table.plan.compile.catalog-objects [Anchor link for: table plan compile catalog objects](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-plan-compile-catalog-objects)<br>BatchStreaming | ALL | Enum | Strategy how to persist catalog objects such as tables, functions, or data types into a plan during compilation.<br>It influences the need for catalog metadata to be present during a restore operation and affects the plan size.<br>This configuration option does not affect anonymous/inline or temporary objects. Anonymous/inline objects will be persisted entirely (including schema and options) if possible or fail the compilation otherwise. Temporary objects will be persisted only by their identifier and the object needs to be present in the session context during a restore.<br>Possible values:<br>- "ALL": All metadata about catalog tables, functions, or data types will be persisted into the plan during compilation. For catalog tables, this includes the table's identifier, schema, and options. For catalog functions, this includes the function's identifier and class. For catalog data types, this includes the identifier and entire type structure. With this strategy, the catalog's metadata doesn't have to be available anymore during a restore operation.<br>- "SCHEMA": In addition to an identifier, schema information about catalog tables, functions, or data types will be persisted into the plan during compilation. A schema allows for detecting incompatible changes in the catalog during a plan restore operation. However, all other metadata will still be retrieved from the catalog.<br>- "IDENTIFIER": Only the identifier of catalog tables, functions, or data types will be persisted into the plan during compilation. All metadata will be retrieved from the catalog during a restore operation. With this strategy, plans become less verbose. |
| ##### table.plan.force-recompile [Anchor link for: table plan force recompile](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-plan-force-recompile)<br>Streaming | false | Boolean | When false COMPILE PLAN statement will fail if the output plan file is already existing, unless the clause IF NOT EXISTS is used. When true COMPILE PLAN will overwrite the existing output plan file. We strongly suggest to enable this flag only for debugging purpose. |
| ##### table.plan.restore.catalog-objects [Anchor link for: table plan restore catalog objects](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-plan-restore-catalog-objects)<br>BatchStreaming | ALL | Enum | Strategy how to restore catalog objects such as tables, functions, or data types using a given plan and performing catalog lookups if necessary. It influences the need for catalog metadata to bepresent and enables partial enrichment of plan information.<br>Possible values:<br>- "ALL": Reads all metadata about catalog tables, functions, or data types that has been persisted in the plan. The strategy performs a catalog lookup by identifier to fill in missing information or enrich mutable options. If the original object is not available in the catalog anymore, pipelines can still be restored if all information necessary is contained in the plan.<br>- "ALL\_ENFORCED": Requires that all metadata about catalog tables, functions, or data types has been persisted in the plan. The strategy will neither perform a catalog lookup by identifier nor enrich mutable options with catalog information. A restore will fail if not all information necessary is contained in the plan.<br>- "IDENTIFIER": Uses only the identifier of catalog tables, functions, or data types and always performs a catalog lookup. A restore will fail if the original object is not available in the catalog anymore. Additional metadata that might be contained in the plan will be ignored. |
| ##### table.resources.download-dir [Anchor link for: table resources download dir](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-resources-download-dir)<br>BatchStreaming | System.getProperty("java.io.tmpdir") | String | Local directory that is used by planner for storing downloaded resources. |
| ##### table.rtas-ctas.atomicity-enabled [Anchor link for: table rtas ctas atomicity enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-rtas-ctas-atomicity-enabled)<br>BatchStreaming | false | Boolean | Specifies if the CREATE TABLE/REPLACE TABLE/CREATE OR REPLACE AS SELECT statement is executed atomically. By default, the statement is non-atomic. The target table is created/replaced on the client side, and it will not be rolled back even though the job fails or is canceled. If set this option to true and the underlying DynamicTableSink implements the SupportsStaging interface, the statement is expected to be executed atomically, the behavior of which depends on the actual DynamicTableSink. |
| ##### table.sql-dialect [Anchor link for: table sql dialect](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#table-sql-dialect)<br>BatchStreaming | "default" | String | The SQL dialect defines how to parse a SQL query. A different SQL dialect may support different SQL grammar. Currently supported dialects are: default and hive |

### Materialized Table Options  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#materialized-table-options)

The following options can be used to adjust the behavior of the materialized table.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### materialized-table.default-freshness.continuous [Anchor link for: materialized table default freshness continuous](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#materialized-table-default-freshness-continuous)<br>BatchStreaming | 3 min | Duration | The default freshness interval for continuous refresh mode when the FRESHNESS clause is omitted in a materialized table definition. |
| ##### materialized-table.default-freshness.full [Anchor link for: materialized table default freshness full](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#materialized-table-default-freshness-full)<br>BatchStreaming | 1 h | Duration | The default freshness interval for full refresh mode when the FRESHNESS clause is omitted in a materialized table definition. |
| ##### materialized-table.refresh-mode.freshness-threshold [Anchor link for: materialized table refresh mode freshness threshold](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#materialized-table-refresh-mode-freshness-threshold)<br>BatchStreaming | 30 min | Duration | Specifies a time threshold for determining the materialized table refresh mode. If the materialized table defined FRESHNESS is below this threshold, it run in continuous mode. Otherwise, it switches to full refresh mode. |
| ##### partition.fields.\#.date-formatter [Anchor link for: partition fields date formatter](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#partition-fields-date-formatter)<br>BatchStreaming | (none) | String | Specifies the time partition formatter for the partitioned materialized table, where '#' denotes a string-based partition field name. This serves as a hint to the framework regarding which partition to refresh in full refresh mode. |

### SQL Client Options  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#sql-client-options)

The following options can be used to adjust the behavior of the sql client.

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### sql-client.display.color-schema [Anchor link for: sql client display color schema](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#sql-client-display-color-schema)<br>BatchStreaming | "DEFAULT" | String | SQL highlight color schema to be used at SQL client. Possible values: 'default', 'dark', 'light', 'chester', 'vs2010', 'solarized', 'obsidian', 'geshi' |
| ##### sql-client.display.print-time-cost [Anchor link for: sql client display print time cost](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#sql-client-display-print-time-cost)<br>Batch | true | Boolean | Determine whether to display the time consumption of the query. By default, no query time cost will be displayed. |
| ##### sql-client.display.show-line-numbers [Anchor link for: sql client display show line numbers](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#sql-client-display-show-line-numbers)<br>BatchStreaming | false | Boolean | Determines whether there should be shown line numbers in multiline SQL or not. |
| ##### sql-client.execution.max-table-result.rows [Anchor link for: sql client execution max table result rows](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#sql-client-execution-max-table-result-rows)<br>BatchStreaming | 1000000 | Integer | The number of rows to cache when in the table mode. If the number of rows exceeds the specified value, it retries the row in the FIFO style. |
| ##### sql-client.execution.result-mode [Anchor link for: sql client execution result mode](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#sql-client-execution-result-mode)<br>BatchStreaming | TABLE | Enum | Determines how the query result should be displayed.<br>Possible values:<br>- "TABLE": Materializes results in memory and visualizes them in a regular, paginated table representation.<br>- "CHANGELOG": Visualizes the result stream that is produced by a continuous query.<br>- "TABLEAU": Display results in the screen directly in a tableau format. |
| ##### sql-client.verbose [Anchor link for: sql client verbose](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/\#sql-client-verbose)<br>BatchStreaming | false | Boolean | Determine whether to output the verbose output to the console. If set the option true, it will print the exception stack. Otherwise, it only output the cause. |