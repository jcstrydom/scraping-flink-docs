[Skip to content](https://github.com/apache/doris#start-of-content)

You signed in with another tab or window. [Reload](https://github.com/apache/doris) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/apache/doris) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/apache/doris) to refresh your session.Dismiss alert

{{ message }}

[apache](https://github.com/apache)/ **[doris](https://github.com/apache/doris)** Public

- [Notifications](https://github.com/login?return_to=%2Fapache%2Fdoris) You must be signed in to change notification settings
- [Fork\\
3.7k](https://github.com/login?return_to=%2Fapache%2Fdoris)
- [Star\\
14.9k](https://github.com/login?return_to=%2Fapache%2Fdoris)


Apache Doris is an easy-to-use, high performance and unified analytics database.


[doris.apache.org](https://doris.apache.org/ "https://doris.apache.org")

### License

[Apache-2.0 license](https://github.com/apache/doris/blob/master/LICENSE.txt)

[14.9k\\
stars](https://github.com/apache/doris/stargazers) [3.7k\\
forks](https://github.com/apache/doris/forks) [Branches](https://github.com/apache/doris/branches) [Tags](https://github.com/apache/doris/tags) [Activity](https://github.com/apache/doris/activity)

[Star](https://github.com/login?return_to=%2Fapache%2Fdoris)

[Notifications](https://github.com/login?return_to=%2Fapache%2Fdoris) You must be signed in to change notification settings

# apache/doris

master

[**638** Branches](https://github.com/apache/doris/branches) [**211** Tags](https://github.com/apache/doris/tags)

[Go to Branches page](https://github.com/apache/doris/branches)[Go to Tags page](https://github.com/apache/doris/tags)

Go to file

Code

Open more actions menu

## Folders and files

| Name | Name | Last commit message | Last commit date |
| --- | --- | --- | --- |
| ## Latest commit<br>[![zzzxl1993](https://avatars.githubusercontent.com/u/33418555?v=4&size=40)](https://github.com/zzzxl1993)[zzzxl1993](https://github.com/apache/doris/commits?author=zzzxl1993)<br>[\[fix\](inverted index) AcceptNullPredicate should include null rows in…](https://github.com/apache/doris/commit/08b68a2d8da11b0eb985fe9f1bdc85c66229adaf)<br>Open commit detailsfailure<br>3 hours agoJan 18, 2026<br>[08b68a2](https://github.com/apache/doris/commit/08b68a2d8da11b0eb985fe9f1bdc85c66229adaf) · 3 hours agoJan 18, 2026<br>## History<br>[29,255 Commits](https://github.com/apache/doris/commits/master/) <br>Open commit details<br>[View commit history for this file.](https://github.com/apache/doris/commits/master/) |
| [.github](https://github.com/apache/doris/tree/master/.github ".github") | [.github](https://github.com/apache/doris/tree/master/.github ".github") | [\[chore\](ci) cancel auto pick to branch-3.1 (](https://github.com/apache/doris/commit/55dc13b10f9333aec3fdfe56867ae0eca146b9f0 "[chore](ci) cancel auto pick to branch-3.1 (#59821)") [#59821](https://github.com/apache/doris/pull/59821) [)](https://github.com/apache/doris/commit/55dc13b10f9333aec3fdfe56867ae0eca146b9f0 "[chore](ci) cancel auto pick to branch-3.1 (#59821)") | 5 days agoJan 13, 2026 |
| [.idea](https://github.com/apache/doris/tree/master/.idea ".idea") | [.idea](https://github.com/apache/doris/tree/master/.idea ".idea") | [\[enhance\](regression) enhance docker network by add network subnet (](https://github.com/apache/doris/commit/1edeacd0a54a86f0272e91aab81fd5f850c664c3 "[enhance](regression) enhance docker network by add network subnet (#26862)") [#…](https://github.com/apache/doris/pull/26862) | 3 years agoNov 14, 2023 |
| [be](https://github.com/apache/doris/tree/master/be "be") | [be](https://github.com/apache/doris/tree/master/be "be") | [\[fix\](inverted index) AcceptNullPredicate should include null rows in…](https://github.com/apache/doris/commit/08b68a2d8da11b0eb985fe9f1bdc85c66229adaf "[fix](inverted index) AcceptNullPredicate should include null rows in result bitmap (#59959)  ### What problem does this PR solve?  Issue Number: close #xxx  Related PR: #xxx  Problem Summary:  ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. -->     - [ ] Regression test     - [x] Unit Test     - [ ] Manual test (add detailed scripts or steps below)     - [ ] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed.         - [ ] Previous test can cover this change.         - [ ] No code files have been changed.         - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed:     - [x] No.     - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation?     - [x] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->   ```") | 3 hours agoJan 18, 2026 |
| [bin](https://github.com/apache/doris/tree/master/bin "bin") | [bin](https://github.com/apache/doris/tree/master/bin "bin") | [\[Fix\](java-opts)Fix Incorrect --add-opens Usage for JNI Compatibility (](https://github.com/apache/doris/commit/dcdf1ab8ab759d379f39fcfd0d549b70ec147e83 "[Fix](java-opts)Fix Incorrect --add-opens Usage for JNI Compatibility (#59278)  ### What problem does this PR solve? Related PR: #58936  When using JNI calls, the --add-opens parameter requires an explicit = to ensure compatibility with modular boundaries and to correctly interpret the relationship between modules and packages. Omitting the = results in a JVM error (Malformed option), preventing the program from running properly.   ``` [INTERNAL_ERROR]failed to init reader, err: [JNI_ERROR]ExceptionInInitializerError: null | CAUSED BY: InaccessibleObjectException: Unable to make field private volatile java.lang.String java.net.URI.string accessible: module java.base does not \"opens java.net\" to unnamed module @5f565482  ```  https://docs.oracle.com/en/java/javase/11/migrate/index.html#GUID-A868D0B9-026F-4D46-B979-901834343F9E  In addition, to avoid conflicts with other dependencies (e.g., paimon-oss), it is necessary to prioritize the loading of paimon-oss by delegating it to the preload mechanism, ensuring consistent and stable module loading order.") […](https://github.com/apache/doris/pull/59278) | last monthDec 24, 2025 |
| [build-support](https://github.com/apache/doris/tree/master/build-support "build-support") | [build-support](https://github.com/apache/doris/tree/master/build-support "build-support") | [\[chore\](format) Remove benchmark dir from code style check dirs (](https://github.com/apache/doris/commit/3ad7d19fff9c7fe68d63152e4f2195cdc665c70d "[chore](format) Remove benchmark dir from code style check dirs (#55457)  ### What problem does this PR solve?  Related PR: https://github.com/apache/doris/pull/52967/files  Problem Summary:  ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. -->     - [ ] Regression test     - [ ] Unit Test     - [ ] Manual test (add detailed scripts or steps below)     - [ ] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed.         - [ ] Previous test can cover this change.         - [ ] No code files have been changed.         - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed:     - [ ] No.     - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation?     - [ ] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->") [#55457](https://github.com/apache/doris/pull/55457) [)](https://github.com/apache/doris/commit/3ad7d19fff9c7fe68d63152e4f2195cdc665c70d "[chore](format) Remove benchmark dir from code style check dirs (#55457)  ### What problem does this PR solve?  Related PR: https://github.com/apache/doris/pull/52967/files  Problem Summary:  ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. -->     - [ ] Regression test     - [ ] Unit Test     - [ ] Manual test (add detailed scripts or steps below)     - [ ] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed.         - [ ] Previous test can cover this change.         - [ ] No code files have been changed.         - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed:     - [ ] No.     - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation?     - [ ] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->") | 5 months agoSep 1, 2025 |
| [cloud](https://github.com/apache/doris/tree/master/cloud "cloud") | [cloud](https://github.com/apache/doris/tree/master/cloud "cloud") | [\[fix\](azure) Handle rate limit exception for azure client (](https://github.com/apache/doris/commit/efdf08ecc2470b2034778966c40c52a6025469be "[fix](azure) Handle rate limit exception for azure client (#59867)  rate limit will throw `std::runtime_error`, but some functions don't use `do_azure_client_call` to handle it.") [#59867](https://github.com/apache/doris/pull/59867) [)](https://github.com/apache/doris/commit/efdf08ecc2470b2034778966c40c52a6025469be "[fix](azure) Handle rate limit exception for azure client (#59867)  rate limit will throw `std::runtime_error`, but some functions don't use `do_azure_client_call` to handle it.") | 2 days agoJan 16, 2026 |
| [common](https://github.com/apache/doris/tree/master/common "common") | [common](https://github.com/apache/doris/tree/master/common "common") | [\[feat\](catalog) Support passing credentials\_provider\_type to BE for S…](https://github.com/apache/doris/commit/86f0b3a089615111707bfe3ed2c0bfcf33ad22c7 "[feat](catalog) Support passing credentials_provider_type to BE for S3 access (#59082)  #58740   This is the backend change for PR #58740, with additional test cases added on top of it.  ---------  Co-authored-by: Calvin Kirs <guoqiang@selectdb.com>") | last monthDec 18, 2025 |
| [conf](https://github.com/apache/doris/tree/master/conf "conf") | [conf](https://github.com/apache/doris/tree/master/conf "conf") | [\[Fix\](java-opts)Fix Incorrect --add-opens Usage for JNI Compatibility (](https://github.com/apache/doris/commit/dcdf1ab8ab759d379f39fcfd0d549b70ec147e83 "[Fix](java-opts)Fix Incorrect --add-opens Usage for JNI Compatibility (#59278)  ### What problem does this PR solve? Related PR: #58936  When using JNI calls, the --add-opens parameter requires an explicit = to ensure compatibility with modular boundaries and to correctly interpret the relationship between modules and packages. Omitting the = results in a JVM error (Malformed option), preventing the program from running properly.   ``` [INTERNAL_ERROR]failed to init reader, err: [JNI_ERROR]ExceptionInInitializerError: null | CAUSED BY: InaccessibleObjectException: Unable to make field private volatile java.lang.String java.net.URI.string accessible: module java.base does not \"opens java.net\" to unnamed module @5f565482  ```  https://docs.oracle.com/en/java/javase/11/migrate/index.html#GUID-A868D0B9-026F-4D46-B979-901834343F9E  In addition, to avoid conflicts with other dependencies (e.g., paimon-oss), it is necessary to prioritize the loading of paimon-oss by delegating it to the preload mechanism, ensuring consistent and stable module loading order.") […](https://github.com/apache/doris/pull/59278) | last monthDec 24, 2025 |
| [contrib](https://github.com/apache/doris/tree/master/contrib "contrib") | [contrib](https://github.com/apache/doris/tree/master/contrib "contrib") | [\[feature\](inverted index) Implement es-like boolean query (](https://github.com/apache/doris/commit/b4e78bafa82605d013793399a2fc398ee4e10c18 "[feature](inverted index) Implement es-like boolean query (#58545)") [#58545](https://github.com/apache/doris/pull/58545) [)](https://github.com/apache/doris/commit/b4e78bafa82605d013793399a2fc398ee4e10c18 "[feature](inverted index) Implement es-like boolean query (#58545)") | last monthDec 19, 2025 |
| [dist](https://github.com/apache/doris/tree/master/dist "dist") | [dist](https://github.com/apache/doris/tree/master/dist "dist") | [\[Chore\](Notice)Update Notice year to 2026 (](https://github.com/apache/doris/commit/742395e2fbbc78e5e88d6e596af589b777c44eab "[Chore](Notice)Update Notice year to 2026 (#59822)  happy new year") [#59822](https://github.com/apache/doris/pull/59822) [)](https://github.com/apache/doris/commit/742395e2fbbc78e5e88d6e596af589b777c44eab "[Chore](Notice)Update Notice year to 2026 (#59822)  happy new year") | 5 days agoJan 13, 2026 |
| [docker](https://github.com/apache/doris/tree/master/docker "docker") | [docker](https://github.com/apache/doris/tree/master/docker "docker") | [\[fix\](parquet) Fix struct column reading error when all queried field…](https://github.com/apache/doris/commit/99b971cd27e9be6698198b9710d96638749c612b "[fix](parquet) Fix struct column reading error when all queried fields are missing after schema evolution (#59586)  ### What problem does this PR solve? - relate pr: #57204  **Problem Summary:** When querying struct fields in Iceberg tables after schema evolution, if all queried struct fields are missing in old Parquet files, the code fails with error: ``` File column name 'removed' not found in struct children ```  **Root Cause:** When all queried struct sub-fields are missing in the old Parquet file (e.g., newly added fields after schema evolution), the code needs to find a reference column from the file schema to get repetition level (RL) and definition level (DL) information. However, if the reference column (e.g., `removed`) was dropped from the table schema, calling `root_node->get_children_node_by_file_column_name()` will fail because the column doesn't exist in `root_node`.  **Scenario:** 1. Create table with struct containing: `removed`, `rename`, `keep`, `drop_and_add` 2. Insert data (creates Parquet file with these fields) 3. Perform schema evolution: DROP `a_struct.removed`, DROP then ADD `a_struct.drop_and_add` (gets new field ID), ADD `a_struct.added` 4. Query `struct_element(a_struct, 'drop_and_add')` or `struct_element(a_struct, 'added')` on the old file 5. The query fails because: - All queried fields (`drop_and_add`, `added`) are missing in the old file - Code tries to use `removed` as reference column (it exists in file but was dropped from table schema) - Accessing `removed` via `root_node` fails because it doesn't exist in table schema  ### Solution: Use `TableSchemaChangeHelper::ConstNode::get_instance()` instead of looking up from `root_node` for the reference column. Since the reference column is only used to get RL/DL information (not for schema mapping), using `ConstNode` is safe and avoids the issue where the reference column doesn't exist in `root_node`.  ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. -->     - [ ] Regression test     - [ ] Unit Test     - [ ] Manual test (add detailed scripts or steps below)     - [ ] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed.         - [ ] Previous test can cover this change.         - [ ] No code files have been changed.         - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed:     - [ ] No.     - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation?     - [ ] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->") | 5 days agoJan 13, 2026 |
| [docs](https://github.com/apache/doris/tree/master/docs "docs") | [docs](https://github.com/apache/doris/tree/master/docs "docs") | [docs:update slack link (](https://github.com/apache/doris/commit/bd6384e95931570d25bfbaadfdbc0d8e5c613bbc "docs:update slack link (#59668)") [#59668](https://github.com/apache/doris/pull/59668) [)](https://github.com/apache/doris/commit/bd6384e95931570d25bfbaadfdbc0d8e5c613bbc "docs:update slack link (#59668)") | last weekJan 8, 2026 |
| [extension](https://github.com/apache/doris/tree/master/extension "extension") | [extension](https://github.com/apache/doris/tree/master/extension "extension") | [\[feature\](dbt) support dbt-core 1.10 (](https://github.com/apache/doris/commit/c45d7f19ce6d815a242fc3580c60479ef677b4e9 "[feature](dbt) support dbt-core 1.10 (#53448)  ### What problem does this PR solve?  Issue Number: close #xxx  Related PR: #xxx  Problem Summary:  ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. -->     - [ ] Regression test     - [ ] Unit Test     - [ ] Manual test (add detailed scripts or steps below)     - [ ] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed.         - [ ] Previous test can cover this change.         - [ ] No code files have been changed.         - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed:     - [ ] No.     - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation?     - [ ] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->") [#53448](https://github.com/apache/doris/pull/53448) [)](https://github.com/apache/doris/commit/c45d7f19ce6d815a242fc3580c60479ef677b4e9 "[feature](dbt) support dbt-core 1.10 (#53448)  ### What problem does this PR solve?  Issue Number: close #xxx  Related PR: #xxx  Problem Summary:  ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. -->     - [ ] Regression test     - [ ] Unit Test     - [ ] Manual test (add detailed scripts or steps below)     - [ ] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed.         - [ ] Previous test can cover this change.         - [ ] No code files have been changed.         - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed:     - [ ] No.     - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation?     - [ ] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->") | 6 months agoJul 20, 2025 |
| [fe](https://github.com/apache/doris/tree/master/fe "fe") | [fe](https://github.com/apache/doris/tree/master/fe "fe") | [\[fix\](cloud) cloud\_unique\_id is requried when load from cluster snaps…](https://github.com/apache/doris/commit/95bafe42b26bf6431a6c6527480a1ea067384f1d "[fix](cloud) cloud_unique_id is requried when load from cluster snapshot (#59820)") | 7 hours agoJan 17, 2026 |
| [fe\_plugins](https://github.com/apache/doris/tree/master/fe_plugins "fe_plugins") | [fe\_plugins](https://github.com/apache/doris/tree/master/fe_plugins "fe_plugins") | [\[Improvement\]\[Audit\] add sql type for query audit (](https://github.com/apache/doris/commit/22dc6c9faf5bcda2fda5dcdeff5c57ca923f45d7 " [Improvement][Audit] add sql type for query audit (#37790)   Co-authored-by: garenshi <garenshi@tencent.com>") [#37790](https://github.com/apache/doris/pull/37790) [)](https://github.com/apache/doris/commit/22dc6c9faf5bcda2fda5dcdeff5c57ca923f45d7 " [Improvement][Audit] add sql type for query audit (#37790)   Co-authored-by: garenshi <garenshi@tencent.com>") | 2 years agoJul 26, 2024 |
| [fs\_brokers](https://github.com/apache/doris/tree/master/fs_brokers "fs_brokers") | [fs\_brokers](https://github.com/apache/doris/tree/master/fs_brokers "fs_brokers") | [\[Fix\](Streamingjob) fix postgres incr consumer too slow (](https://github.com/apache/doris/commit/eb01099b4a4fb48ecb419124aeb095f6583a5006 "[Fix](Streamingjob) fix postgres incr consumer too slow (#59919)  ### What problem does this PR solve?  Issue Number: close #xxx  Related PR:  https://github.com/apache/doris/pull/59798  If you close the reader before committing the LSN, the commit will not be completed.") [#59919](https://github.com/apache/doris/pull/59919) [)](https://github.com/apache/doris/commit/eb01099b4a4fb48ecb419124aeb095f6583a5006 "[Fix](Streamingjob) fix postgres incr consumer too slow (#59919)  ### What problem does this PR solve?  Issue Number: close #xxx  Related PR:  https://github.com/apache/doris/pull/59798  If you close the reader before committing the LSN, the commit will not be completed.") | 2 days agoJan 16, 2026 |
| [gensrc](https://github.com/apache/doris/tree/master/gensrc "gensrc") | [gensrc](https://github.com/apache/doris/tree/master/gensrc "gensrc") | [\[fix\](txn) Committed transaction should not be aborted (](https://github.com/apache/doris/commit/f2cec5fb8f914d70c755aa7a52177aae7f7c31a1 "[fix](txn) Committed transaction should not be aborted (#59850)   A lazy task first marks the transaction status as committed, then asynchronously converts tmp rowsets. Once marked as committed, the transaction should not be aborted.") [#59850](https://github.com/apache/doris/pull/59850) [)](https://github.com/apache/doris/commit/f2cec5fb8f914d70c755aa7a52177aae7f7c31a1 "[fix](txn) Committed transaction should not be aborted (#59850)   A lazy task first marks the transaction status as committed, then asynchronously converts tmp rowsets. Once marked as committed, the transaction should not be aborted.") | 2 days agoJan 16, 2026 |
| [minidump](https://github.com/apache/doris/tree/master/minidump "minidump") | [minidump](https://github.com/apache/doris/tree/master/minidump "minidump") | [\[feature\](Nereids) Add minidump replay and refactor user feature of m…](https://github.com/apache/doris/commit/f84af95ac411bb5c9251db2055300b14d8be297e "[feature](Nereids) Add minidump replay and refactor user feature of minidump (#20716)  ### Two main changes: - 1、add minidump replay - 2、change minidump serialization of statistic messages and some interface between main logic of nereids optimizer and minidump  ### Use of nereids ut: - 1、save minidump files:           Execute command by mysql-client: ``` set enable_nereids_planner=true; set enable_minidump=true; ```         Execute sql in mysql-client - 2、use nereids-ut script to execute directory: ``` cp -r ${DORIS_HOME}/minidump ${DORIS_HOME}/output/fe && cd ${DORIS_HOME}/output/fe ./nereids_ut --d ${directory_of_minidump_files} ```  ### Refactor of minidump - move statistics used serialization to serialization of input and serialize with catalogs - generating minidump file only when enable_minidump flag is set, minidump module interactive with main optimizer only by : serializeInputsToDumpFile(catalog, statistics, query) && serializeOutputsToDumpFile(outputplan).") | 3 years agoJul 25, 2023 |
| [pytest](https://github.com/apache/doris/tree/master/pytest "pytest") | [pytest](https://github.com/apache/doris/tree/master/pytest "pytest") | [\[remove\](sync JOB) Remove the code about SYNC JOB (](https://github.com/apache/doris/commit/81b2d2bde57f136bd410948d54abbdcbea734977 "[remove](sync JOB) Remove the code about SYNC JOB (#58008)  now we do not support the SYNC JOB feature, so remove the code from kernel.") [#58008](https://github.com/apache/doris/pull/58008) [)](https://github.com/apache/doris/commit/81b2d2bde57f136bd410948d54abbdcbea734977 "[remove](sync JOB) Remove the code about SYNC JOB (#58008)  now we do not support the SYNC JOB feature, so remove the code from kernel.") | 2 months agoNov 17, 2025 |
| [regression-test](https://github.com/apache/doris/tree/master/regression-test "regression-test") | [regression-test](https://github.com/apache/doris/tree/master/regression-test "regression-test") | [\[fix\](posexplode) fix return type check failure (](https://github.com/apache/doris/commit/59e1392cfbf2cfd4f811286f723af45a9ebafb72 "[fix](posexplode) fix return type check failure (#59734)") [#59734](https://github.com/apache/doris/pull/59734) [)](https://github.com/apache/doris/commit/59e1392cfbf2cfd4f811286f723af45a9ebafb72 "[fix](posexplode) fix return type check failure (#59734)") | 7 hours agoJan 17, 2026 |
| [samples](https://github.com/apache/doris/tree/master/samples "samples") | [samples](https://github.com/apache/doris/tree/master/samples "samples") | [\[chore\](typo)Fix typos in thrift (](https://github.com/apache/doris/commit/84c1966b6d01f06742f25fad9269cc4fbf726c4a "[chore](typo)Fix typos in thrift (#57267)  In Thrift, fields are distinguished by IDs rather than names, so modifying field names does not break compatibility.") [#57267](https://github.com/apache/doris/pull/57267) [)](https://github.com/apache/doris/commit/84c1966b6d01f06742f25fad9269cc4fbf726c4a "[chore](typo)Fix typos in thrift (#57267)  In Thrift, fields are distinguished by IDs rather than names, so modifying field names does not break compatibility.") | 3 months agoOct 24, 2025 |
| [sdk/go-doris-sdk](https://github.com/apache/doris/tree/master/sdk/go-doris-sdk "This path skips through empty directories") | [sdk/go-doris-sdk](https://github.com/apache/doris/tree/master/sdk/go-doris-sdk "This path skips through empty directories") | [\[fix\](sdk)Fix the log package from being ignored by gitignore (](https://github.com/apache/doris/commit/ff91007778114556f4dc3bb74c3b3d9cc8d95058 "[fix](sdk)Fix the log package from being ignored by gitignore (#58455)  Fix the log package from being ignored by gitignore, the previous pkg version was missing the log package.") [#58455](https://github.com/apache/doris/pull/58455) [)](https://github.com/apache/doris/commit/ff91007778114556f4dc3bb74c3b3d9cc8d95058 "[fix](sdk)Fix the log package from being ignored by gitignore (#58455)  Fix the log package from being ignored by gitignore, the previous pkg version was missing the log package.") | 2 months agoNov 27, 2025 |
| [task\_executor\_simulator/conf](https://github.com/apache/doris/tree/master/task_executor_simulator/conf "This path skips through empty directories") | [task\_executor\_simulator/conf](https://github.com/apache/doris/tree/master/task_executor_simulator/conf "This path skips through empty directories") | [\[feature\](executor) Add the time-sharing executor framework and use i…](https://github.com/apache/doris/commit/b408144eed29a182c2f7d23766b09289757780e0 "[feature](executor) Add the time-sharing executor framework and use it in the scanner section. (#51690)  ### What problem does this PR solve?  Problem Summary:  When concurrent large queries occupy all resources, small queries become hungry and run very slowly.  ### Release note In order to solve the problem that when concurrent large queries occupy all resources, small queries become hungry and run very slowly.  A dynamic multi-level priority scheduling is introduced. 1. The time-sharing executor framework is implemented. Source code in `be/src/vec/exec/executor` and `be/src/vec/exec/executor/time_sharing` 2. Currently, this mechanism is used in the scanner, and will be used in the computing layer in the future. Time-sharing executor framework framework currently have options for automatic scheduling and concurrency control. Since the current implementation of the scanning part relies on manual scheduling and already has the ability of concurrency control, the automatic scheduling and concurrency control of the framework should be turned off when integrating the scanning part. Also keep the original implementation of scanner scheduler. Controlled by `enable_task_executor_in_internal_table ` and `enable_task_executor_in_internal_table `.  4. A task executor simulator is introduced。By controlling the total number of tasks, the number of concurrency, the number of splits of each task, and the splits generated by different generators of splits (Leaf and Intermediate) (for example, different rates, different waiting times, etc.), we can simulate scenarios and observe relevant performance indicators. In order to compare with the scenario before the code was modified, a test fifo queue was introduced to compare with the current multi-level priority queue.     - Source code in `be/src/vec/exec/executor/simulator`.   - Command: run-task-executor-simulator.sh `task_executor_simulator/conf/experiment_starve_slow_splits.json`  ### Task Executor Simulator Performance Results A system is constructed that first generates many slow splits to simulate the situation of concurrent large queries, and then executes fast splits to simulate the situation of small queries. `task_executor_simulator/conf/experiment_starve_slow_splits.json`  From the results, the multi-level priority queue is the first-in-first-out queue in terms of the task completion rate of fast leaf, while the task completion rates of the middle and slow leaf are not as good as the first-in-first-out queue.  #### Multi-level Priority Queue  ##### Fast Leaf ``` Completed tasks           : 4338 In-progress tasks         : 40 ``` ##### Intermediate ``` Completed tasks           : 220 In-progress tasks         : 40 ``` ##### Slow Leaf ``` Completed tasks           : 81 In-progress tasks         : 40 ```  #### FIFO Queue  ##### Fast Leaf ``` Completed tasks           : 1644 In-progress tasks         : 40 ``` ##### Intermediate ``` Completed tasks           : 316 In-progress tasks         : 40 ``` ##### Slow Leaf ``` Completed tasks           : 285 In-progress tasks         : 40 ```") | 6 months agoJul 20, 2025 |
| [thirdparty](https://github.com/apache/doris/tree/master/thirdparty "thirdparty") | [thirdparty](https://github.com/apache/doris/tree/master/thirdparty "thirdparty") | [\[fix\](mac) download one package on Mac failed (](https://github.com/apache/doris/commit/fb626ba7cbfb0f0932a9c91038fc1b6e9cf34353 "[fix](mac) download one package on Mac failed (#59909)  ### What problem does this PR solve?  zsh do not support it: ```  % echo \"${GIVEN_LIB,,}\" zsh: bad substitution  ```  releated pr: https://github.com/apache/doris/pull/34385") [#59909](https://github.com/apache/doris/pull/59909) [)](https://github.com/apache/doris/commit/fb626ba7cbfb0f0932a9c91038fc1b6e9cf34353 "[fix](mac) download one package on Mac failed (#59909)  ### What problem does this PR solve?  zsh do not support it: ```  % echo \"${GIVEN_LIB,,}\" zsh: bad substitution  ```  releated pr: https://github.com/apache/doris/pull/34385") | 3 days agoJan 15, 2026 |
| [tools](https://github.com/apache/doris/tree/master/tools "tools") | [tools](https://github.com/apache/doris/tree/master/tools "tools") | [\[chore\](tools) Update TPC-DS DDLs (](https://github.com/apache/doris/commit/a143c34dc551292fc7f587d3536ac3776445ef8e "[chore](tools) Update TPC-DS DDLs (#59756)  ### What problem does this PR solve?  Issue Number: close #xxx  Related PR: #xxx  Problem Summary:  After test on 4*16C32G, we got the best 100G, 1T schemas. and use 100G for 1G, 1T for 10T now.  ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. -->     - [ ] Regression test     - [ ] Unit Test     - [ ] Manual test (add detailed scripts or steps below)     - [ ] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed.         - [ ] Previous test can cover this change.         - [ ] No code files have been changed.         - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed:     - [ ] No.     - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation?     - [ ] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->") [#59756](https://github.com/apache/doris/pull/59756) [)](https://github.com/apache/doris/commit/a143c34dc551292fc7f587d3536ac3776445ef8e "[chore](tools) Update TPC-DS DDLs (#59756)  ### What problem does this PR solve?  Issue Number: close #xxx  Related PR: #xxx  Problem Summary:  After test on 4*16C32G, we got the best 100G, 1T schemas. and use 100G for 1G, 1T for 10T now.  ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. -->     - [ ] Regression test     - [ ] Unit Test     - [ ] Manual test (add detailed scripts or steps below)     - [ ] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed.         - [ ] Previous test can cover this change.         - [ ] No code files have been changed.         - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed:     - [ ] No.     - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation?     - [ ] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->") | 5 days agoJan 12, 2026 |
| [ui](https://github.com/apache/doris/tree/master/ui "ui") | [ui](https://github.com/apache/doris/tree/master/ui "ui") | [\[fix\](ui) fix ui show incomplete profile (](https://github.com/apache/doris/commit/3872cc02641bd91771e56818458e207a78a3166b "[fix](ui) fix ui show incomplete profile (#58613)  fix ui show incomplete profile  before fix: ``` - nested columns:                  -   user_profile:                  -     origin type: struct>                  -     pruned type: struct ```  after fix: ``` - nested columns:                  -   user_profile:                  -     origin type: struct<name:varchar(100),age:int,address:varchar(200),properties:map<varchar(65533),varchar(65533)>>                  -     pruned type: struct<age:int> ```") [#58613](https://github.com/apache/doris/pull/58613) [)](https://github.com/apache/doris/commit/3872cc02641bd91771e56818458e207a78a3166b "[fix](ui) fix ui show incomplete profile (#58613)  fix ui show incomplete profile  before fix: ``` - nested columns:                  -   user_profile:                  -     origin type: struct>                  -     pruned type: struct ```  after fix: ``` - nested columns:                  -   user_profile:                  -     origin type: struct<name:varchar(100),age:int,address:varchar(200),properties:map<varchar(65533),varchar(65533)>>                  -     pruned type: struct<age:int> ```") | last monthDec 2, 2025 |
| [webroot](https://github.com/apache/doris/tree/master/webroot "webroot") | [webroot](https://github.com/apache/doris/tree/master/webroot "webroot") | [\[Bug\] Show create table null pointer of storage policy and error httt…](https://github.com/apache/doris/commit/aa1bcdbc185aad81fdb31cc64fa035d69c19a89d "[Bug] Show create table null pointer of storage policy and error htttp path of tablet info (#10950)  Co-authored-by: lihaopeng <lihaopeng@baidu.com>") | 4 years agoJul 22, 2022 |
| [.asf.yaml](https://github.com/apache/doris/blob/master/.asf.yaml ".asf.yaml") | [.asf.yaml](https://github.com/apache/doris/blob/master/.asf.yaml ".asf.yaml") | [\[chore\](ci) migrate nonConcurrent pipeline (](https://github.com/apache/doris/commit/a0deaa902ded0f51dc5125d31055ad38189edece "[chore](ci) migrate nonConcurrent pipeline (#59700)") [#59700](https://github.com/apache/doris/pull/59700) [)](https://github.com/apache/doris/commit/a0deaa902ded0f51dc5125d31055ad38189edece "[chore](ci) migrate nonConcurrent pipeline (#59700)") | last weekJan 8, 2026 |
| [.clang-format](https://github.com/apache/doris/blob/master/.clang-format ".clang-format") | [.clang-format](https://github.com/apache/doris/blob/master/.clang-format ".clang-format") | [\[refactor\]\[style\] Use clang-format to sort includes (](https://github.com/apache/doris/commit/718a51a3883345548f325d7f4eb3856e682c4e8d "[refactor][style] Use clang-format to sort includes (#9483)") [#9483](https://github.com/apache/doris/pull/9483) [)](https://github.com/apache/doris/commit/718a51a3883345548f325d7f4eb3856e682c4e8d "[refactor][style] Use clang-format to sort includes (#9483)") | 4 years agoMay 10, 2022 |
| [.clang-format-ignore](https://github.com/apache/doris/blob/master/.clang-format-ignore ".clang-format-ignore") | [.clang-format-ignore](https://github.com/apache/doris/blob/master/.clang-format-ignore ".clang-format-ignore") | [\[chore\](cloud) Add a defer utility](https://github.com/apache/doris/commit/a5fdfb9bb90c3b4cbe34ef6e72fd2bb98a064fb7 "[chore](cloud) Add a defer utility `DORIS_CLOUD_DEFER` (#52041)")`DORIS_CLOUD_DEFER` [(](https://github.com/apache/doris/commit/a5fdfb9bb90c3b4cbe34ef6e72fd2bb98a064fb7 "[chore](cloud) Add a defer utility `DORIS_CLOUD_DEFER` (#52041)") [#52041](https://github.com/apache/doris/pull/52041) [)](https://github.com/apache/doris/commit/a5fdfb9bb90c3b4cbe34ef6e72fd2bb98a064fb7 "[chore](cloud) Add a defer utility `DORIS_CLOUD_DEFER` (#52041)") | 7 months agoJun 23, 2025 |
| [.clang-tidy](https://github.com/apache/doris/blob/master/.clang-tidy ".clang-tidy") | [.clang-tidy](https://github.com/apache/doris/blob/master/.clang-tidy ".clang-tidy") | [\[chore\](BE) remove some useless function and clang-tidy check (](https://github.com/apache/doris/commit/a02e20a6ea8ad853d65fe9653f8650c2897fd1c9 "[chore](BE) remove some useless function and clang-tidy check (#54159)") [#54159](https://github.com/apache/doris/pull/54159) [)](https://github.com/apache/doris/commit/a02e20a6ea8ad853d65fe9653f8650c2897fd1c9 "[chore](BE) remove some useless function and clang-tidy check (#54159)") | 6 months agoAug 1, 2025 |
| [.clangd](https://github.com/apache/doris/blob/master/.clangd ".clangd") | [.clangd](https://github.com/apache/doris/blob/master/.clangd ".clangd") | [\[Chore\](build) enable -Wpedantic and update lowest gcc version to 11.1 (](https://github.com/apache/doris/commit/5e4bb98900628c716ce7420dbebd569f86cdcd4a "[Chore](build) enable -Wpedantic and update lowest gcc version to 11.1 (#16290)  enable -Wpedantic and update lowest gcc version to 11.1") | 3 years agoFeb 2, 2023 |
| [.dockerignore](https://github.com/apache/doris/blob/master/.dockerignore ".dockerignore") | [.dockerignore](https://github.com/apache/doris/blob/master/.dockerignore ".dockerignore") | [\[fix\](docker) Add docker ignore files and change docker default sourc… (](https://github.com/apache/doris/commit/6a62835f7e9689745e1a93a8907ccb40f7a44038 "[fix](docker) Add docker ignore files and change docker default sourc… (#25696)") | 3 years agoOct 26, 2023 |
| [.editorconfig](https://github.com/apache/doris/blob/master/.editorconfig ".editorconfig") | [.editorconfig](https://github.com/apache/doris/blob/master/.editorconfig ".editorconfig") | [\[enhancement\](thirdparty) Support building thirdparty on macOS (](https://github.com/apache/doris/commit/e3c19ded443e25daf8d888c3e338787ec05e1476 "[enhancement](thirdparty) Support building thirdparty on macOS (#10677)") [#10677](https://github.com/apache/doris/pull/10677) [)](https://github.com/apache/doris/commit/e3c19ded443e25daf8d888c3e338787ec05e1476 "[enhancement](thirdparty) Support building thirdparty on macOS (#10677)") | 4 years agoJul 17, 2022 |
| [.gitattributes](https://github.com/apache/doris/blob/master/.gitattributes ".gitattributes") | [.gitattributes](https://github.com/apache/doris/blob/master/.gitattributes ".gitattributes") | [\[fix\](github) force use text diff for .out files (](https://github.com/apache/doris/commit/ebd27d8e883c28d49d35dfd4a7e46d432c221bc9 " [fix](github) force use text diff for .out files (#56330)  when executing `git diff`, treat .out file as text file") [#56330](https://github.com/apache/doris/pull/56330) [)](https://github.com/apache/doris/commit/ebd27d8e883c28d49d35dfd4a7e46d432c221bc9 " [fix](github) force use text diff for .out files (#56330)  when executing `git diff`, treat .out file as text file") | 4 months agoSep 22, 2025 |
| [.gitignore](https://github.com/apache/doris/blob/master/.gitignore ".gitignore") | [.gitignore](https://github.com/apache/doris/blob/master/.gitignore ".gitignore") | [\[regression\](hudi) Impl new Hudi Docker environment (](https://github.com/apache/doris/commit/4a0e4afe83392d621507f9f65d5054f8712d9c53 "[regression](hudi) Impl new Hudi Docker environment (#59401)  ### What problem does this PR solve?  # Hudi Docker Environment  This directory contains the Docker Compose configuration for setting up a Hudi test environment with Spark, Hive Metastore, MinIO (S3-compatible storage), and PostgreSQL.  ## Components  - **Spark**: Apache Spark 3.5.7 for processing Hudi tables - **Hive Metastore**: Starburst Hive Metastore for table metadata management - **PostgreSQL**: Database backend for Hive Metastore - **MinIO**: S3-compatible object storage for Hudi data files  ## Important Configuration Parameters  ### Container UID - **Parameter**: `CONTAINER_UID` in `custom_settings.env` - **Default**: `doris--` - **Note**: Must be set to a unique value to avoid conflicts with other Docker environments - **Example**: `CONTAINER_UID=\"doris--bender--\"`  ### Port Configuration (`hudi.env.tpl`) - `HIVE_METASTORE_PORT`: Port for Hive Metastore Thrift service (default: 19083) - `MINIO_API_PORT`: MinIO S3 API port (default: 19100) - `MINIO_CONSOLE_PORT`: MinIO web console port (default: 19101) - `SPARK_UI_PORT`: Spark web UI port (default: 18080)  ### MinIO Credentials (`hudi.env.tpl`) - `MINIO_ROOT_USER`: MinIO access key (default: `minio`) - `MINIO_ROOT_PASSWORD`: MinIO secret key (default: `minio123`) - `HUDI_BUCKET`: S3 bucket name for Hudi data (default: `datalake`)  ### Version Compatibility ⚠️ **Important**: Hadoop versions must match Spark's built-in Hadoop version - **Spark Version**: 3.5.7 (uses Hadoop 3.3.4) - default build for Hudi 1.0.2 - **Hadoop AWS Version**: 3.3.4 (matching Spark's Hadoop) - **Hudi Bundle Version**: 1.0.2 Spark 3.5 bundle (default build, matches Spark 3.5.7, matches Doris's Hudi version to avoid versionCode compatibility issues) - **AWS SDK v1 Version**: 1.12.262 (required for Hadoop 3.3.4 S3A support, 1.12.x series) - **PostgreSQL JDBC Version**: 42.7.1 (compatible with Hive Metastore) - **Hudi 1.0.x Compatibility**: Supports Spark 3.5.x (default), 3.4.x, and 3.3.x  ### JAR Dependencies (`hudi.env.tpl`) All JAR file versions and URLs are configurable: - `HUDI_BUNDLE_VERSION` / `HUDI_BUNDLE_URL`: Hudi Spark bundle - `HADOOP_AWS_VERSION` / `HADOOP_AWS_URL`: Hadoop S3A filesystem support - `AWS_SDK_BUNDLE_VERSION` / `AWS_SDK_BUNDLE_URL`: AWS Java SDK Bundle v1 (required for Hadoop 3.3.4 S3A support, 1.12.x series)  **Note**: `hadoop-common` is already included in Spark's built-in Hadoop distribution, so it's not configured here. - `POSTGRESQL_JDBC_VERSION` / `POSTGRESQL_JDBC_URL`: PostgreSQL JDBC driver  ## Starting the Environment  ```bash # Start Hudi environment ./docker/thirdparties/run-thirdparties-docker.sh -c hudi  # Stop Hudi environment ./docker/thirdparties/run-thirdparties-docker.sh -c hudi --stop ```  ## Adding Data  ⚠️ **Important**: To ensure data consistency after Docker restarts, **only use SQL scripts** to add data. Data added through `spark-sql` interactive shell is temporary and will not persist after container restart.  ### Using SQL Scripts  Add new SQL files in `scripts/create_preinstalled_scripts/hudi/` directory: - Files are executed in alphabetical order (e.g., `01_config_and_database.sql`, `02_create_user_activity_log_tables.sql`, etc.) - Use descriptive names with numeric prefixes to control execution order - Use environment variable substitution: `${HIVE_METASTORE_URIS}` and `${HUDI_BUCKET}` - **Data created through SQL scripts will persist after Docker restart**  Example: Create `08_create_custom_table.sql`: ```sql USE regression_hudi;  CREATE TABLE IF NOT EXISTS my_hudi_table (   id BIGINT,   name STRING,   created_at TIMESTAMP ) USING hudi TBLPROPERTIES (   type = 'cow',   primaryKey = 'id',   preCombineField = 'created_at',   hoodie.datasource.hive_sync.enable = 'true',   hoodie.datasource.hive_sync.metastore.uris = '${HIVE_METASTORE_URIS}',   hoodie.datasource.hive_sync.mode = 'hms' ) LOCATION 's3a://${HUDI_BUCKET}/warehouse/regression_hudi/my_hudi_table';  INSERT INTO my_hudi_table VALUES   (1, 'Alice', TIMESTAMP '2024-01-01 10:00:00'),   (2, 'Bob', TIMESTAMP '2024-01-02 11:00:00'); ```  After adding SQL files, restart the container to execute them: ```bash docker restart doris--hudi-spark ```  ## Creating Hudi Catalog in Doris  After starting the Hudi Docker environment, you can create a Hudi catalog in Doris to access Hudi tables:  ```sql -- Create Hudi catalog CREATE CATALOG IF NOT EXISTS hudi_catalog PROPERTIES (     'type' = 'hms',     'hive.metastore.uris' = 'thrift://<externalEnvIp>:19083',     's3.endpoint' = 'http://<externalEnvIp>:19100',     's3.access_key' = 'minio',     's3.secret_key' = 'minio123',     's3.region' = 'us-east-1',     'use_path_style' = 'true' );  -- Switch to Hudi catalog SWITCH hudi_catalog;  -- Use database USE regression_hudi;  -- Show tables SHOW TABLES;  -- Query Hudi table SELECT * FROM user_activity_log_cow_partition LIMIT 10; ```  **Configuration Parameters:** - `hive.metastore.uris`: Hive Metastore Thrift service address (default port: 19083) - `s3.endpoint`: MinIO S3 API endpoint (default port: 19100) - `s3.access_key`: MinIO access key (default: `minio`) - `s3.secret_key`: MinIO secret key (default: `minio123`) - `s3.region`: S3 region (default: `us-east-1`) - `use_path_style`: Use path-style access for MinIO (required: `true`)  Replace `<externalEnvIp>` with your actual external environment IP address (e.g., `127.0.0.1` for localhost).  ## Debugging with Spark SQL  ⚠️ **Note**: The methods below are for debugging purposes only. Data created through `spark-sql` interactive shell will **not persist** after Docker restart. To add persistent data, use SQL scripts as described in the \"Adding Data\" section.  ### 1. Connect to Spark Container  ```bash docker exec -it doris--hudi-spark bash ```  ### 2. Start Spark SQL Interactive Shell  ```bash /opt/spark/bin/spark-sql \   --master local[*] \   --name hudi-debug \   --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \   --conf spark.sql.catalogImplementation=hive \   --conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension \   --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog \   --conf spark.sql.warehouse.dir=s3a://datalake/warehouse ```  ### 3. Common Debugging Commands  ```sql -- Show databases SHOW DATABASES;  -- Use database USE regression_hudi;  -- Show tables SHOW TABLES;  -- Describe table structure DESCRIBE EXTENDED user_activity_log_cow_partition;  -- Query data SELECT * FROM user_activity_log_cow_partition LIMIT 10;  -- Check Hudi table properties SHOW TBLPROPERTIES user_activity_log_cow_partition;  -- View Spark configuration SET -v;  -- Check Hudi-specific configurations SET hoodie.datasource.write.hive_style_partitioning; ```  ### 4. View Spark Web UI  Access Spark Web UI at: `http://localhost:18080` (or configured `SPARK_UI_PORT`)  ### 5. Check Container Logs  ```bash # View Spark container logs docker logs doris--hudi-spark --tail 100 -f  # View Hive Metastore logs docker logs doris--hudi-metastore --tail 100 -f  # View MinIO logs docker logs doris--hudi-minio --tail 100 -f ```  ### 6. Verify S3 Data  ```bash # Access MinIO console # URL: http://localhost:19101 (or configured MINIO_CONSOLE_PORT) # Username: minio (or MINIO_ROOT_USER) # Password: minio123 (or MINIO_ROOT_PASSWORD)  # Or use MinIO client docker exec -it doris--hudi-minio-mc mc ls myminio/datalake/warehouse/regression_hudi/ ```  ## Troubleshooting  ### Container Exits Immediately - Check logs: `docker logs doris--hudi-spark` - Verify SUCCESS file exists: `docker exec doris--hudi-spark test -f /opt/hudi-scripts/SUCCESS` - Ensure Hive Metastore is running: `docker ps | grep metastore`  ### ClassNotFoundException Errors - Verify JAR files are downloaded: `docker exec doris--hudi-spark ls -lh /opt/hudi-cache/` - Check JAR versions match Spark's Hadoop version (3.3.4) - Review `hudi.env.tpl` for correct version numbers  ### S3A Connection Issues - Verify MinIO is running: `docker ps | grep minio` - Check MinIO credentials in `hudi.env.tpl` - Test S3 connection: `docker exec doris--hudi-minio-mc mc ls myminio/`  ### Hive Metastore Connection Issues - Check Metastore is ready: `docker logs doris--hudi-metastore | grep \"Metastore is ready\"` - Verify PostgreSQL is running: `docker ps | grep metastore-db` - Test connection: `docker exec doris--hudi-metastore-db pg_isready -U hive`  ## File Structure  ``` hudi/ ├── hudi.yaml.tpl          # Docker Compose template ├── hudi.env.tpl           # Environment variables template ├── scripts/ │   ├── init.sh            # Initialization script │   ├── create_preinstalled_scripts/ │   │   └── hudi/          # SQL scripts (01_config_and_database.sql, 02_create_user_activity_log_tables.sql, ...) │   └── SUCCESS            # Initialization marker (generated) └── cache/                 # Downloaded JAR files (generated) ```  ## Notes  - All generated files (`.yaml`, `.env`, `cache/`, `SUCCESS`) are ignored by Git - SQL scripts support environment variable substitution using `${VARIABLE_NAME}` syntax - Hadoop version compatibility is critical - must match Spark's built-in version - Container keeps running after initialization for healthcheck and debugging  ## Notes  - All generated files (`.yaml`, `.env`, `cache/`, `SUCCESS`) are ignored by Git - SQL scripts support environment variable substitution using `${VARIABLE_NAME}` syntax - Hadoop version compatibility is critical - must match Spark's built-in version - Container keeps running after initialization for healthcheck and debugging") [#59401](https://github.com/apache/doris/pull/59401) [)](https://github.com/apache/doris/commit/4a0e4afe83392d621507f9f65d5054f8712d9c53 "[regression](hudi) Impl new Hudi Docker environment (#59401)  ### What problem does this PR solve?  # Hudi Docker Environment  This directory contains the Docker Compose configuration for setting up a Hudi test environment with Spark, Hive Metastore, MinIO (S3-compatible storage), and PostgreSQL.  ## Components  - **Spark**: Apache Spark 3.5.7 for processing Hudi tables - **Hive Metastore**: Starburst Hive Metastore for table metadata management - **PostgreSQL**: Database backend for Hive Metastore - **MinIO**: S3-compatible object storage for Hudi data files  ## Important Configuration Parameters  ### Container UID - **Parameter**: `CONTAINER_UID` in `custom_settings.env` - **Default**: `doris--` - **Note**: Must be set to a unique value to avoid conflicts with other Docker environments - **Example**: `CONTAINER_UID=\"doris--bender--\"`  ### Port Configuration (`hudi.env.tpl`) - `HIVE_METASTORE_PORT`: Port for Hive Metastore Thrift service (default: 19083) - `MINIO_API_PORT`: MinIO S3 API port (default: 19100) - `MINIO_CONSOLE_PORT`: MinIO web console port (default: 19101) - `SPARK_UI_PORT`: Spark web UI port (default: 18080)  ### MinIO Credentials (`hudi.env.tpl`) - `MINIO_ROOT_USER`: MinIO access key (default: `minio`) - `MINIO_ROOT_PASSWORD`: MinIO secret key (default: `minio123`) - `HUDI_BUCKET`: S3 bucket name for Hudi data (default: `datalake`)  ### Version Compatibility ⚠️ **Important**: Hadoop versions must match Spark's built-in Hadoop version - **Spark Version**: 3.5.7 (uses Hadoop 3.3.4) - default build for Hudi 1.0.2 - **Hadoop AWS Version**: 3.3.4 (matching Spark's Hadoop) - **Hudi Bundle Version**: 1.0.2 Spark 3.5 bundle (default build, matches Spark 3.5.7, matches Doris's Hudi version to avoid versionCode compatibility issues) - **AWS SDK v1 Version**: 1.12.262 (required for Hadoop 3.3.4 S3A support, 1.12.x series) - **PostgreSQL JDBC Version**: 42.7.1 (compatible with Hive Metastore) - **Hudi 1.0.x Compatibility**: Supports Spark 3.5.x (default), 3.4.x, and 3.3.x  ### JAR Dependencies (`hudi.env.tpl`) All JAR file versions and URLs are configurable: - `HUDI_BUNDLE_VERSION` / `HUDI_BUNDLE_URL`: Hudi Spark bundle - `HADOOP_AWS_VERSION` / `HADOOP_AWS_URL`: Hadoop S3A filesystem support - `AWS_SDK_BUNDLE_VERSION` / `AWS_SDK_BUNDLE_URL`: AWS Java SDK Bundle v1 (required for Hadoop 3.3.4 S3A support, 1.12.x series)  **Note**: `hadoop-common` is already included in Spark's built-in Hadoop distribution, so it's not configured here. - `POSTGRESQL_JDBC_VERSION` / `POSTGRESQL_JDBC_URL`: PostgreSQL JDBC driver  ## Starting the Environment  ```bash # Start Hudi environment ./docker/thirdparties/run-thirdparties-docker.sh -c hudi  # Stop Hudi environment ./docker/thirdparties/run-thirdparties-docker.sh -c hudi --stop ```  ## Adding Data  ⚠️ **Important**: To ensure data consistency after Docker restarts, **only use SQL scripts** to add data. Data added through `spark-sql` interactive shell is temporary and will not persist after container restart.  ### Using SQL Scripts  Add new SQL files in `scripts/create_preinstalled_scripts/hudi/` directory: - Files are executed in alphabetical order (e.g., `01_config_and_database.sql`, `02_create_user_activity_log_tables.sql`, etc.) - Use descriptive names with numeric prefixes to control execution order - Use environment variable substitution: `${HIVE_METASTORE_URIS}` and `${HUDI_BUCKET}` - **Data created through SQL scripts will persist after Docker restart**  Example: Create `08_create_custom_table.sql`: ```sql USE regression_hudi;  CREATE TABLE IF NOT EXISTS my_hudi_table (   id BIGINT,   name STRING,   created_at TIMESTAMP ) USING hudi TBLPROPERTIES (   type = 'cow',   primaryKey = 'id',   preCombineField = 'created_at',   hoodie.datasource.hive_sync.enable = 'true',   hoodie.datasource.hive_sync.metastore.uris = '${HIVE_METASTORE_URIS}',   hoodie.datasource.hive_sync.mode = 'hms' ) LOCATION 's3a://${HUDI_BUCKET}/warehouse/regression_hudi/my_hudi_table';  INSERT INTO my_hudi_table VALUES   (1, 'Alice', TIMESTAMP '2024-01-01 10:00:00'),   (2, 'Bob', TIMESTAMP '2024-01-02 11:00:00'); ```  After adding SQL files, restart the container to execute them: ```bash docker restart doris--hudi-spark ```  ## Creating Hudi Catalog in Doris  After starting the Hudi Docker environment, you can create a Hudi catalog in Doris to access Hudi tables:  ```sql -- Create Hudi catalog CREATE CATALOG IF NOT EXISTS hudi_catalog PROPERTIES (     'type' = 'hms',     'hive.metastore.uris' = 'thrift://<externalEnvIp>:19083',     's3.endpoint' = 'http://<externalEnvIp>:19100',     's3.access_key' = 'minio',     's3.secret_key' = 'minio123',     's3.region' = 'us-east-1',     'use_path_style' = 'true' );  -- Switch to Hudi catalog SWITCH hudi_catalog;  -- Use database USE regression_hudi;  -- Show tables SHOW TABLES;  -- Query Hudi table SELECT * FROM user_activity_log_cow_partition LIMIT 10; ```  **Configuration Parameters:** - `hive.metastore.uris`: Hive Metastore Thrift service address (default port: 19083) - `s3.endpoint`: MinIO S3 API endpoint (default port: 19100) - `s3.access_key`: MinIO access key (default: `minio`) - `s3.secret_key`: MinIO secret key (default: `minio123`) - `s3.region`: S3 region (default: `us-east-1`) - `use_path_style`: Use path-style access for MinIO (required: `true`)  Replace `<externalEnvIp>` with your actual external environment IP address (e.g., `127.0.0.1` for localhost).  ## Debugging with Spark SQL  ⚠️ **Note**: The methods below are for debugging purposes only. Data created through `spark-sql` interactive shell will **not persist** after Docker restart. To add persistent data, use SQL scripts as described in the \"Adding Data\" section.  ### 1. Connect to Spark Container  ```bash docker exec -it doris--hudi-spark bash ```  ### 2. Start Spark SQL Interactive Shell  ```bash /opt/spark/bin/spark-sql \   --master local[*] \   --name hudi-debug \   --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \   --conf spark.sql.catalogImplementation=hive \   --conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension \   --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog \   --conf spark.sql.warehouse.dir=s3a://datalake/warehouse ```  ### 3. Common Debugging Commands  ```sql -- Show databases SHOW DATABASES;  -- Use database USE regression_hudi;  -- Show tables SHOW TABLES;  -- Describe table structure DESCRIBE EXTENDED user_activity_log_cow_partition;  -- Query data SELECT * FROM user_activity_log_cow_partition LIMIT 10;  -- Check Hudi table properties SHOW TBLPROPERTIES user_activity_log_cow_partition;  -- View Spark configuration SET -v;  -- Check Hudi-specific configurations SET hoodie.datasource.write.hive_style_partitioning; ```  ### 4. View Spark Web UI  Access Spark Web UI at: `http://localhost:18080` (or configured `SPARK_UI_PORT`)  ### 5. Check Container Logs  ```bash # View Spark container logs docker logs doris--hudi-spark --tail 100 -f  # View Hive Metastore logs docker logs doris--hudi-metastore --tail 100 -f  # View MinIO logs docker logs doris--hudi-minio --tail 100 -f ```  ### 6. Verify S3 Data  ```bash # Access MinIO console # URL: http://localhost:19101 (or configured MINIO_CONSOLE_PORT) # Username: minio (or MINIO_ROOT_USER) # Password: minio123 (or MINIO_ROOT_PASSWORD)  # Or use MinIO client docker exec -it doris--hudi-minio-mc mc ls myminio/datalake/warehouse/regression_hudi/ ```  ## Troubleshooting  ### Container Exits Immediately - Check logs: `docker logs doris--hudi-spark` - Verify SUCCESS file exists: `docker exec doris--hudi-spark test -f /opt/hudi-scripts/SUCCESS` - Ensure Hive Metastore is running: `docker ps | grep metastore`  ### ClassNotFoundException Errors - Verify JAR files are downloaded: `docker exec doris--hudi-spark ls -lh /opt/hudi-cache/` - Check JAR versions match Spark's Hadoop version (3.3.4) - Review `hudi.env.tpl` for correct version numbers  ### S3A Connection Issues - Verify MinIO is running: `docker ps | grep minio` - Check MinIO credentials in `hudi.env.tpl` - Test S3 connection: `docker exec doris--hudi-minio-mc mc ls myminio/`  ### Hive Metastore Connection Issues - Check Metastore is ready: `docker logs doris--hudi-metastore | grep \"Metastore is ready\"` - Verify PostgreSQL is running: `docker ps | grep metastore-db` - Test connection: `docker exec doris--hudi-metastore-db pg_isready -U hive`  ## File Structure  ``` hudi/ ├── hudi.yaml.tpl          # Docker Compose template ├── hudi.env.tpl           # Environment variables template ├── scripts/ │   ├── init.sh            # Initialization script │   ├── create_preinstalled_scripts/ │   │   └── hudi/          # SQL scripts (01_config_and_database.sql, 02_create_user_activity_log_tables.sql, ...) │   └── SUCCESS            # Initialization marker (generated) └── cache/                 # Downloaded JAR files (generated) ```  ## Notes  - All generated files (`.yaml`, `.env`, `cache/`, `SUCCESS`) are ignored by Git - SQL scripts support environment variable substitution using `${VARIABLE_NAME}` syntax - Hadoop version compatibility is critical - must match Spark's built-in version - Container keeps running after initialization for healthcheck and debugging  ## Notes  - All generated files (`.yaml`, `.env`, `cache/`, `SUCCESS`) are ignored by Git - SQL scripts support environment variable substitution using `${VARIABLE_NAME}` syntax - Hadoop version compatibility is critical - must match Spark's built-in version - Container keeps running after initialization for healthcheck and debugging") | 2 weeks agoJan 7, 2026 |
| [.gitmodules](https://github.com/apache/doris/blob/master/.gitmodules ".gitmodules") | [.gitmodules](https://github.com/apache/doris/blob/master/.gitmodules ".gitmodules") | [\[feat\](index) Ann Index (](https://github.com/apache/doris/commit/4bc6b148ca5b3d0fd829f3493f6eb065156c2f15 "[feat](index) Ann Index (#54276)  ### What problem does this PR solve?  Introducing Ann index to doris.   This pull request introduces foundational support for ANN (Approximate Nearest Neighbor) vector index functionality in the storage engine, including new runtime structures, configuration options, and initial integration with the build system. The changes lay the groundwork for ANN-based search and statistics collection, and begin integrating ANN index support into various storage and query execution paths.  The implementation of ann index is based on [faiss](https://github.com/facebookresearch/faiss). Faiss could return distance directly, so this pr using [virtual slot ref](https://github.com/apache/doris/pull/52701) to return result from index.  Each data segment of doris will have a faiss index if user creates a table with Ann index, and new segment generated by compaction will have a faiss index automatically.  Currently, create index and build index is not supported, index defination be added to ddl if you want it.  **ANN Index Feature Integration:**  * Added new runtime structures and parameters for ANN index operations, including `AnnIndexStats`, `AnnIndexParam`, `RangeSearchParams`, `RangeSearchResult`, and others in `ann_search_params.h`, as well as `RangeSearchRuntimeInfo` for managing ANN range search context. [[1]](diffhunk://#diff-088dbea44296fb3669fe0cd22005df6aff33f8b60b20d5a49a68c6bbd22c29d1R1-R109) [[2]](diffhunk://#diff-ec34e664611a5877cab8f157919c35fe9b697428533c702536c97fd4f05769bdR1-R97) [[3]](diffhunk://#diff-d41283d91ba2756db2b45cadf964a78ad1a5c3360e1b854cb1a3d1f60817c804R1-R44) * Extended `StorageReadOptions` and `RowsetReaderContext` to include `ann_topn_runtime` for passing ANN runtime information through the storage read path. [[1]](diffhunk://#diff-8dec3cee74c5e0821835a7125bede7e3358bd3b5067b2748262193cb4cf80d48R126) [[2]](diffhunk://#diff-19fb296aa338021a0806017aa78ddadbea1791de3f4545724e34f8d9683a6551R95) [[3]](diffhunk://#diff-66ec81528c724b2f69e242d61f35d8a54d7bb5286a7e334c486166a3cc946642R106) * Added new ANN-related statistics fields (timing and row counts) to `OlapReaderStatistics` for monitoring ANN index operations.  **Build System and Dependency Updates:**  * Added `doris-faiss` and `doris-openblas` as submodules for ANN/vector index support, and integrated the new `Vector` library into the build process and as a dependency for relevant targets. [[1]](diffhunk://#diff-fe7afb5c9c916e521401d3fcfb4277d5071798c3baf83baf11d6071742823584R24-R31) [[2]](diffhunk://#diff-3507aac2aff9b5fe5f66d28967f3aa848491d4ced2466f6bf201ab3a97531837R532) [[3]](diffhunk://#diff-3507aac2aff9b5fe5f66d28967f3aa848491d4ced2466f6bf201ab3a97531837R787-R788) [[4]](diffhunk://#diff-d67261040b7ca84a64e8aeef5f7e1a8bab5efcc20fcdd3402f24160f56c29959R26)  **Index Handling and Schema Integration:**  * Updated index file writer accessors and naming from \"inverted_index\" to more generic \"index\" to accommodate ANN and other index types. [[1]](diffhunk://#diff-0c1c144f791918ef5b05ded169a7efb22a0ae67565e641cc03c31d4c2872729eL747-R748) [[2]](diffhunk://#diff-60cd05e044b4218e4a4d774abe89636fa0f6e1290dd0ff7892231d30770cd2b1L193-R193) * Changed index creation logic in `SegmentFlusher` to use `has_extra_index()` (supporting both inverted and ANN indexes) instead of `has_inverted_index()`. [[1]](diffhunk://#diff-7e9f53b4ef59bdb00d10393a2941be9201ddd46c3aab957d1dae8bc5d8898ebeL139-R139) [[2]](diffhunk://#diff-7e9f53b4ef59bdb00d10393a2941be9201ddd46c3aab957d1dae8bc5d8898ebeL157-R157) [[3]](diffhunk://#diff-7e9f53b4ef59bdb00d10393a2941be9201ddd46c3aab957d1dae8bc5d8898ebeL176-R176) [[4]](diffhunk://#diff-7e9f53b4ef59bdb00d10393a2941be9201ddd46c3aab957d1dae8bc5d8898ebeL193-R193)  **Configuration:**  * Introduced a new configuration option `opm_threads_limit` to control the maximum number of OpenMP threads used per Doris thread, which is relevant for vectorized/ANN computation. [[1]](diffhunk://#diff-b626e6ab16bc72abf40db76bf5094fcc8ca3c37534c2eb83b63b7805e1b601ffR1578-R1580) [[2]](diffhunk://#diff-46e8c1ada0d43acf8c2965e46e90909089aada1f46531976c10605b837f8da3dR1634-R1635)  These changes set up the infrastructure required for future development of ANN vector index features, including search, filtering, and statistics collection.  Co-authored-by: chenlinzhong <490103404@qq.com> Co-authored-by: morrySnow <zhangwenxin@selectdb.com>") [#54276](https://github.com/apache/doris/pull/54276) [)](https://github.com/apache/doris/commit/4bc6b148ca5b3d0fd829f3493f6eb065156c2f15 "[feat](index) Ann Index (#54276)  ### What problem does this PR solve?  Introducing Ann index to doris.   This pull request introduces foundational support for ANN (Approximate Nearest Neighbor) vector index functionality in the storage engine, including new runtime structures, configuration options, and initial integration with the build system. The changes lay the groundwork for ANN-based search and statistics collection, and begin integrating ANN index support into various storage and query execution paths.  The implementation of ann index is based on [faiss](https://github.com/facebookresearch/faiss). Faiss could return distance directly, so this pr using [virtual slot ref](https://github.com/apache/doris/pull/52701) to return result from index.  Each data segment of doris will have a faiss index if user creates a table with Ann index, and new segment generated by compaction will have a faiss index automatically.  Currently, create index and build index is not supported, index defination be added to ddl if you want it.  **ANN Index Feature Integration:**  * Added new runtime structures and parameters for ANN index operations, including `AnnIndexStats`, `AnnIndexParam`, `RangeSearchParams`, `RangeSearchResult`, and others in `ann_search_params.h`, as well as `RangeSearchRuntimeInfo` for managing ANN range search context. [[1]](diffhunk://#diff-088dbea44296fb3669fe0cd22005df6aff33f8b60b20d5a49a68c6bbd22c29d1R1-R109) [[2]](diffhunk://#diff-ec34e664611a5877cab8f157919c35fe9b697428533c702536c97fd4f05769bdR1-R97) [[3]](diffhunk://#diff-d41283d91ba2756db2b45cadf964a78ad1a5c3360e1b854cb1a3d1f60817c804R1-R44) * Extended `StorageReadOptions` and `RowsetReaderContext` to include `ann_topn_runtime` for passing ANN runtime information through the storage read path. [[1]](diffhunk://#diff-8dec3cee74c5e0821835a7125bede7e3358bd3b5067b2748262193cb4cf80d48R126) [[2]](diffhunk://#diff-19fb296aa338021a0806017aa78ddadbea1791de3f4545724e34f8d9683a6551R95) [[3]](diffhunk://#diff-66ec81528c724b2f69e242d61f35d8a54d7bb5286a7e334c486166a3cc946642R106) * Added new ANN-related statistics fields (timing and row counts) to `OlapReaderStatistics` for monitoring ANN index operations.  **Build System and Dependency Updates:**  * Added `doris-faiss` and `doris-openblas` as submodules for ANN/vector index support, and integrated the new `Vector` library into the build process and as a dependency for relevant targets. [[1]](diffhunk://#diff-fe7afb5c9c916e521401d3fcfb4277d5071798c3baf83baf11d6071742823584R24-R31) [[2]](diffhunk://#diff-3507aac2aff9b5fe5f66d28967f3aa848491d4ced2466f6bf201ab3a97531837R532) [[3]](diffhunk://#diff-3507aac2aff9b5fe5f66d28967f3aa848491d4ced2466f6bf201ab3a97531837R787-R788) [[4]](diffhunk://#diff-d67261040b7ca84a64e8aeef5f7e1a8bab5efcc20fcdd3402f24160f56c29959R26)  **Index Handling and Schema Integration:**  * Updated index file writer accessors and naming from \"inverted_index\" to more generic \"index\" to accommodate ANN and other index types. [[1]](diffhunk://#diff-0c1c144f791918ef5b05ded169a7efb22a0ae67565e641cc03c31d4c2872729eL747-R748) [[2]](diffhunk://#diff-60cd05e044b4218e4a4d774abe89636fa0f6e1290dd0ff7892231d30770cd2b1L193-R193) * Changed index creation logic in `SegmentFlusher` to use `has_extra_index()` (supporting both inverted and ANN indexes) instead of `has_inverted_index()`. [[1]](diffhunk://#diff-7e9f53b4ef59bdb00d10393a2941be9201ddd46c3aab957d1dae8bc5d8898ebeL139-R139) [[2]](diffhunk://#diff-7e9f53b4ef59bdb00d10393a2941be9201ddd46c3aab957d1dae8bc5d8898ebeL157-R157) [[3]](diffhunk://#diff-7e9f53b4ef59bdb00d10393a2941be9201ddd46c3aab957d1dae8bc5d8898ebeL176-R176) [[4]](diffhunk://#diff-7e9f53b4ef59bdb00d10393a2941be9201ddd46c3aab957d1dae8bc5d8898ebeL193-R193)  **Configuration:**  * Introduced a new configuration option `opm_threads_limit` to control the maximum number of OpenMP threads used per Doris thread, which is relevant for vectorized/ANN computation. [[1]](diffhunk://#diff-b626e6ab16bc72abf40db76bf5094fcc8ca3c37534c2eb83b63b7805e1b601ffR1578-R1580) [[2]](diffhunk://#diff-46e8c1ada0d43acf8c2965e46e90909089aada1f46531976c10605b837f8da3dR1634-R1635)  These changes set up the infrastructure required for future development of ANN vector index features, including search, filtering, and statistics collection.  Co-authored-by: chenlinzhong <490103404@qq.com> Co-authored-by: morrySnow <zhangwenxin@selectdb.com>") | 5 months agoAug 22, 2025 |
| [.licenserc.yaml](https://github.com/apache/doris/blob/master/.licenserc.yaml ".licenserc.yaml") | [.licenserc.yaml](https://github.com/apache/doris/blob/master/.licenserc.yaml ".licenserc.yaml") | [\[Fix\](catalog)Resources should be closed when dropping a Catalog. (](https://github.com/apache/doris/commit/40191ed2fb3e73df7faf683e006ad182d1ba124f "[Fix](catalog)Resources should be closed when dropping a Catalog. (#59512)  ### What problem does this PR solve? When creating an AWS SDK V2 async client (e.g. S3AsyncClient), the SDK requires a thread pool to manage asynchronous task scheduling, timeouts, and retries (e.g. ScheduledExecutorService or async executor).   If no executor is explicitly provided, the AWS SDK creates its own internal thread pool, which is expected to be shut down when client.close() is invoked.  ### Issue  In Doris, when using Paimon Catalog, some catalog implementations provide an empty close() method. As a result, when a user executes DROP CATALOG:  starting with Hadoop 3.4, the AWS SDK was upgraded to v2. Since #57226 upgraded HDFS to 3.4.2, the catalog runs into this issue.  Some Catalog instance is discarded,AWS SDK client.close() is never called,The internally created thread pool cannot be shut down.  This leads to a thread leak  FYI https://github.com/aws/aws-sdk-java-v2/issues/3746  ### Problem Analysis  The lifecycle of Catalog creation and destruction is complex and managed internally by Paimon  Doris cannot reliably intervene in the call chain to enforce AWS SDK client.close()  If each Catalog creates its own AWS SDK client with an internally managed thread pool, threads will continue to leak as Catalogs are repeatedly created and dropped   ### Solution  This PR resolves the issue by introducing a shared executor strategy:  A Doris-managed shared thread pool is explicitly passed when creating AWS SDK V2 clients   This prevents the AWS SDK from implicitly creating per-client internal thread pools  The lifecycle of the executor is fully controlled by Doris and no longer depends on Paimon Catalog’s close() implementation   With this approach, even if a Paimon Catalog’s close() method is a no-op, the system will no longer leak threads. None") [#5…](https://github.com/apache/doris/pull/59512) | 2 weeks agoJan 7, 2026 |
| [.rat-excludes](https://github.com/apache/doris/blob/master/.rat-excludes ".rat-excludes") | [.rat-excludes](https://github.com/apache/doris/blob/master/.rat-excludes ".rat-excludes") | [\[chore\](build) Ignore clucene checks (](https://github.com/apache/doris/commit/ed368d7f6cd008604b886875e47216f390df0b4d "[chore](build) Ignore clucene checks (#19353)") [#19353](https://github.com/apache/doris/pull/19353) [)](https://github.com/apache/doris/commit/ed368d7f6cd008604b886875e47216f390df0b4d "[chore](build) Ignore clucene checks (#19353)") | 3 years agoMay 6, 2023 |
| [.shellcheckrc](https://github.com/apache/doris/blob/master/.shellcheckrc ".shellcheckrc") | [.shellcheckrc](https://github.com/apache/doris/blob/master/.shellcheckrc ".shellcheckrc") | [\[chore\](workflow) Add shellcheck to check shell scripts (](https://github.com/apache/doris/commit/4fa53b4cdbd226ddeb79fdeda3a0e3917888d601 "[chore](workflow) Add shellcheck to check shell scripts (#11744)") [#11744](https://github.com/apache/doris/pull/11744) [)](https://github.com/apache/doris/commit/4fa53b4cdbd226ddeb79fdeda3a0e3917888d601 "[chore](workflow) Add shellcheck to check shell scripts (#11744)") | 4 years agoAug 18, 2022 |
| [CODE\_OF\_CONDUCT.md](https://github.com/apache/doris/blob/master/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md") | [CODE\_OF\_CONDUCT.md](https://github.com/apache/doris/blob/master/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md") | [\[Docs\](Community)Use the standard ASF Code of Conduct template (](https://github.com/apache/doris/commit/bf4e6787955108ab5e1ab3f6fa0100b14cb4161d "[Docs](Community)Use the standard ASF Code of Conduct template (#29586)") [#29586](https://github.com/apache/doris/pull/29586) [)](https://github.com/apache/doris/commit/bf4e6787955108ab5e1ab3f6fa0100b14cb4161d "[Docs](Community)Use the standard ASF Code of Conduct template (#29586)") | 3 years agoJan 5, 2024 |
| [CONTRIBUTING.md](https://github.com/apache/doris/blob/master/CONTRIBUTING.md "CONTRIBUTING.md") | [CONTRIBUTING.md](https://github.com/apache/doris/blob/master/CONTRIBUTING.md "CONTRIBUTING.md") | [docs:update slack link (](https://github.com/apache/doris/commit/bd6384e95931570d25bfbaadfdbc0d8e5c613bbc "docs:update slack link (#59668)") [#59668](https://github.com/apache/doris/pull/59668) [)](https://github.com/apache/doris/commit/bd6384e95931570d25bfbaadfdbc0d8e5c613bbc "docs:update slack link (#59668)") | last weekJan 8, 2026 |
| [CONTRIBUTING\_CN.md](https://github.com/apache/doris/blob/master/CONTRIBUTING_CN.md "CONTRIBUTING_CN.md") | [CONTRIBUTING\_CN.md](https://github.com/apache/doris/blob/master/CONTRIBUTING_CN.md "CONTRIBUTING_CN.md") | [docs:update slack link (](https://github.com/apache/doris/commit/bd6384e95931570d25bfbaadfdbc0d8e5c613bbc "docs:update slack link (#59668)") [#59668](https://github.com/apache/doris/pull/59668) [)](https://github.com/apache/doris/commit/bd6384e95931570d25bfbaadfdbc0d8e5c613bbc "docs:update slack link (#59668)") | last weekJan 8, 2026 |
| [LICENSE.txt](https://github.com/apache/doris/blob/master/LICENSE.txt "LICENSE.txt") | [LICENSE.txt](https://github.com/apache/doris/blob/master/LICENSE.txt "LICENSE.txt") | [\[Fix\](catalog)Resources should be closed when dropping a Catalog. (](https://github.com/apache/doris/commit/40191ed2fb3e73df7faf683e006ad182d1ba124f "[Fix](catalog)Resources should be closed when dropping a Catalog. (#59512)  ### What problem does this PR solve? When creating an AWS SDK V2 async client (e.g. S3AsyncClient), the SDK requires a thread pool to manage asynchronous task scheduling, timeouts, and retries (e.g. ScheduledExecutorService or async executor).   If no executor is explicitly provided, the AWS SDK creates its own internal thread pool, which is expected to be shut down when client.close() is invoked.  ### Issue  In Doris, when using Paimon Catalog, some catalog implementations provide an empty close() method. As a result, when a user executes DROP CATALOG:  starting with Hadoop 3.4, the AWS SDK was upgraded to v2. Since #57226 upgraded HDFS to 3.4.2, the catalog runs into this issue.  Some Catalog instance is discarded,AWS SDK client.close() is never called,The internally created thread pool cannot be shut down.  This leads to a thread leak  FYI https://github.com/aws/aws-sdk-java-v2/issues/3746  ### Problem Analysis  The lifecycle of Catalog creation and destruction is complex and managed internally by Paimon  Doris cannot reliably intervene in the call chain to enforce AWS SDK client.close()  If each Catalog creates its own AWS SDK client with an internally managed thread pool, threads will continue to leak as Catalogs are repeatedly created and dropped   ### Solution  This PR resolves the issue by introducing a shared executor strategy:  A Doris-managed shared thread pool is explicitly passed when creating AWS SDK V2 clients   This prevents the AWS SDK from implicitly creating per-client internal thread pools  The lifecycle of the executor is fully controlled by Doris and no longer depends on Paimon Catalog’s close() implementation   With this approach, even if a Paimon Catalog’s close() method is a no-op, the system will no longer leak threads. None") [#5…](https://github.com/apache/doris/pull/59512) | 2 weeks agoJan 7, 2026 |
| [NOTICE.txt](https://github.com/apache/doris/blob/master/NOTICE.txt "NOTICE.txt") | [NOTICE.txt](https://github.com/apache/doris/blob/master/NOTICE.txt "NOTICE.txt") | [\[Chore\](Notice)Update Notice year to 2026 (](https://github.com/apache/doris/commit/742395e2fbbc78e5e88d6e596af589b777c44eab "[Chore](Notice)Update Notice year to 2026 (#59822)  happy new year") [#59822](https://github.com/apache/doris/pull/59822) [)](https://github.com/apache/doris/commit/742395e2fbbc78e5e88d6e596af589b777c44eab "[Chore](Notice)Update Notice year to 2026 (#59822)  happy new year") | 5 days agoJan 13, 2026 |
| [README.md](https://github.com/apache/doris/blob/master/README.md "README.md") | [README.md](https://github.com/apache/doris/blob/master/README.md "README.md") | [docs:update slack link (](https://github.com/apache/doris/commit/bd6384e95931570d25bfbaadfdbc0d8e5c613bbc "docs:update slack link (#59668)") [#59668](https://github.com/apache/doris/pull/59668) [)](https://github.com/apache/doris/commit/bd6384e95931570d25bfbaadfdbc0d8e5c613bbc "docs:update slack link (#59668)") | last weekJan 8, 2026 |
| [build-for-release.sh](https://github.com/apache/doris/blob/master/build-for-release.sh "build-for-release.sh") | [build-for-release.sh](https://github.com/apache/doris/blob/master/build-for-release.sh "build-for-release.sh") | [\[chore\](build) Fix missing ms in output when build for release arm64 (](https://github.com/apache/doris/commit/d6ea787146e56a61e41e5aa335ea8a082ce803a0 "[chore](build) Fix missing ms in output when build for release arm64 (#46031)") [#…](https://github.com/apache/doris/pull/46031) | 2 years agoDec 30, 2024 |
| [build-plugin.sh](https://github.com/apache/doris/blob/master/build-plugin.sh "build-plugin.sh") | [build-plugin.sh](https://github.com/apache/doris/blob/master/build-plugin.sh "build-plugin.sh") | [\[fix\](code) remove unused files (](https://github.com/apache/doris/commit/6cfe60f8b7576439079bc0b0856a63cb24a81e7a "[fix](code) remove unused files (#46078)  1. remove unused files. 2. rename build_plugin.sh to build-plugin.sh") [#46078](https://github.com/apache/doris/pull/46078) [)](https://github.com/apache/doris/commit/6cfe60f8b7576439079bc0b0856a63cb24a81e7a "[fix](code) remove unused files (#46078)  1. remove unused files. 2. rename build_plugin.sh to build-plugin.sh") | 2 years agoDec 26, 2024 |
| [build.sh](https://github.com/apache/doris/blob/master/build.sh "build.sh") | [build.sh](https://github.com/apache/doris/blob/master/build.sh "build.sh") | [\[Feature\](Streaming Job) Extend streaming job to support Postgres syn…](https://github.com/apache/doris/commit/5f9265695bd64cc8ab1f9b9ef60f847d32164cb2 "[Feature](Streaming Job) Extend streaming job to support Postgres synchronization (#59461)  ### What problem does this PR solve?  This Issues (https://github.com/apache/doris/issues/58896) implements multi-table synchronization in MySQL, The main purpose of this PR is to extend the data source to Postgres.") | 2 weeks agoJan 4, 2026 |
| [doap\_Doris.rdf](https://github.com/apache/doris/blob/master/doap_Doris.rdf "doap_Doris.rdf") | [doap\_Doris.rdf](https://github.com/apache/doris/blob/master/doap_Doris.rdf "doap_Doris.rdf") | [\[fix\](doap) Fix DOAP syntax (](https://github.com/apache/doris/commit/17a17cd10344bf06e63fde04a67458edc443fde2 "[fix](doap) Fix DOAP syntax (#36956)  xmlns strings need to match exactly, and these vocabularies are defined with `http` namespace strings, so we need to follow that.") [#36956](https://github.com/apache/doris/pull/36956) [)](https://github.com/apache/doris/commit/17a17cd10344bf06e63fde04a67458edc443fde2 "[fix](doap) Fix DOAP syntax (#36956)  xmlns strings need to match exactly, and these vocabularies are defined with `http` namespace strings, so we need to follow that.") | 2 years agoJun 27, 2024 |
| [env.sh](https://github.com/apache/doris/blob/master/env.sh "env.sh") | [env.sh](https://github.com/apache/doris/blob/master/env.sh "env.sh") | [\[Enhancement\](build) disable build azure on arm and suppress libunwin…](https://github.com/apache/doris/commit/ce921bf92993c6fef8e34cd886f6a2e841f32f13 "[Enhancement](build) disable build azure on arm and suppress libunwind build warning (#54521)  fix: move target system and architecture detection to env.sh and set DISABLE_BUILD_AZURE based on architecture fix: add warning suppression for incompatible pointer types in libunwind build") | 5 months agoAug 10, 2025 |
| [generated-source.sh](https://github.com/apache/doris/blob/master/generated-source.sh "generated-source.sh") | [generated-source.sh](https://github.com/apache/doris/blob/master/generated-source.sh "generated-source.sh") | [\[chore\](build) speed up generated-source.sh (](https://github.com/apache/doris/commit/c647dd597a439ac462e72021410d4fb144998109 "[chore](build) speed up generated-source.sh (#50980)") [#50980](https://github.com/apache/doris/pull/50980) [)](https://github.com/apache/doris/commit/c647dd597a439ac462e72021410d4fb144998109 "[chore](build) speed up generated-source.sh (#50980)") | 8 months agoMay 16, 2025 |
| [run-be-ut.sh](https://github.com/apache/doris/blob/master/run-be-ut.sh "run-be-ut.sh") | [run-be-ut.sh](https://github.com/apache/doris/blob/master/run-be-ut.sh "run-be-ut.sh") | [\[fix\](inverted index) fix fs set when meet segment corruption (](https://github.com/apache/doris/commit/88ec7dcfb82416c66616b80d8d5c1f440c1ecc63 "[fix](inverted index) fix fs set when meet segment corruption (#58317)   Related PR: #55692") [#58317](https://github.com/apache/doris/pull/58317) [)](https://github.com/apache/doris/commit/88ec7dcfb82416c66616b80d8d5c1f440c1ecc63 "[fix](inverted index) fix fs set when meet segment corruption (#58317)   Related PR: #55692") | 2 months agoNov 26, 2025 |
| [run-cloud-ut.sh](https://github.com/apache/doris/blob/master/run-cloud-ut.sh "run-cloud-ut.sh") | [run-cloud-ut.sh](https://github.com/apache/doris/blob/master/run-cloud-ut.sh "run-cloud-ut.sh") | [\[Test\](client) Add s3 storage client test for recycler s3 accessor (](https://github.com/apache/doris/commit/92b686d5c50c91b50c080a3a4f1896a9d07d2214 "[Test](client) Add s3 storage client test for recycler s3 accessor  (#58242)  successor PR to https://github.com/apache/doris/pull/57456  If you want to run unit tests using a real cloud provider, you can configure it in custom_env.sh. ``` # Enable S3 client for unit tests export ENABLE_S3_CLIENT=1 # S3 client config export S3_AK=\"xxx\" export S3_SK=\"xxx\" export S3_ENDPOINT=\"xxx\" export S3_PROVIDER=\"xxx\" export S3_BUCKET=\"xxx\" export S3_REGION=\"xxx\" export S3_PREFIX=\"\" ```") [#…](https://github.com/apache/doris/pull/58242) | 2 months agoDec 1, 2025 |
| [run-fe-ut.sh](https://github.com/apache/doris/blob/master/run-fe-ut.sh "run-fe-ut.sh") | [run-fe-ut.sh](https://github.com/apache/doris/blob/master/run-fe-ut.sh "run-fe-ut.sh") | [\[chore\](feut) add maven.test.failure.ignore (](https://github.com/apache/doris/commit/a59168417abc4fc874565406f35ef23ea3aa7417 "[chore](feut) add maven.test.failure.ignore (#51134)") [#51134](https://github.com/apache/doris/pull/51134) [)](https://github.com/apache/doris/commit/a59168417abc4fc874565406f35ef23ea3aa7417 "[chore](feut) add maven.test.failure.ignore (#51134)") | 8 months agoMay 27, 2025 |
| [run-regression-test.sh](https://github.com/apache/doris/blob/master/run-regression-test.sh "run-regression-test.sh") | [run-regression-test.sh](https://github.com/apache/doris/blob/master/run-regression-test.sh "run-regression-test.sh") | [\[improve\](case) Fix spark flink case in jdk17 (](https://github.com/apache/doris/commit/04e5604246cfa89528b5aa9e68dc9034c4e158c5 "[improve](case) Fix spark flink case in jdk17 (#56992)  ### What problem does this PR solve?  Fix spark flink case in jdk17") [#56992](https://github.com/apache/doris/pull/56992) [)](https://github.com/apache/doris/commit/04e5604246cfa89528b5aa9e68dc9034c4e158c5 "[improve](case) Fix spark flink case in jdk17 (#56992)  ### What problem does this PR solve?  Fix spark flink case in jdk17") | 3 months agoOct 16, 2025 |
| [sonar-project.properties](https://github.com/apache/doris/blob/master/sonar-project.properties "sonar-project.properties") | [sonar-project.properties](https://github.com/apache/doris/blob/master/sonar-project.properties "sonar-project.properties") | [\[chore\](workflow) Fix security issues in Code Checks (](https://github.com/apache/doris/commit/e9ef6c7da71af7a73ca45e60cc9587d774629254 "[chore](workflow) Fix security issues in Code Checks (#24761)  The workflow `Code Checks` needs write permissions granted by the event `pull_request_target` to comment on pull requests. However, if the workflow ran users' code, the malicious code would do some dangerous actions on our repository.  The following changes are made in this PR: 1. Instead of applying patches, we use `sed` to modify the `entrypoint.sh` in action-sh-checker explicitly in the workflow. 2. Revoke the write permissions when generating `compile_commands.json` which is produced by executing the build script `build.sh`.") [#24761](https://github.com/apache/doris/pull/24761) [)](https://github.com/apache/doris/commit/e9ef6c7da71af7a73ca45e60cc9587d774629254 "[chore](workflow) Fix security issues in Code Checks (#24761)  The workflow `Code Checks` needs write permissions granted by the event `pull_request_target` to comment on pull requests. However, if the workflow ran users' code, the malicious code would do some dangerous actions on our repository.  The following changes are made in this PR: 1. Instead of applying patches, we use `sed` to modify the `entrypoint.sh` in action-sh-checker explicitly in the workflow. 2. Revoke the write permissions when generating `compile_commands.json` which is produced by executing the build script `build.sh`.") | 3 years agoSep 21, 2023 |
| View all files |

## Repository files navigation

## 🌍 Read this in other languages

[Permalink: 🌍 Read this in other languages](https://github.com/apache/doris#-read-this-in-other-languages)

[English](https://github.com/apache/doris/blob/master/README.md) • [العربية](https://github.com/apache/doris/blob/master/docs/ar-SA/README.md) • [বাংলা](https://github.com/apache/doris/blob/master/docs/bn-BD/README.md) • [Deutsch](https://github.com/apache/doris/blob/master/docs/de-DE/README.md) • [Español](https://github.com/apache/doris/blob/master/docs/es-ES/README.md) • [فارسی](https://github.com/apache/doris/blob/master/docs/fa-IR/README.md) • [Français](https://github.com/apache/doris/blob/master/docs/fr-FR/README.md) • [हिन्दी](https://github.com/apache/doris/blob/master/docs/hi-IN/README.md) • [Bahasa Indonesia](https://github.com/apache/doris/blob/master/docs/id-ID/README.md) • [Italiano](https://github.com/apache/doris/blob/master/docs/it-IT/README.md) • [日本語](https://github.com/apache/doris/blob/master/docs/ja-JP/README.md) • [한국어](https://github.com/apache/doris/blob/master/docs/ko-KR/README.md) • [Polski](https://github.com/apache/doris/blob/master/docs/pl-PL/README.md) • [Português](https://github.com/apache/doris/blob/master/docs/pt-BR/README.md) • [Română](https://github.com/apache/doris/blob/master/docs/ro-RO/README.md) • [Русский](https://github.com/apache/doris/blob/master/docs/ru-RU/README.md) • [Slovenščina](https://github.com/apache/doris/blob/master/docs/sl-SI/README.md) • [ไทย](https://github.com/apache/doris/blob/master/docs/th-TH/README.md) • [Türkçe](https://github.com/apache/doris/blob/master/docs/tr-TR/README.md) • [Українська](https://github.com/apache/doris/blob/master/docs/uk-UA/README.md) • [Tiếng Việt](https://github.com/apache/doris/blob/master/docs/vi-VN/README.md) • [简体中文](https://github.com/apache/doris/blob/master/docs/zh-CN/README.md) • [繁體中文](https://github.com/apache/doris/blob/master/docs/zh-TW/README.md)

# Apache Doris

[Permalink: Apache Doris](https://github.com/apache/doris#apache-doris)

[![License](https://camo.githubusercontent.com/b3dd5777bf0d89dc72a2045cbf476c7bcdab389bd0ad40aeb339da333c3a2265/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322d3445423142412e737667)](https://www.apache.org/licenses/LICENSE-2.0.html)[![GitHub release](https://camo.githubusercontent.com/a3af46b954500837d366541b3d8b68939c4f4fa1cbf8d3d128212bbc9e0e1a01/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6170616368652f646f7269732e737667)](https://github.com/apache/doris/releases)[![OSSRank](https://camo.githubusercontent.com/b25a891a95a7cf65a486da864c412c1bd99da200ebbb3faf08e31798846a0603/68747470733a2f2f736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f6f737372616e6b2e636f6d2f736869656c642f353136)](https://ossrank.com/p/516)[![Commit activity](https://camo.githubusercontent.com/0644e93fbbe2a85f697df80b8c778949b4ef9eae6bd32d5bca0431f101baec02/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f6170616368652f646f726973)](https://github.com/apache/doris/commits/master/)[![EN doc](https://camo.githubusercontent.com/4cfddde641679b293d25a1586bb55ad9c031c6691306c83ae5f292b1e1081b40/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63732d456e676c6973682d626c75652e737667)](https://doris.apache.org/docs/gettingStarted/what-is-apache-doris)[![CN doc](https://camo.githubusercontent.com/509cdb11ce87781d605e01c95c7839ff3ff5f29524ddf3a79bfdc25b4c030df3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362539362538372545362541312541332d2545342542382541442545362539362538372545372538392538382d626c75652e737667)](https://doris.apache.org/zh-CN/docs/gettingStarted/what-is-apache-doris)

[![Official Website](https://camo.githubusercontent.com/ec672cb7317c2447c2bba0f3a645e882b625491375f03f669d60527891a16978/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d56697369742532307468652532304f6666696369616c253230576562736974652532302545322538362539322d7267622831352c3231342c313036293f7374796c653d666f722d7468652d6261646765)](https://doris.apache.org/)[![Quick Download](https://camo.githubusercontent.com/c147fc47ce93eba38962a8b5b429a33fffabaf474df889acd34e8cf2dc363ad0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d517569636b253230253230446f776e6c6f61642532302545322538362539322d7267622836362c35362c323535293f7374796c653d666f722d7468652d6261646765)](https://doris.apache.org/download)

[![](https://camo.githubusercontent.com/416881b3484ed0c4336ca2f9047243f1bc7eff939af2335a69909d29032fdeea/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d2040446f7269735f417061636865202d3432343534393f7374796c653d736f6369616c266c6f676f3d78)](https://twitter.com/doris_apache)[![](https://camo.githubusercontent.com/6003ddad43eb38b1562df238c1de822c921fd73e388809ab00ffe54e4a7977c0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d2044697363757373696f6e202d7265643f7374796c653d736f6369616c266c6f676f3d646973636f75727365)](https://github.com/apache/doris/discussions)[![](https://camo.githubusercontent.com/6a987a657689c4f0c1942c73644ae63765fa7a37370488c9ebeecbb273c90738/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4d656469756d2d7265643f7374796c653d736f6369616c266c6f676f3d6d656469756d)](https://medium.com/@ApacheDoris)

* * *

[![apache%2Fdoris | Trendshift](https://camo.githubusercontent.com/e99b17085338cec86d51f212bb009c3e82d7c54a1ae7375e099eb7a3f0541108/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f31313536)](https://trendshift.io/repositories/1156)

Apache Doris is an easy-to-use, high-performance and real-time analytical database based on MPP architecture, known for its extreme speed and ease of use. It only requires a sub-second response time to return query results under massive data and can support not only high-concurrency point query scenarios but also high-throughput complex analysis scenarios.

All this makes Apache Doris an ideal tool for scenarios including report analysis, ad-hoc query, unified data warehouse, and data lake query acceleration. On Apache Doris, users can build various applications, such as user behavior analysis, AB test platform, log retrieval analysis, user portrait analysis, and order analysis.

🎉 Check out the 🔗 [All releases](https://doris.apache.org/docs/releasenotes/all-release), where you'll find a chronological summary of Apache Doris versions released over the past year.

👀 Explore the 🔗 [Official Website](https://doris.apache.org/) to discover Apache Doris's core features, blogs, and user cases in detail.

## 📈 Usage Scenarios

[Permalink: 📈 Usage Scenarios](https://github.com/apache/doris#-usage-scenarios)

As shown in the figure below, after various data integration and processing, the data sources are usually stored in the real-time data warehouse Apache Doris and the offline data lake or data warehouse (in Apache Hive, Apache Iceberg or Apache Hudi).

[![](https://camo.githubusercontent.com/b2a4f2dbc2871fca02480d377fc2f44ab5394105387d8089ffab47be163de096/68747470733a2f2f63646e2e73656c65637464622e636f6d2f7374617469632f576861745f69735f4170616368655f446f7269735f335f613631363932633263652e706e67)](https://camo.githubusercontent.com/b2a4f2dbc2871fca02480d377fc2f44ab5394105387d8089ffab47be163de096/68747470733a2f2f63646e2e73656c65637464622e636f6d2f7374617469632f576861745f69735f4170616368655f446f7269735f335f613631363932633263652e706e67)

Apache Doris is widely used in the following scenarios:

- **Real-time Data Analysis**:
  - **Real-time Reporting and Decision-making**: Doris provides real-time updated reports and dashboards for both internal and external enterprise use, supporting real-time decision-making in automated processes.

  - **Ad Hoc Analysis**: Doris offers multidimensional data analysis capabilities, enabling rapid business intelligence analysis and ad hoc queries to help users quickly uncover insights from complex data.

  - **User Profiling and Behavior Analysis**: Doris can analyze user behaviors such as participation, retention, and conversion, while also supporting scenarios like population insights and crowd selection for behavior analysis.
- **Lakehouse Analytics**:
  - **Lakehouse Query Acceleration**: Doris accelerates lakehouse data queries with its efficient query engine.

  - **Federated Analytics**: Doris supports federated queries across multiple data sources, simplifying architecture and eliminating data silos.

  - **Real-time Data Processing**: Doris combines real-time data streams and batch data processing capabilities to meet the needs of high concurrency and low-latency complex business requirements.
- **SQL-based Observability**:
  - **Log and Event Analysis**: Doris enables real-time or batch analysis of logs and events in distributed systems, helping to identify issues and optimize performance.

## Overall Architecture

[Permalink: Overall Architecture](https://github.com/apache/doris#overall-architecture)

Apache Doris uses the MySQL protocol, is highly compatible with MySQL syntax, and supports standard SQL. Users can access Apache Doris through various client tools, and it seamlessly integrates with BI tools.

### Storage-Compute Integrated Architecture

[Permalink: Storage-Compute Integrated Architecture](https://github.com/apache/doris#storage-compute-integrated-architecture)

The storage-compute integrated architecture of Apache Doris is streamlined and easy to maintain. As shown in the figure below, it consists of only two types of processes:

- **Frontend (FE):** Primarily responsible for handling user requests, query parsing and planning, metadata management, and node management tasks.

- **Backend (BE):** Primarily responsible for data storage and query execution. Data is partitioned into shards and stored with multiple replicas across BE nodes.


[![The overall architecture of Apache Doris](https://camo.githubusercontent.com/2c596fd207cac4fffabfa8bfd382052b9350e701903056c779c93208612918a5/68747470733a2f2f63646e2e73656c65637464622e636f6d2f7374617469632f576861745f69735f4170616368655f446f7269735f616462323633393765322e706e67)](https://camo.githubusercontent.com/2c596fd207cac4fffabfa8bfd382052b9350e701903056c779c93208612918a5/68747470733a2f2f63646e2e73656c65637464622e636f6d2f7374617469632f576861745f69735f4170616368655f446f7269735f616462323633393765322e706e67)

In a production environment, multiple FE nodes can be deployed for disaster recovery. Each FE node maintains a full copy of the metadata. The FE nodes are divided into three roles:

| Role | Function |
| --- | --- |
| Master | The FE Master node is responsible for metadata read and write operations. When metadata changes occur in the Master, they are synchronized to Follower or Observer nodes via the BDB JE protocol. |
| Follower | The Follower node is responsible for reading metadata. If the Master node fails, a Follower node can be selected as the new Master. |
| Observer | The Observer node is responsible for reading metadata and is mainly used to increase query concurrency. It does not participate in cluster leadership elections. |

Both FE and BE processes are horizontally scalable, enabling a single cluster to support hundreds of machines and tens of petabytes of storage capacity. The FE and BE processes use a consistency protocol to ensure high availability of services and high reliability of data. The storage-compute integrated architecture is highly integrated, significantly reducing the operational complexity of distributed systems.

## Core Features of Apache Doris

[Permalink: Core Features of Apache Doris](https://github.com/apache/doris#core-features-of-apache-doris)

- **High Availability**: In Apache Doris, both metadata and data are stored with multiple replicas, synchronizing data logs via the quorum protocol. Data write is considered successful once a majority of replicas have completed the write, ensuring that the cluster remains available even if a few nodes fail. Apache Doris supports both same-city and cross-region disaster recovery, enabling dual-cluster master-slave modes. When some nodes experience failures, the cluster can automatically isolate the faulty nodes, preventing the overall cluster availability from being affected.

- **High Compatibility**: Apache Doris is highly compatible with the MySQL protocol and supports standard SQL syntax, covering most MySQL and Hive functions. This high compatibility allows users to seamlessly migrate and integrate existing applications and tools. Apache Doris supports the MySQL ecosystem, enabling users to connect Doris using MySQL Client tools for more convenient operations and maintenance. It also supports MySQL protocol compatibility for BI reporting tools and data transmission tools, ensuring efficiency and stability in data analysis and data transmission processes.

- **Real-Time Data Warehouse**: Based on Apache Doris, a real-time data warehouse service can be built. Apache Doris offers second-level data ingestion capabilities, capturing incremental changes from upstream online transactional databases into Doris within seconds. Leveraging vectorized engines, MPP architecture, and Pipeline execution engines, Doris provides sub-second data query capabilities, thereby constructing a high-performance, low-latency real-time data warehouse platform.

- **Unified Lakehouse**: Apache Doris can build a unified lakehouse architecture based on external data sources such as data lakes or relational databases. The Doris unified lakehouse solution enables seamless integration and free data flow between data lakes and data warehouses, helping users directly utilize data warehouse capabilities to solve data analysis problems in data lakes while fully leveraging data lake data management capabilities to enhance data value.

- **Flexible Modeling**: Apache Doris offers various modeling approaches, such as wide table models, pre-aggregation models, star/snowflake schemas, etc. During data import, data can be flattened into wide tables and written into Doris through compute engines like Flink or Spark, or data can be directly imported into Doris, performing data modeling operations through views, materialized views, or real-time multi-table joins.


## Technical overview

[Permalink: Technical overview](https://github.com/apache/doris#technical-overview)

Doris provides an efficient SQL interface and is fully compatible with the MySQL protocol. Its query engine is based on an MPP (Massively Parallel Processing) architecture, capable of efficiently executing complex analytical queries and achieving low-latency real-time queries. Through columnar storage technology for data encoding and compression, it significantly optimizes query performance and storage compression ratio.

### Interface

[Permalink: Interface](https://github.com/apache/doris#interface)

Apache Doris adopts the MySQL protocol, supports standard SQL, and is highly compatible with MySQL syntax. Users can access Apache Doris through various client tools and seamlessly integrate it with BI tools, including but not limited to Smartbi, DataEase, FineBI, Tableau, Power BI, and Apache Superset. Apache Doris can work as the data source for any BI tools that support the MySQL protocol.

### Storage engine

[Permalink: Storage engine](https://github.com/apache/doris#storage-engine)

Apache Doris has a columnar storage engine, which encodes, compresses, and reads data by column. This enables a very high data compression ratio and largely reduces unnecessary data scanning, thus making more efficient use of IO and CPU resources.

Apache Doris supports various index structures to minimize data scans:

- **Sorted Compound Key Index**: Users can specify three columns at most to form a compound sort key. This can effectively prune data to better support highly concurrent reporting scenarios.

- **Min/Max Index**: This enables effective data filtering in equivalence and range queries of numeric types.

- **BloomFilter Index**: This is very effective in equivalence filtering and pruning of high-cardinality columns.

- **Inverted Index**: This enables fast searching for any field.


Apache Doris supports a variety of data models and has optimized them for different scenarios:

- **Detail Model (Duplicate Key Model):** A detail data model designed to meet the detailed storage requirements of fact tables.

- **Primary Key Model (Unique Key Model):** Ensures unique keys; data with the same key is overwritten, enabling row-level data updates.

- **Aggregate Model (Aggregate Key Model):** Merges value columns with the same key, significantly improving performance through pre-aggregation.


Apache Doris also supports strongly consistent single-table materialized views and asynchronously refreshed multi-table materialized views. Single-table materialized views are automatically refreshed and maintained by the system, requiring no manual intervention from users. Multi-table materialized views can be refreshed periodically using in-cluster scheduling or external scheduling tools, reducing the complexity of data modeling.

### 🔍 Query Engine

[Permalink: 🔍 Query Engine](https://github.com/apache/doris#-query-engine)

Apache Doris has an MPP-based query engine for parallel execution between and within nodes. It supports distributed shuffle join for large tables to better handle complicated queries.

[![Query Engine](https://camo.githubusercontent.com/2966f19a7cabe9240d94a85852652e593953bb6b205728890960c284298a5bbf/68747470733a2f2f63646e2e73656c65637464622e636f6d2f7374617469632f576861745f69735f4170616368655f446f7269735f315f633666356261326166392e706e67)](https://camo.githubusercontent.com/2966f19a7cabe9240d94a85852652e593953bb6b205728890960c284298a5bbf/68747470733a2f2f63646e2e73656c65637464622e636f6d2f7374617469632f576861745f69735f4170616368655f446f7269735f315f633666356261326166392e706e67)

The query engine of Apache Doris is fully vectorized, with all memory structures laid out in a columnar format. This can largely reduce virtual function calls, increase cache hit rates, and make efficient use of SIMD instructions. Apache Doris delivers a 5~10 times higher performance in wide table aggregation scenarios than non-vectorized engines.

[![Doris query engine](https://camo.githubusercontent.com/4a5be4761342d6ac61d8787851ee300fca3cca8189714c3d04280e4d1e52a368/68747470733a2f2f63646e2e73656c65637464622e636f6d2f7374617469632f576861745f69735f4170616368655f446f7269735f325f323963663538636336622e706e67)](https://camo.githubusercontent.com/4a5be4761342d6ac61d8787851ee300fca3cca8189714c3d04280e4d1e52a368/68747470733a2f2f63646e2e73656c65637464622e636f6d2f7374617469632f576861745f69735f4170616368655f446f7269735f325f323963663538636336622e706e67)

Apache Doris uses adaptive query execution technology to dynamically adjust the execution plan based on runtime statistics. For example, it can generate a runtime filter and push it to the probe side. Specifically, it pushes the filters to the lowest-level scan node on the probe side, which largely reduces the data amount to be processed and increases join performance. The runtime filter of Apache Doris supports In/Min/Max/Bloom Filter.

Apache Doris uses a Pipeline execution engine that breaks down queries into multiple sub-tasks for parallel execution, fully leveraging multi-core CPU capabilities. It simultaneously addresses the thread explosion problem by limiting the number of query threads. The Pipeline execution engine reduces data copying and sharing, optimizes sorting and aggregation operations, thereby significantly improving query efficiency and throughput.

In terms of the optimizer, Apache Doris employs a combined optimization strategy of CBO (Cost-Based Optimizer), RBO (Rule-Based Optimizer), and HBO (History-Based Optimizer). RBO supports constant folding, subquery rewriting, predicate pushdown, and more. CBO supports join reordering and other optimizations. HBO recommends the optimal execution plan based on historical query information. These multiple optimization measures ensure that Doris can enumerate high-performance query plans across various types of queries.

## 🎆 Why choose Apache Doris?

[Permalink: 🎆 Why choose Apache Doris?](https://github.com/apache/doris#-why-choose-apache-doris)

- 🎯 **Easy to Use:** Two processes, no other dependencies; online cluster scaling, automatic replica recovery; compatible with MySQL protocol, and using standard SQL.

- 🚀 **High Performance:** Extremely fast performance for low-latency and high-throughput queries with columnar storage engine, modern MPP architecture, vectorized query engine, pre-aggregated materialized view and data index.

- 🖥️ **Single Unified:** A single system can support real-time data serving, interactive data analysis and offline data processing scenarios.

- ⚛️ **Federated Querying:** Supports federated querying of data lakes such as Hive, Iceberg, Hudi, and databases such as MySQL and Elasticsearch.

- ⏩ **Various Data Import Methods:** Supports batch import from HDFS/S3 and stream import from MySQL Binlog/Kafka; supports micro-batch writing through HTTP interface and real-time writing using Insert in JDBC.

- 🚙 **Rich Ecology:** Spark uses Spark-Doris-Connector to read and write Doris; Flink-Doris-Connector enables Flink CDC to implement exactly-once data writing to Doris; DBT Doris Adapter is provided to transform data in Doris with DBT.


## 🙌 Contributors

[Permalink: 🙌 Contributors](https://github.com/apache/doris#-contributors)

**Apache Doris has graduated from Apache incubator successfully and become a Top-Level Project in June 2022**.

We deeply appreciate 🔗 [community contributors](https://github.com/apache/doris/graphs/contributors) for their contribution to Apache Doris.

[![contrib graph](https://camo.githubusercontent.com/f5d9f3d71e51b0fa5e03d217994c6206ee5013976563d736b0e1c4485f52e3dc/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6170616368652f646f726973)](https://github.com/apache/doris/graphs/contributors)

## 👨‍👩‍👧‍👦 Users

[Permalink: 👨‍👩‍👧‍👦 Users](https://github.com/apache/doris#%E2%80%8D%E2%80%8D%E2%80%8D-users)

Apache Doris now has a wide user base in China and around the world, and as of today, **Apache Doris is used in production environments in thousands of companies worldwide.** More than 80% of the top 50 Internet companies in China in terms of market capitalization or valuation have been using Apache Doris for a long time, including Baidu, Meituan, Xiaomi, Jingdong, Bytedance, Tencent, NetEase, Kwai, Sina, 360, Mihoyo, and Ke Holdings. It is also widely used in some traditional industries such as finance, energy, manufacturing, and telecommunications.

The users of Apache Doris: 🔗 [Users](https://doris.apache.org/users)

Add your company logo at Apache Doris Website: 🔗 [Add Your Company](https://github.com/apache/doris/discussions/27683)

## 👣 Get Started

[Permalink: 👣 Get Started](https://github.com/apache/doris#-get-started)

### 📚 Docs

[Permalink: 📚 Docs](https://github.com/apache/doris#-docs)

All Documentation 🔗 [Docs](https://doris.apache.org/docs/gettingStarted/what-is-apache-doris)

### ⬇️ Download

[Permalink: ⬇️ Download](https://github.com/apache/doris#%EF%B8%8F-download)

All release and binary version 🔗 [Download](https://doris.apache.org/download)

### 🗄️ Compile

[Permalink: 🗄️ Compile](https://github.com/apache/doris#%EF%B8%8F-compile)

See how to compile 🔗 [Compilation](https://doris.apache.org/community/source-install/compilation-with-docker))

### 📮 Install

[Permalink: 📮 Install](https://github.com/apache/doris#-install)

See how to install and deploy 🔗 [Installation and deployment](https://doris.apache.org/docs/install/preparation/env-checking)

## 🧩 Components

[Permalink: 🧩 Components](https://github.com/apache/doris#-components)

### 📝 Doris Connector

[Permalink: 📝 Doris Connector](https://github.com/apache/doris#-doris-connector)

Doris provides support for Spark/Flink to read data stored in Doris through Connector, and also supports to write data to Doris through Connector.

🔗 [apache/doris-flink-connector](https://github.com/apache/doris-flink-connector)

🔗 [apache/doris-spark-connector](https://github.com/apache/doris-spark-connector)

## 🌈 Community and Support

[Permalink: 🌈 Community and Support](https://github.com/apache/doris#-community-and-support)

### 📤 Subscribe Mailing Lists

[Permalink: 📤 Subscribe Mailing Lists](https://github.com/apache/doris#-subscribe-mailing-lists)

Mail List is the most recognized form of communication in Apache community. See how to 🔗 [Subscribe Mailing Lists](https://doris.apache.org/community/subscribe-mail-list)

### 🙋 Report Issues or Submit Pull Request

[Permalink: 🙋 Report Issues or Submit Pull Request](https://github.com/apache/doris#-report-issues-or-submit-pull-request)

If you meet any questions, feel free to file a 🔗 [GitHub Issue](https://github.com/apache/doris/issues) or post it in 🔗 [GitHub Discussion](https://github.com/apache/doris/discussions) and fix it by submitting a 🔗 [Pull Request](https://github.com/apache/doris/pulls)

### 🍻 How to Contribute

[Permalink: 🍻 How to Contribute](https://github.com/apache/doris#-how-to-contribute)

We welcome your suggestions, comments (including criticisms), comments and contributions. See 🔗 [How to Contribute](https://doris.apache.org/community/how-to-contribute/) and 🔗 [Code Submission Guide](https://doris.apache.org/community/how-to-contribute/pull-request/)

### ⌨️ Doris Improvement Proposals (DSIP)

[Permalink: ⌨️ Doris Improvement Proposals (DSIP)](https://github.com/apache/doris#%EF%B8%8F-doris-improvement-proposals-dsip)

🔗 [Doris Improvement Proposal (DSIP)](https://cwiki.apache.org/confluence/display/DORIS/Doris+Improvement+Proposals) can be thought of as **A Collection of Design Documents for all Major Feature Updates or Improvements**.

### 🔑 Backend C++ Coding Specification

[Permalink: 🔑 Backend C++ Coding Specification](https://github.com/apache/doris#-backend-c-coding-specification)

🔗 [Backend C++ Coding Specification](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=240883637) should be strictly followed, which will help us achieve better code quality.

## 💬 Contact Us

[Permalink: 💬 Contact Us](https://github.com/apache/doris#-contact-us)

Contact us through the following mailing list.

| Name | Scope |  |  |  |
| :-- | :-- | :-- | :-- | :-- |
| [dev@doris.apache.org](mailto:dev@doris.apache.org) | Development-related discussions | [Subscribe](mailto:dev-subscribe@doris.apache.org) | [Unsubscribe](mailto:dev-unsubscribe@doris.apache.org) | [Archives](http://mail-archives.apache.org/mod_mbox/doris-dev/) |

## 🧰 Links

[Permalink: 🧰 Links](https://github.com/apache/doris#-links)

- Apache Doris Official Website - [Site](https://doris.apache.org/)
- Developer Mailing list - [dev@doris.apache.org](mailto:dev@doris.apache.org). Mail to [dev-subscribe@doris.apache.org](mailto:dev-subscribe@doris.apache.org), follow the reply to subscribe the mail list.
- Slack channel - [Join the Slack](https://doris.apache.org/slack)
- Twitter - [Follow @doris\_apache](https://twitter.com/doris_apache)

## 📜 License

[Permalink: 📜 License](https://github.com/apache/doris#-license)

[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)

> **Note**
> Some licenses of the third-party dependencies are not compatible with Apache 2.0 License. So you need to disable
> some Doris features to be complied with Apache 2.0 License. For details, refer to the `thirdparty/LICENSE.txt`

## About

Apache Doris is an easy-to-use, high performance and unified analytics database.


[doris.apache.org](https://doris.apache.org/ "https://doris.apache.org")

### Topics

[agent](https://github.com/topics/agent "Topic: agent") [bigquery](https://github.com/topics/bigquery "Topic: bigquery") [real-time](https://github.com/topics/real-time "Topic: real-time") [sql](https://github.com/topics/sql "Topic: sql") [database](https://github.com/topics/database "Topic: database") [ai](https://github.com/topics/ai "Topic: ai") [spark](https://github.com/topics/spark "Topic: spark") [snowflake](https://github.com/topics/snowflake "Topic: snowflake") [olap](https://github.com/topics/olap "Topic: olap") [query-engine](https://github.com/topics/query-engine "Topic: query-engine") [redshift](https://github.com/topics/redshift "Topic: redshift") [dbt](https://github.com/topics/dbt "Topic: dbt") [elt](https://github.com/topics/elt "Topic: elt") [iceberg](https://github.com/topics/iceberg "Topic: iceberg") [hudi](https://github.com/topics/hudi "Topic: hudi") [delta-lake](https://github.com/topics/delta-lake "Topic: delta-lake") [lakehouse](https://github.com/topics/lakehouse "Topic: lakehouse") [paimon](https://github.com/topics/paimon "Topic: paimon")

### Resources

[Readme](https://github.com/apache/doris#readme-ov-file)

### License

[Apache-2.0 license](https://github.com/apache/doris#Apache-2.0-1-ov-file)

### Code of conduct

[Code of conduct](https://github.com/apache/doris#coc-ov-file)

### Contributing

[Contributing](https://github.com/apache/doris#contributing-ov-file)

### Security policy

[Security policy](https://github.com/apache/doris#security-ov-file)

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/apache/doris).

[Activity](https://github.com/apache/doris/activity)

[Custom properties](https://github.com/apache/doris/custom-properties)

### Stars

[**14.9k**\\
stars](https://github.com/apache/doris/stargazers)

### Watchers

[**283**\\
watching](https://github.com/apache/doris/watchers)

### Forks

[**3.7k**\\
forks](https://github.com/apache/doris/forks)

[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fapache%2Fdoris&report=apache+%28user%29)

## [Releases\  81](https://github.com/apache/doris/releases)

[Apache Doris 4.0.2 Release\\
Latest\\
\\
on Dec 14, 2025Dec 15, 2025](https://github.com/apache/doris/releases/tag/4.0.2-rc02)

[\+ 80 releases](https://github.com/apache/doris/releases)

## [Packages\  0](https://github.com/orgs/apache/packages?repo_name=doris)

No packages published

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/apache/doris).

## [Contributors\  710](https://github.com/apache/doris/graphs/contributors)

- [![@morningman](https://avatars.githubusercontent.com/u/2899462?s=64&v=4)](https://github.com/morningman)
- [![@BiteTheDDDDt](https://avatars.githubusercontent.com/u/7939630?s=64&v=4)](https://github.com/BiteTheDDDDt)
- [![@Gabriel39](https://avatars.githubusercontent.com/u/37700562?s=64&v=4)](https://github.com/Gabriel39)
- [![@morrySnow](https://avatars.githubusercontent.com/u/101034200?s=64&v=4)](https://github.com/morrySnow)
- [![@HappenLee](https://avatars.githubusercontent.com/u/10553413?s=64&v=4)](https://github.com/HappenLee)
- [![@englefly](https://avatars.githubusercontent.com/u/1747806?s=64&v=4)](https://github.com/englefly)
- [![@Mryange](https://avatars.githubusercontent.com/u/59914473?s=64&v=4)](https://github.com/Mryange)
- [![@w41ter](https://avatars.githubusercontent.com/u/7908062?s=64&v=4)](https://github.com/w41ter)
- [![@starocean999](https://avatars.githubusercontent.com/u/40539150?s=64&v=4)](https://github.com/starocean999)
- [![@zy-kkk](https://avatars.githubusercontent.com/u/70795751?s=64&v=4)](https://github.com/zy-kkk)
- [![@xinyiZzz](https://avatars.githubusercontent.com/u/13197424?s=64&v=4)](https://github.com/xinyiZzz)
- [![@Jibing-Li](https://avatars.githubusercontent.com/u/64681310?s=64&v=4)](https://github.com/Jibing-Li)
- [![@zhangstar333](https://avatars.githubusercontent.com/u/87313068?s=64&v=4)](https://github.com/zhangstar333)
- [![@yiguolei](https://avatars.githubusercontent.com/u/9208457?s=64&v=4)](https://github.com/yiguolei)

[\+ 696 contributors](https://github.com/apache/doris/graphs/contributors)

## Languages

- [Java47.0%](https://github.com/apache/doris/search?l=java)
- [C++44.9%](https://github.com/apache/doris/search?l=c%2B%2B)
- [Python5.1%](https://github.com/apache/doris/search?l=python)
- [Shell1.1%](https://github.com/apache/doris/search?l=shell)
- [Thrift0.4%](https://github.com/apache/doris/search?l=thrift)
- [C0.4%](https://github.com/apache/doris/search?l=c)
- Other1.1%

You can’t perform that action at this time.