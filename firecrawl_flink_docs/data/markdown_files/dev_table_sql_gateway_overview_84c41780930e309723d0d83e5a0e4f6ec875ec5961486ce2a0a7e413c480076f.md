## Introduction  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#introduction)

The SQL Gateway is a service that enables multiple clients from the remote to execute SQL in concurrency. It provides
an easy way to submit the Flink Job, look up the metadata, and analyze the data online.

The SQL Gateway is composed of pluggable endpoints and the `SqlGatewayService`. The `SqlGatewayService` is a processor that is
reused by the endpoints to handle the requests. The endpoint is an entry point that allows users to connect. Depending on the
type of the endpoints, users can use different utils to connect.

![SQL Gateway Architecture](https://nightlies.apache.org/flink/flink-docs-release-2.2/fig/sql-gateway-architecture.png)

## Getting Started  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#getting-started)

This section describes how to setup and run your first Flink SQL program from the command-line.

The SQL Gateway is bundled in the regular Flink distribution and thus runnable out-of-the-box. It requires only a running Flink cluster where table programs can be executed. For more information about setting up a Flink cluster see the [Cluster & Deployment](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/resource-providers/standalone/overview/) part. If you simply want to try out the SQL Gateway, you can also start a local cluster with one worker using the following command:

```bash
$ ./bin/start-cluster.sh
```

### Starting the SQL Gateway  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#starting-the-sql-gateway)

The SQL Gateway scripts are also located in the binary directory of Flink. Users can start by calling:

```bash
$ ./bin/sql-gateway.sh start -Dsql-gateway.endpoint.rest.address=localhost
```

The command starts the SQL Gateway with REST Endpoint that listens on the address localhost:8083. You can use the curl command to check
whether the REST Endpoint is available.

```bash
$ curl http://localhost:8083/v1/info
{"productName":"Apache Flink","version":"2.2.0"}
```

### Running SQL Queries  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#running-sql-queries)

For validating your setup and cluster connection, you can work with following steps.

**Step 1: Open a session**

```bash
$ curl --request POST http://localhost:8083/v1/sessions
{"sessionHandle":"..."}
```

The `sessionHandle` in the return results is used by the SQL Gateway to uniquely identify every active user.

**Step 2: Execute a query**

```bash
$ curl --request POST http://localhost:8083/v1/sessions/${sessionHandle}/statements/ --data '{"statement": "SELECT 1"}'
{"operationHandle":"..."}
```

The `operationHandle` in the return results is used by the SQL Gateway to uniquely identify the submitted SQL.

The Flink SQL Gateway allows clients to specify which Flink cluster to submit jobs to, enabling remote execution of SQL statements and facilitating easier interaction with Flink clusters through a REST API. Enrich the POST request body with [rest.address](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#rest-address) and [rest.port](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#rest-port) inside the `executionConfig` variable to set the Flink cluster address. For example:

```bash
$ curl --request POST http://localhost:8083/v1/sessions/${sessionHandle}/statements/ --data '{"executionConfig": {"rest.address":"jobmanager-host", "rest.port":8081},"statement": "SELECT 1"}'
{"operationHandle":"..."}
```

**Step 3: Fetch results**

With the `sessionHandle` and `operationHandle` above, you can fetch the corresponding results.

```bash
$ curl --request GET http://localhost:8083/v1/sessions/${sessionHandle}/operations/${operationHandle}/result/0
{
  "results": {
    "columns": [\
      {\
        "name": "EXPR$0",\
        "logicalType": {\
          "type": "INTEGER",\
          "nullable": false\
        }\
      }\
    ],
    "data": [\
      {\
        "kind": "INSERT",\
        "fields": [\
          1\
        ]\
      }\
    ]
  },
  "resultType": "PAYLOAD",
  "nextResultUri": "..."
}
```

The `nextResultUri` in the results is used to fetch the next batch results if it is not `null`.

```bash
$ curl --request GET ${nextResultUri}
```

### Deploying a Script  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#deploying-a-script)

SQL Gateway supports deploying a script in [Application Mode](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/overview/). In application mode, [JobManager](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/concepts/flink-architecture/#jobmanager) is responsible for compiling the script.
If you want to use custom resources in the script, e.g. Kafka Source, please use [ADD JAR](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/sql/jar/) command to download the [required artifacts](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/configuration/connector/#available-artifacts).

Here is an example for deploying a script to a Flink native K8S Cluster with cluster id `CLUSTER_ID`.

```bash
$ curl --request POST http://localhost:8083/sessions/${SESSION_HANDLE}/scripts \
--header 'Content-Type: application/json' \
--data-raw '{
    "script": "CREATE TEMPORARY TABLE sink(a INT) WITH ( '\''connector'\'' = '\''blackhole'\''); INSERT INTO sink VALUES (1), (2), (3);",
    "executionConfig": {
        "execution.target": "kubernetes-application",
        "kubernetes.cluster-id": "'${CLUSTER_ID}'",
        "kubernetes.container.image.ref": "'${FLINK_IMAGE_NAME}'",
        "jobmanager.memory.process.size": "1000m",
        "taskmanager.memory.process.size": "1000m",
        "kubernetes.jobmanager.cpu": 0.5,
        "kubernetes.taskmanager.cpu": 0.5,
        "kubernetes.rest-service.exposed.type": "NodePort"
    }
}'
```

Note If you want to run the script with PyFlink, please use an image with PyFlink installed. You can refer to
[Enabling PyFlink in docker](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/resource-providers/standalone/docker/#enabling-python) for more details.

## Configuration  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#configuration)

### SQL Gateway startup options  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-startup-options)

Currently, the SQL Gateway script has the following optional commands. They are discussed in details in the subsequent paragraphs.

```bash
$ ./bin/sql-gateway.sh --help

Usage: sql-gateway.sh [start|start-foreground|stop|stop-all] [args]
  commands:
    start               - Run a SQL Gateway as a daemon
    start-foreground    - Run a SQL Gateway as a console application
    stop                - Stop the SQL Gateway daemon
    stop-all            - Stop all the SQL Gateway daemons
    -h | --help         - Show this help message
```

For “start” or “start-foreground” command, you are able to configure the SQL Gateway in the CLI.

```bash
$ ./bin/sql-gateway.sh start --help

Start the Flink SQL Gateway as a daemon to submit Flink SQL.

  Syntax: start [OPTIONS]
     -D <property=value>   Use value for given property
     -h,--help             Show the help message with descriptions of all
                           options.
```

### SQL Gateway Configuration  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-configuration)

You can configure the SQL Gateway when starting the SQL Gateway below, or any valid [Flink configuration](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/config/) entry:

```bash
$ ./sql-gateway -Dkey=value
```

| Key | Default | Type | Description |
| --- | --- | --- | --- |
| ##### sql-gateway.session.check-interval [Anchor link for: sql gateway session check interval](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-session-check-interval) | 1 min | Duration | The check interval for idle session timeout, which can be disabled by setting to zero. |
| ##### sql-gateway.session.idle-timeout [Anchor link for: sql gateway session idle timeout](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-session-idle-timeout) | 10 min | Duration | Timeout interval for closing the session when the session hasn't been accessed during the interval. If setting to zero, the session will not be closed. |
| ##### sql-gateway.session.max-num [Anchor link for: sql gateway session max num](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-session-max-num) | 1000000 | Integer | The maximum number of the active session for sql gateway service. |
| ##### sql-gateway.session.plan-cache.enabled [Anchor link for: sql gateway session plan cache enabled](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-session-plan-cache-enabled) | false | Boolean | When it is true, sql gateway will cache and reuse plans for queries per session. |
| ##### sql-gateway.session.plan-cache.size [Anchor link for: sql gateway session plan cache size](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-session-plan-cache-size) | 100 | Integer | Plan cache size, it takes effect iff \`table.optimizer.plan-cache.enabled\` is true. |
| ##### sql-gateway.session.plan-cache.ttl [Anchor link for: sql gateway session plan cache ttl](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-session-plan-cache-ttl) | 1 hour | Duration | TTL for plan cache, it controls how long will the cache expire after write, it takes effect iff \`table.optimizer.plan-cache.enabled\` is true. |
| ##### sql-gateway.worker.keepalive-time [Anchor link for: sql gateway worker keepalive time](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-worker-keepalive-time) | 5 min | Duration | Keepalive time for an idle worker thread. When the number of workers exceeds min workers, excessive threads are killed after this time interval. |
| ##### sql-gateway.worker.threads.max [Anchor link for: sql gateway worker threads max](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-worker-threads-max) | 500 | Integer | The maximum number of worker threads for sql gateway service. |
| ##### sql-gateway.worker.threads.min [Anchor link for: sql gateway worker threads min](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#sql-gateway-worker-threads-min) | 5 | Integer | The minimum number of worker threads for sql gateway service. |

## Supported Endpoints  [\#](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql-gateway/overview/\#supported-endpoints)

Flink natively supports [REST Endpoint](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/sql-gateway/rest/) and [HiveServer2 Endpoint](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/sql-gateway/hiveserver2/).
The SQL Gateway is bundled with the REST Endpoint by default. With the flexible architecture, users are able to start the SQL Gateway with the specified endpoints by calling

```bash
$ ./bin/sql-gateway.sh start -Dsql-gateway.endpoint.type=hiveserver2
```

or add the following config in the [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file):

```yaml
sql-gateway.endpoint.type: hiveserver2
```

> Notice: The CLI command has higher priority if [Flink configuration file](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/deployment/config/#flink-configuration-file) also contains the option `sql-gateway.endpoint.type`.

For the specific endpoint, please refer to the corresponding page.