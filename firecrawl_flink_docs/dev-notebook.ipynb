{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa795b2",
   "metadata": {},
   "source": [
    "# FireCrawl playpen\n",
    "\n",
    "This is a simple notebook to discover what the response of `Firecrawl`'s response object looks like...\n",
    "\n",
    "The documentation takes time... and I got a bit unpatient... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b7972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 19:54:00,136 - models.processdata.ResponseProcessor - DEBUG - Initializing ResponseProcessor\n"
     ]
    }
   ],
   "source": [
    "from firecrawl import Firecrawl\n",
    "import dotenv, os, ast, json\n",
    "import logging\n",
    "\n",
    "from models.processdata import ResponseProcessor\n",
    "# proc = ResponseProcessor(root_url=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\",log_level=logging.INFO)\n",
    "proc = ResponseProcessor(root_url=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\",log_level=logging.DEBUG)\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(\".env\"))\n",
    "firecrawl = Firecrawl(api_key=os.getenv('FIRECRAWL_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13291fa2",
   "metadata": {},
   "source": [
    "## /scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9461c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting scrape...\n",
      "\n",
      " Scrape finished...\n",
      "\n",
      " Writing to file...\n",
      "\n",
      " Scrape response:\n",
      "# Concepts  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Starting scrape...\")\n",
    "\n",
    "# Crawl with scrape options\n",
    "response = firecrawl.scrape(\n",
    "    url='https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
    "    wait_for=2000,\n",
    "    only_main_content=True,\n",
    "    formats=['markdown'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Scrape finished...\")\n",
    "\n",
    "print('\\n Writing to file...')\n",
    "with open(\"./flink_firecrawl_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.model_dump()['markdown'])\n",
    "\n",
    "print(\"\\n Scrape response:\")\n",
    "print(response.model_dump()['markdown'][:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde48fa",
   "metadata": {},
   "source": [
    "This prints the markdown content of the scraped page. I.e. it works!!! YES!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7379d5",
   "metadata": {},
   "source": [
    "## /response_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eec95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/flink_firecrawl_markdown.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "md_content = '\\n'.join(lines)\n",
    "\n",
    "with open('./data/flink_firecrawl_response_full.txt', 'r', encoding='utf-8') as f:\n",
    "    full_content = f.read()\n",
    "\n",
    "file_response = ast.literal_eval(full_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc90181",
   "metadata": {},
   "source": [
    "# Metadata extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab8d71",
   "metadata": {},
   "source": [
    "## Datamodel\n",
    "\n",
    "In this part we are describing the data that needs to be saved from the scraping per page.\n",
    "\n",
    "1. Main content into `.md`-file:\n",
    "    1. File name = `<prefix>_<page_id>.md`\n",
    "        1. `<prefix>` = url - `<https://../docs/>`\n",
    "        2. `<page_id>` = hash of `<prefix>`\n",
    "2. Meta-data:\n",
    "    1. page_id: hash\n",
    "    2. title: str\n",
    "    3. url: str\n",
    "    4. parent_url: str\n",
    "    5. is_root_url: bool\n",
    "    6. child_urls (a list of tuples for ('link_text','link_url')): list[(str,str)]\n",
    "    7. scrape_timestamp: timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8032c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 18:17:10,767 - models.processdata.ResponseProcessor - INFO - parse_raw_response called\n",
      "2026-02-04 18:17:10,770 - models.processdata.ResponseProcessor - INFO - extract_summaries_with_ollama called\n",
      "2026-02-04 18:17:21,554 - models.processdata.ResponseProcessor - INFO - Saved markdown file\n",
      "2026-02-04 18:17:21,555 - models.processdata.ResponseProcessor - INFO - process_response completed\n"
     ]
    }
   ],
   "source": [
    "processed = proc.process_response(file_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ba0e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< PageMetadata\n",
       "    page_id=d699b5373c84d3776703d9c89d472a1ecee196e604219eb74f8e5647e6a4513c,\n",
       "    prefix=concepts_overview,\n",
       "    url=https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview,\n",
       "    title=Overview | Apache Flink,\n",
       "    version=flink-docs-release-1.20,\n",
       "    slug=flink,\n",
       "    summary=\"Exploring Flink's Streaming APIs for stateful and timely processing\",\n",
       "    headings[2]=\n",
       "      -->  1: Concepts\n",
       "      -->  2: Flink’s APIs,\n",
       "    is_root_url=True,\n",
       "    parent_url=None,\n",
       "    child_urls[7]=\n",
       "      -->  Handson Training (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview)\n",
       "      -->  Data Pipelines ETL (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl)\n",
       "      -->  Fault Tolerance (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance)\n",
       "      -->  Streaming Analytics (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics)\n",
       "      -->  DataStream API (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview)\n",
       "      -->  Process Function (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function)\n",
       "      -->  Table API (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview),\n",
       "    scrape_timestamp=2026-02-04 18:17:21.554192\n",
       " >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d960b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/markdown_files/concepts_stateful_stream_processing_2242824968fe3664ac00b3506911daf8e28b527e9f76a7f85c2c04e20e9ff783.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "concepts_content = '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c929dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This documentation is for an out-of-date version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/concepts/stateful-stream-processing/).\n",
      "\n",
      "\n",
      "\n",
      "# Stateful Stream Processing  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#stateful-stream-processing)\n",
      "\n",
      "\n",
      "\n",
      "## What is State?  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-pr\n"
     ]
    }
   ],
   "source": [
    "print(concepts_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea5f0d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 19:54:11,513 - models.processdata.ResponseProcessor - INFO - extract_summaries_with_ollama called\n",
      "2026-02-04 19:54:11,519 - models.processdata.ResponseProcessor - DEBUG - Requesting slug from Ollama\n",
      "2026-02-04 19:54:11,520 - models.processdata.ResponseProcessor - DEBUG - Requesting slug prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, write a specific 'slug' from the page.\n",
      "A 'slug' is a single-word, lowercase identifier (no spaces) that will specifically summarize the page.\n",
      "Only respond with this 'slug'.\n",
      "\n",
      "MARKDOWN:\n",
      "> This documentation is for an out-of-date version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/concepts/stateful-stream-processing/).\n",
      "\n",
      "\n",
      "\n",
      "# Stateful Stream Processing  [\\#](htt' \n",
      "...\n",
      "2026-02-04 19:54:11,521 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for slug, attempt 1/3\n",
      "2026-02-04 19:54:45,065 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for slug succeeded\n",
      "2026-02-04 19:54:45,065 - models.processdata.ResponseProcessor - DEBUG - Response [length=2017]: \n",
      " 'Here's a summary of the provided text:\n",
      "\n",
      "**State and Fault Tolerance**\n",
      "\n",
      "* Flink provides state management for both streaming and batch programs.\n",
      "* The `ExecutionMode` enum is used to specify the execution mode: `BATCH`, `STREAMING`, or `DEFAULT`.\n",
      "* In BATCH execution mode, streams are bounded, which simplifies fault tolerance but increases recovery costs.\n",
      "\n",
      "**Checkpointing**\n",
      "\n",
      "* Checkpointing involves taking a snapshot of the program's state.\n",
      "* There are two types of checkpointing:\n",
      "\t+ **Aligned Checkpointing**: This is the default behavior, where checkpoints are aligned with the end of an operator's execution.\n",
      "\t+ **Unaligned Checkpointing**: In this mode, operators keep processing all inputs after receiving a checkpoint barrier. This reduces latency but increases recovery costs.\n",
      "\n",
      "**Savepoints**\n",
      "\n",
      "* Savepoints are manually triggered checkpoints that allow users to update their programs and Flink cluster without losing state.\n",
      "* Unlike regular checkpoints, savepoints do not automatically expire when newer checkpoints are completed.\n",
      "\n",
      "**State Backends**\n",
      "\n",
      "* State backends provide a way to store the program's state in a key-value index or simplified data structure.\n",
      "* The choice of state backend depends on the application requirements and performance characteristics.\n",
      "\n",
      "**Fault Tolerance**\n",
      "\n",
      "* Flink provides fault tolerance mechanisms for both streaming and batch programs.\n",
      "* In BATCH execution mode, fault tolerance relies on fully replaying the streams instead of checkpointing.\n",
      "\n",
      "**State Management**\n",
      "\n",
      "* Flink manages state for both streaming and batch programs using a similar architecture.\n",
      "* The state management system includes checkpoints, savepoints, and state backends to ensure data consistency and availability.\n",
      "\n",
      "Overall, Flink provides a robust state management system that supports both streaming and batch programs. Understanding the differences between checkpointing, savepoints, and state backends is crucial for designing efficient and scalable state management solutions in Flink applications.' \n",
      "...\n",
      "2026-02-04 19:54:45,066 - models.processdata.ResponseProcessor - DEBUG - Received slug response from Ollama\n",
      "2026-02-04 19:54:45,067 - models.processdata.ResponseProcessor - DEBUG - Requesting summary from Ollama\n",
      "2026-02-04 19:54:45,067 - models.processdata.ResponseProcessor - DEBUG - Requesting summary prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, create a specific 'summary' that identifies the page.\n",
      "In this case a 'summary' is a concise specific sentence that identifies the page, and is only around 100 characters long.\n",
      "Only respond with this 'summary'.\n",
      "\n",
      "MARKDOWN:\n",
      "> This documentation is for an out-of-date version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/concepts/stateful-stream-processing/).\n",
      "\n",
      "\n",
      "\n",
      "#' \n",
      "...\n",
      "2026-02-04 19:54:45,068 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for summary, attempt 1/3\n",
      "2026-02-04 19:55:20,279 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for summary succeeded\n",
      "2026-02-04 19:55:20,279 - models.processdata.ResponseProcessor - DEBUG - Response [length=2162]: \n",
      " 'Flink provides a robust state management system that allows for fault-tolerant processing of data streams and batch jobs. Here's an overview of the key concepts:\n",
      "\n",
      "**State Management**\n",
      "\n",
      "* Flink uses a concept called \"state\" to store data that is processed by the application.\n",
      "* State can be thought of as a collection of values that are accessed and updated during the execution of the job.\n",
      "\n",
      "**Checkpointing**\n",
      "\n",
      "* Checkpointing is a mechanism used to save the state of an application at regular intervals.\n",
      "* When checkpointing, Flink takes a snapshot of the current state and stores it in a persistent storage location (e.g., disk).\n",
      "* This allows for fault-tolerant recovery in case of job failures or network partitions.\n",
      "\n",
      "**Savepoints**\n",
      "\n",
      "* Savepoints are manual checkpoints that can be triggered by the user.\n",
      "* Unlike regular checkpointing, savepoints do not automatically expire when newer checkpoints are completed.\n",
      "* Savepoints allow users to take control of their jobs' state and save it at specific points during execution.\n",
      "\n",
      "**State Backends**\n",
      "\n",
      "* Flink uses a variety of state backends to store data, including:\n",
      "\t+ In-memory hash maps\n",
      "\t+ RocksDB key-value stores\n",
      "\t+ HDFS (Hadoop Distributed File System)\n",
      "* The choice of state backend depends on the application's requirements and the amount of state being processed.\n",
      "\n",
      "**Exactly Once vs. At Least Once**\n",
      "\n",
      "* Flink provides two modes for handling data consistency:\n",
      "\t+ Exactly once: Ensures that each record is processed at most once, without duplication or omission.\n",
      "\t+ At least once: Guarantees that all records will be processed eventually, but may not ensure exactness.\n",
      "\n",
      "**Fault Tolerance in Batch Programs**\n",
      "\n",
      "* In batch programs, Flink uses a different approach for fault tolerance than in streaming programs:\n",
      "\t+ Instead of checkpointing and recovery, Flink fully replays the input streams during job execution.\n",
      "\t+ This method is more expensive in terms of processing time but ensures that the entire batch job can be executed reliably.\n",
      "\n",
      "Overall, Flink's state management system provides a robust and flexible framework for fault-tolerant data processing, with options for both streaming and batch programs.' \n",
      "...\n",
      "2026-02-04 19:55:20,280 - models.processdata.ResponseProcessor - DEBUG - Received summary response from Ollama\n",
      "2026-02-04 19:55:20,281 - models.processdata.ResponseProcessor - DEBUG - Requesting headings from Ollama\n",
      "2026-02-04 19:55:20,281 - models.processdata.ResponseProcessor - DEBUG - Requesting headings prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, extract all headings from the page.\n",
      "Each heading must be described by a:\n",
      " * 'level' - which is the index of the heading on the page (type=integer)\n",
      " * 'text' - the text of the heading (type=string)\n",
      "Example: {\"headings\":[{\"level\":1,\"text\":\"Concepts\"},{\"level\":2,\"text\":\"Flink’s APIs\"}]}\n",
      "Respond with a valid JSON payload providing the top-level 'headings' key, with it's list.\n",
      "ONLY provide JSON object, with no back-ticks and no further desc' \n",
      "...\n",
      "2026-02-04 19:55:20,282 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for headings, attempt 1/3\n",
      "2026-02-04 19:55:54,195 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for headings succeeded\n",
      "2026-02-04 19:55:54,195 - models.processdata.ResponseProcessor - DEBUG - Response [length=2163]: \n",
      " 'This text provides a comprehensive overview of Flink's state management and fault tolerance mechanisms, specifically for stream processing programs.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "1. **State**: Flink stores data in memory or on disk to support operations such as aggregation, filtering, and joining.\n",
      "2. **Checkpoints**: Regular snapshots of the program's state are taken to ensure consistency and facilitate recovery.\n",
      "3. **Savepoints**: Manually triggered checkpoints that allow users to update their programs and cluster without losing any state.\n",
      "4. **Exactly Once vs. At Least Once**: Flink provides guarantees for data consistency, with exactly once guarantee available when alignment is enabled.\n",
      "\n",
      "**Execution Modes:**\n",
      "\n",
      "1. **Stream Processing**: Flink executes stream processing programs using the STREAM execution mode.\n",
      "2. **Batch Execution**: Flink executes batch programs as a special case of streaming programs in BATCH execution mode.\n",
      "\n",
      "**State and Fault Tolerance:**\n",
      "\n",
      "1. **Fault Tolerance for Batch Programs**: Flink's fault tolerance mechanism does not use checkpointing, instead relying on full replay of streams to recover from failures.\n",
      "2. **State Backend**: In batch execution mode, simplified in-memory/out-of-core data structures are used, rather than key/value indexes.\n",
      "\n",
      "**Recovery and Restore:**\n",
      "\n",
      "1. **Recovery**: Flink's recovery process involves fully replaying the input streams to restore the program's state.\n",
      "2. **Restore**: The restored program can be executed from a specific point in time, using checkpoints or savepoints as needed.\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "1. **Unaligned Checkpointing**: An optional feature that allows skipping stream alignment during checkpointing, which can improve performance but may introduce additional latency and data duplication.\n",
      "2. **Alignment**: Stream alignment is enabled by default for programs with multiple predecessors or senders, providing exactly once guarantees even in at least once mode.\n",
      "\n",
      "Overall, Flink's state management and fault tolerance mechanisms provide a robust and flexible way to manage data consistency and recover from failures, making it an attractive choice for stream processing applications.' \n",
      "...\n",
      "2026-02-04 19:55:54,196 - models.processdata.ResponseProcessor - DEBUG - Received headings response from Ollama\n",
      "2026-02-04 19:55:54,197 - models.processdata.ResponseProcessor - DEBUG - Response [length=2163]: \n",
      " 'This text provides a comprehensive overview of Flink's state management and fault tolerance mechanisms, specifically for stream processing programs.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "1. **State**: Flink stores data in memory or on disk to support operations such as aggregation, filtering, and joining.\n",
      "2. **Checkpoints**: Regular snapshots of the program's state are taken to ensure consistency and facilitate recovery.\n",
      "3. **Savepoints**: Manually triggered checkpoints that allow users to update their programs and cluster without losing any state.\n",
      "4. **Exactly Once vs. At Least Once**: Flink provides guarantees for data consistency, with exactly once guarantee available when alignment is enabled.\n",
      "\n",
      "**Execution Modes:**\n",
      "\n",
      "1. **Stream Processing**: Flink executes stream processing programs using the STREAM execution mode.\n",
      "2. **Batch Execution**: Flink executes batch programs as a special case of streaming programs in BATCH execution mode.\n",
      "\n",
      "**State and Fault Tolerance:**\n",
      "\n",
      "1. **Fault Tolerance for Batch Programs**: Flink's fault tolerance mechanism does not use checkpointing, instead relying on full replay of streams to recover from failures.\n",
      "2. **State Backend**: In batch execution mode, simplified in-memory/out-of-core data structures are used, rather than key/value indexes.\n",
      "\n",
      "**Recovery and Restore:**\n",
      "\n",
      "1. **Recovery**: Flink's recovery process involves fully replaying the input streams to restore the program's state.\n",
      "2. **Restore**: The restored program can be executed from a specific point in time, using checkpoints or savepoints as needed.\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "1. **Unaligned Checkpointing**: An optional feature that allows skipping stream alignment during checkpointing, which can improve performance but may introduce additional latency and data duplication.\n",
      "2. **Alignment**: Stream alignment is enabled by default for programs with multiple predecessors or senders, providing exactly once guarantees even in at least once mode.\n",
      "\n",
      "Overall, Flink's state management and fault tolerance mechanisms provide a robust and flexible way to manage data consistency and recover from failures, making it an attractive choice for stream processing applications.' \n",
      "...\n",
      "2026-02-04 19:55:54,198 - models.processdata.ResponseProcessor - DEBUG - Parsing headings JSON from Ollama\n",
      "2026-02-04 19:55:54,198 - models.processdata.ResponseProcessor - ERROR - Failed parsing headings JSON from Ollama, trying ast.literal_eval\n",
      "2026-02-04 19:55:54,199 - models.processdata.ResponseProcessor - ERROR - Failed to evaluate headings response\n",
      "2026-02-04 19:55:54,199 - models.processdata.ResponseProcessor - WARNING - Ollama returned invalid headings, falling back to regex extraction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'slug': \"Here's a summary of the provided text:\\n\\n**State and Fault Tolerance**\\n\\n* Flink provides state management for both streaming and batch programs.\\n* The `ExecutionMode` enum is used to specify the execution mode: `BATCH`, `STREAMING`, or `DEFAULT`.\\n* In BATCH execution mode, streams are bounded, which simplifies fault tolerance but increases recovery costs.\\n\\n**Checkpointing**\\n\\n* Checkpointing involves taking a snapshot of the program's state.\\n* There are two types of checkpointing:\\n\\t+ **Aligned Checkpointing**: This is the default behavior, where checkpoints are aligned with the end of an operator's execution.\\n\\t+ **Unaligned Checkpointing**: In this mode, operators keep processing all inputs after receiving a checkpoint barrier. This reduces latency but increases recovery costs.\\n\\n**Savepoints**\\n\\n* Savepoints are manually triggered checkpoints that allow users to update their programs and Flink cluster without losing state.\\n* Unlike regular checkpoints, savepoints do not automatically expire when newer checkpoints are completed.\\n\\n**State Backends**\\n\\n* State backends provide a way to store the program's state in a key-value index or simplified data structure.\\n* The choice of state backend depends on the application requirements and performance characteristics.\\n\\n**Fault Tolerance**\\n\\n* Flink provides fault tolerance mechanisms for both streaming and batch programs.\\n* In BATCH execution mode, fault tolerance relies on fully replaying the streams instead of checkpointing.\\n\\n**State Management**\\n\\n* Flink manages state for both streaming and batch programs using a similar architecture.\\n* The state management system includes checkpoints, savepoints, and state backends to ensure data consistency and availability.\\n\\nOverall, Flink provides a robust state management system that supports both streaming and batch programs. Understanding the differences between checkpointing, savepoints, and state backends is crucial for designing efficient and scalable state management solutions in Flink applications.\",\n",
       " 'summary': 'Flink provides a robust state management system that allows for fault-tolerant processing of data streams and batch jobs. Here\\'s an overview of the key concepts:\\n\\n**State Management**\\n\\n* Flink uses a concept called \"state\" to store data that is processed by the application.\\n* State can be thought of as a collection of values that are accessed and updated during the execution of the job.\\n\\n**Checkpointing**\\n\\n* Checkpointing is a mechanism used to save the state of an application at regular intervals.\\n* When checkpointing, Flink takes a snapshot of the current state and stores it in a persistent storage location (e.g., disk).\\n* This allows for fault-tolerant recovery in case of job failures or network partitions.\\n\\n**Savepoints**\\n\\n* Savepoints are manual checkpoints that can be triggered by the user.\\n* Unlike regular checkpointing, savepoints do not automatically expire when newer checkpoints are completed.\\n* Savepoints allow users to take control of their jobs\\' state and save it at specific points during execution.\\n\\n**State Backends**\\n\\n* Flink uses a variety of state backends to store data, including:\\n\\t+ In-memory hash maps\\n\\t+ RocksDB key-value stores\\n\\t+ HDFS (Hadoop Distributed File System)\\n* The choice of state backend depends on the application\\'s requirements and the amount of state being processed.\\n\\n**Exactly Once vs. At Least Once**\\n\\n* Flink provides two modes for handling data consistency:\\n\\t+ Exactly once: Ensures that each record is processed at most once, without duplication or omission.\\n\\t+ At least once: Guarantees that all records will be processed eventually, but may not ensure exactness.\\n\\n**Fault Tolerance in Batch Programs**\\n\\n* In batch programs, Flink uses a different approach for fault tolerance than in streaming programs:\\n\\t+ Instead of checkpointing and recovery, Flink fully replays the input streams during job execution.\\n\\t+ This method is more expensive in terms of processing time but ensures that the entire batch job can be executed reliably.\\n\\nOverall, Flink\\'s state management system provides a robust and flexible framework for fault-tolerant data processing, with options for both streaming and batch programs.',\n",
       " 'headings': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc.extract_summaries_with_ollama(concepts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11ee20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a4f8ccb",
   "metadata": {},
   "source": [
    "## /NEXT-STEPS\n",
    "\n",
    "* [x] Limit scraping to specific domain\n",
    "* [x] Allow pruning of child urls depending on limited domain\n",
    "* [ ] Allow DB updates to existing records with LLM calls\n",
    "* [ ] Check each db records child urls to only include existing urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1651a0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
