{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa795b2",
   "metadata": {},
   "source": [
    "# FireCrawl playpen\n",
    "\n",
    "This is a simple notebook to discover what the response of `Firecrawl`'s response object looks like...\n",
    "\n",
    "The documentation takes time... and I got a bit unpatient... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b7972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 21:39:46,033 - models.processdata.ResponseProcessor - DEBUG - Initializing ResponseProcessor\n"
     ]
    }
   ],
   "source": [
    "from firecrawl import Firecrawl\n",
    "import dotenv, os, ast, json\n",
    "import logging\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(\".env\"))\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "from google import genai\n",
    "gemini = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "\n",
    "\n",
    "from models.processdata import ResponseProcessor\n",
    "# proc = ResponseProcessor(root_url=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\",log_level=logging.INFO)\n",
    "proc = ResponseProcessor(root_url=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\",log_level=logging.DEBUG)\n",
    "\n",
    "firecrawl = Firecrawl(api_key=os.getenv('FIRECRAWL_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13291fa2",
   "metadata": {},
   "source": [
    "## /scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9461c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting scrape...\n",
      "\n",
      " Scrape finished...\n",
      "\n",
      " Writing to file...\n",
      "\n",
      " Scrape response:\n",
      "# Concepts  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Starting scrape...\")\n",
    "\n",
    "# Crawl with scrape options\n",
    "response = firecrawl.scrape(\n",
    "    url='https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/',\n",
    "    wait_for=2000,\n",
    "    only_main_content=True,\n",
    "    formats=['markdown'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Scrape finished...\")\n",
    "\n",
    "print('\\n Writing to file...')\n",
    "with open(\"./flink_firecrawl_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.model_dump()['markdown'])\n",
    "\n",
    "print(\"\\n Scrape response:\")\n",
    "print(response.model_dump()['markdown'][:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde48fa",
   "metadata": {},
   "source": [
    "This prints the markdown content of the scraped page. I.e. it works!!! YES!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7379d5",
   "metadata": {},
   "source": [
    "## /response_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eec95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/flink_firecrawl_markdown.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "md_content = '\\n'.join(lines)\n",
    "\n",
    "with open('./data/flink_firecrawl_response_full.txt', 'r', encoding='utf-8') as f:\n",
    "    full_content = f.read()\n",
    "\n",
    "file_response = ast.literal_eval(full_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc90181",
   "metadata": {},
   "source": [
    "# Metadata extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab8d71",
   "metadata": {},
   "source": [
    "## Datamodel\n",
    "\n",
    "In this part we are describing the data that needs to be saved from the scraping per page.\n",
    "\n",
    "1. Main content into `.md`-file:\n",
    "    1. File name = `<prefix>_<page_id>.md`\n",
    "        1. `<prefix>` = url - `<https://../docs/>`\n",
    "        2. `<page_id>` = hash of `<prefix>`\n",
    "2. Meta-data:\n",
    "    1. page_id: hash\n",
    "    2. title: str\n",
    "    3. url: str\n",
    "    4. parent_url: str\n",
    "    5. is_root_url: bool\n",
    "    6. child_urls (a list of tuples for ('link_text','link_url')): list[(str,str)]\n",
    "    7. scrape_timestamp: timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8032c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:20:02,832 - models.processdata.ResponseProcessor - INFO - parse_raw_response called\n",
      "2026-02-08 15:20:02,833 - models.processdata.ResponseProcessor - DEBUG - parse_raw_response called\n",
      "2026-02-08 15:20:02,834 - models.processdata.ResponseProcessor - DEBUG - content_to_hash called\n",
      "2026-02-08 15:20:02,834 - models.processdata.ResponseProcessor - DEBUG - content_to_hash result\n",
      "2026-02-08 15:20:02,835 - models.processdata.ResponseProcessor - DEBUG - extract_version called\n",
      "2026-02-08 15:20:02,836 - models.processdata.ResponseProcessor - DEBUG - extract_version result\n",
      "2026-02-08 15:20:02,837 - models.processdata.ResponseProcessor - DEBUG - extract_prefix called\n",
      "2026-02-08 15:20:02,838 - models.processdata.ResponseProcessor - DEBUG - extract_prefix result\n",
      "2026-02-08 15:20:02,839 - models.processdata.ResponseProcessor - DEBUG - prefix_to_hash called\n",
      "2026-02-08 15:20:02,839 - models.processdata.ResponseProcessor - DEBUG - prefix_to_hash result\n",
      "2026-02-08 15:20:02,841 - models.processdata.ResponseProcessor - DEBUG - extract_markdown_links found matches\n",
      "2026-02-08 15:20:02,842 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,844 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,845 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,845 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,846 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,848 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,848 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,849 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,850 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,851 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,852 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,852 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,853 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,854 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,854 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,855 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,856 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,857 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,857 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,858 - models.processdata.ResponseProcessor - DEBUG - _normalize_url\n",
      "2026-02-08 15:20:02,858 - models.processdata.ResponseProcessor - DEBUG - extract_markdown_links result\n",
      "2026-02-08 15:20:02,859 - models.processdata.ResponseProcessor - INFO - extract_summaries_with_ollama called\n",
      "2026-02-08 15:20:02,861 - models.processdata.ResponseProcessor - DEBUG - Requesting slug from Ollama\n",
      "2026-02-08 15:20:02,862 - models.processdata.ResponseProcessor - DEBUG - Requesting slug prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, write a specific 'slug' from the page.\n",
      "A 'slug' is a single-word, lowercase identifier (no spaces) that will specifically summarize the page.\n",
      "Only respond with this 'slug'.\n",
      "\n",
      "MARKDOWN:\n",
      "# Concepts  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\#concepts)\n",
      "\n",
      "The [Hands-on Training](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview/) explains the basic concepts\n",
      "of s' \n",
      "...\n",
      "2026-02-08 15:20:02,862 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for slug, attempt 1/3\n",
      "2026-02-08 15:20:08,108 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for slug succeeded\n",
      "2026-02-08 15:20:08,109 - models.processdata.ResponseProcessor - DEBUG - Response [length=8]: \n",
      " 'concepts' \n",
      "...\n",
      "2026-02-08 15:20:08,109 - models.processdata.ResponseProcessor - DEBUG - Received slug response from Ollama\n",
      "2026-02-08 15:20:08,110 - models.processdata.ResponseProcessor - DEBUG - Response length: 8:\n",
      " concepts...\n",
      "2026-02-08 15:20:08,110 - models.processdata.ResponseProcessor - DEBUG - Requesting summary from Ollama\n",
      "2026-02-08 15:20:08,111 - models.processdata.ResponseProcessor - DEBUG - Requesting summary prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, create a specific 'summary' that identifies the page.\n",
      "In this case a 'summary' is a concise specific sentence that identifies the page, and is only around 100 characters long.\n",
      "Only respond with this 'summary'.\n",
      "\n",
      "MARKDOWN:\n",
      "# Concepts  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\#concepts)\n",
      "\n",
      "The [Hands-on Training](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overvi' \n",
      "...\n",
      "2026-02-08 15:20:08,111 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for summary, attempt 1/3\n",
      "2026-02-08 15:20:11,548 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for summary succeeded\n",
      "2026-02-08 15:20:11,548 - models.processdata.ResponseProcessor - DEBUG - Response [length=61]: \n",
      " '\"Understanding Flink's APIs for Streaming/Batch Applications\"' \n",
      "...\n",
      "2026-02-08 15:20:11,549 - models.processdata.ResponseProcessor - DEBUG - Received summary response from Ollama\n",
      "2026-02-08 15:20:11,549 - models.processdata.ResponseProcessor - DEBUG - Response length: 61:\n",
      " \"Understanding Flink's APIs for Streaming/Batch Applications\"...\n",
      "2026-02-08 15:20:11,550 - models.processdata.ResponseProcessor - DEBUG - Requesting headings from Ollama\n",
      "2026-02-08 15:20:11,550 - models.processdata.ResponseProcessor - DEBUG - Requesting headings prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, extract all headings from the page.\n",
      "Each heading must be described by a:\n",
      " * 'level' - which is the index of the heading on the page (type=integer)\n",
      " * 'text' - the text of the heading (type=string)\n",
      "Example: {\"headings\":[{\"level\":1,\"text\":\"Concepts\"},{\"level\":2,\"text\":\"Flink’s APIs\"}]}\n",
      "Respond with a valid JSON payload providing the top-level 'headings' key, with it's list.\n",
      "ONLY provide JSON object, with no back-ticks and no further desc' \n",
      "...\n",
      "2026-02-08 15:20:11,550 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for headings, attempt 1/3\n",
      "2026-02-08 15:20:15,735 - models.processdata.ResponseProcessor - DEBUG - Ollama API call for headings succeeded\n",
      "2026-02-08 15:20:15,736 - models.processdata.ResponseProcessor - DEBUG - Response [length=78]: \n",
      " '{\"headings\":[{\"level\":1,\"text\":\"Concepts\"},{\"level\":2,\"text\":\"Flink’s APIs\"}]}' \n",
      "...\n",
      "2026-02-08 15:20:15,736 - models.processdata.ResponseProcessor - DEBUG - Received headings response from Ollama\n",
      "2026-02-08 15:20:15,737 - models.processdata.ResponseProcessor - DEBUG - Response [length=78]: \n",
      " '{\"headings\":[{\"level\":1,\"text\":\"Concepts\"},{\"level\":2,\"text\":\"Flink’s APIs\"}]}' \n",
      "...\n",
      "2026-02-08 15:20:15,737 - models.processdata.ResponseProcessor - DEBUG - Parsing headings JSON from Ollama\n",
      "2026-02-08 15:20:15,743 - models.processdata.ResponseProcessor - INFO - Saved markdown file\n",
      "2026-02-08 15:20:15,743 - models.processdata.ResponseProcessor - INFO - process_response completed\n"
     ]
    }
   ],
   "source": [
    "processed = proc.process_response(file_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34ba0e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< PageMetadata\n",
       "    page_id=d699b5373c84d3776703d9c89d472a1ecee196e604219eb74f8e5647e6a4513c,\n",
       "    prefix=concepts_overview,\n",
       "    url=https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview,\n",
       "    title=Overview | Apache Flink,\n",
       "    version=flink-docs-release-1.20,\n",
       "    slug=concepts,\n",
       "    summary=\"Understanding Flink's APIs for Streaming/Batch Applications\",\n",
       "    headings[2]=\n",
       "      -->  1: Concepts\n",
       "      -->  2: Flink’s APIs,\n",
       "    is_root_url=True,\n",
       "    parent_url=None,\n",
       "    child_urls[7]=\n",
       "      -->  Handson Training (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview)\n",
       "      -->  Data Pipelines ETL (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl)\n",
       "      -->  Fault Tolerance (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance)\n",
       "      -->  Streaming Analytics (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics)\n",
       "      -->  DataStream API (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview)\n",
       "      -->  Process Function (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function)\n",
       "      -->  Table API (https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview),\n",
       "    scrape_timestamp=2026-02-08 15:20:15.742214\n",
       " >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ce30d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Concepts  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\#concepts)\n",
      "\n",
      "\n",
      "\n",
      "The [Hands-on Training](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview/) explains the basic concepts\n",
      "\n",
      "of stateful and timely stream processing that underlie Flink’s APIs, and provides examples of how these mechanisms are used in applications. Stateful stream processing is introduced in the context of [Data Pipelines & ETL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl/#stateful-transformations)\n",
      "\n",
      "and is further developed in the section on [Fault Tolerance](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance/).\n",
      "\n",
      "Timely stream processing is introduced in the section on [Streaming Analytics](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics/).\n",
      "\n",
      "\n",
      "\n",
      "This _Concepts in Depth_ section provides a deeper understanding of how Flink’s architecture and runtime implement these concepts.\n",
      "\n",
      "\n",
      "\n",
      "## Flink’s APIs  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\#flinks-apis)\n",
      "\n",
      "\n",
      "\n",
      "Flink offers different levels of abstraction for developing streaming/batch applications.\n",
      "\n",
      "\n",
      "\n",
      "![Programming levels of abstraction](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/levels_of_abstraction.svg)\n",
      "\n",
      "\n",
      "\n",
      "- The lowest level abstraction simply offers **stateful and timely stream processing**. It is\n",
      "\n",
      "embedded into the [DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/) via the [Process\\\\\n",
      "\n",
      "Function](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function/). It allows\n",
      "\n",
      "users to freely process events from one or more streams, and provides consistent, fault tolerant\n",
      "\n",
      "_state_. In addition, users can register event time and processing time callbacks, allowing\n",
      "\n",
      "programs to realize sophisticated computations.\n",
      "\n",
      "\n",
      "\n",
      "- In practice, many applications do not need the low-level\n",
      "\n",
      "abstractions described above, and can instead program against the **Core APIs**: the\n",
      "\n",
      "[DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/)\n",
      "\n",
      "(bounded/unbounded streams). These fluent APIs offer the\n",
      "\n",
      "common building blocks for data processing, like various forms of\n",
      "\n",
      "user-specified transformations, joins, aggregations, windows, state, etc.\n",
      "\n",
      "Data types processed in these APIs are represented as classes in the\n",
      "\n",
      "respective programming languages.\n",
      "\n",
      "\n",
      "\n",
      "The low level _Process Function_ integrates with the _DataStream API_,\n",
      "\n",
      "making it possible to use the lower-level abstraction on an as-needed basis.\n",
      "\n",
      "The _DataSet API_ offers additional primitives on bounded data sets,\n",
      "\n",
      "like loops/iterations.\n",
      "\n",
      "\n",
      "\n",
      "- The **Table API** is a declarative DSL centered around _tables_, which may\n",
      "\n",
      "be dynamically changing tables (when representing streams). The [Table\\\\\n",
      "\n",
      "API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/) follows the\n",
      "\n",
      "(extended) relational model: Tables have a schema attached (similar to\n",
      "\n",
      "tables in relational databases) and the API offers comparable operations,\n",
      "\n",
      "such as select, project, join, group-by, aggregate, etc. Table API\n",
      "\n",
      "programs declaratively define _what logical operation should be done_\n",
      "\n",
      "rather than specifying exactly _how the code for the operation looks_.\n",
      "\n",
      "Though the Table API is extensible by various types of user-defined\n",
      "\n",
      "functions, it is less expressive than the _Core APIs_, and more concise to\n",
      "\n",
      "use (less code to write). In addition, Table API programs also go through\n",
      "\n",
      "an optimizer that applies optimization rules before execution.\n",
      "\n",
      "\n",
      "\n",
      "One can seamlessly convert between tables and _DataStream_/ _DataSet_,\n",
      "\n",
      "allowing programs to mix the _Table API_ with the _DataStream_ and\n",
      "\n",
      "_DataSet_ APIs.\n",
      "\n",
      "\n",
      "\n",
      "- The highest level abstraction offered by Flink is **SQL**. This abstraction\n",
      "\n",
      "is similar to the _Table API_ both in semantics and expressiveness, but\n",
      "\n",
      "represents programs as SQL query expressions. The [SQL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/#sql) abstraction closely interacts with the\n",
      "\n",
      "Table API, and SQL queries can be executed over tables defined in the\n",
      "\n",
      "_Table API_.\n"
     ]
    }
   ],
   "source": [
    "print(md_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d960b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/markdown_files/concepts_stateful_stream_processing_2242824968fe3664ac00b3506911daf8e28b527e9f76a7f85c2c04e20e9ff783.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "concepts_content = '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5f0d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 16:14:22,025 - models.processdata.ResponseProcessor - INFO - extract_summaries_with_ollama called\n",
      "2026-02-08 16:14:22,027 - models.processdata.ResponseProcessor - DEBUG - Requesting slug from Gemini\n",
      "2026-02-08 16:14:22,028 - models.processdata.ResponseProcessor - DEBUG - Requesting slug prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, write a specific 'slug' from the page.\n",
      "A 'slug' is a single-word, lowercase identifier (no spaces) that will specifically summarize the page.\n",
      "Only respond with this 'slug'.\n",
      "\n",
      "MARKDOWN:\n",
      "> This documentation is for an out-of-date version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/concepts/stateful-stream-processing/).\n",
      "\n",
      "\n",
      "\n",
      "# Stateful Stream Processing  [\\#](htt' \n",
      "...\n",
      "2026-02-08 16:14:22,028 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for slug, attempt 1/3\n",
      "2026-02-08 16:14:22,029 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:22,819 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:22,820 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:23,642 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:23,643 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for slug (attempt 1/3): HTTP Error 404: Not Found\n",
      "2026-02-08 16:14:28,644 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for slug, attempt 2/3\n",
      "2026-02-08 16:14:28,645 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:30,235 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:30,236 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:31,054 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:31,054 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for slug (attempt 2/3): HTTP Error 404: Not Found\n",
      "2026-02-08 16:14:36,055 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for slug, attempt 3/3\n",
      "2026-02-08 16:14:36,056 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:36,859 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:36,861 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:37,495 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:37,496 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for slug (attempt 3/3): HTTP Error 404: Not Found\n",
      "2026-02-08 16:14:37,496 - models.processdata.ResponseProcessor - DEBUG - Requesting summary from Gemini\n",
      "2026-02-08 16:14:37,497 - models.processdata.ResponseProcessor - DEBUG - Requesting summary prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, create a specific 'summary' that identifies the page.\n",
      "In this case a 'summary' is a concise specific sentence that identifies the page, and is only around 100 characters long.\n",
      "Only respond with this 'summary'.\n",
      "\n",
      "MARKDOWN:\n",
      "> This documentation is for an out-of-date version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/concepts/stateful-stream-processing/).\n",
      "\n",
      "\n",
      "\n",
      "#' \n",
      "...\n",
      "2026-02-08 16:14:37,497 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for summary, attempt 1/3\n",
      "2026-02-08 16:14:37,498 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:39,145 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:39,146 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:39,977 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:39,978 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for summary (attempt 1/3): HTTP Error 404: Not Found\n",
      "2026-02-08 16:14:44,979 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for summary, attempt 2/3\n",
      "2026-02-08 16:14:44,980 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:45,801 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:45,802 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:47,280 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:47,281 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for summary (attempt 2/3): HTTP Error 404: Not Found\n",
      "2026-02-08 16:14:52,281 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for summary, attempt 3/3\n",
      "2026-02-08 16:14:52,282 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:52,927 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:52,928 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:54,402 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:54,402 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for summary (attempt 3/3): HTTP Error 404: Not Found\n",
      "2026-02-08 16:14:54,403 - models.processdata.ResponseProcessor - DEBUG - Requesting headings from Gemini\n",
      "2026-02-08 16:14:54,403 - models.processdata.ResponseProcessor - DEBUG - Requesting headings prompt: \n",
      " 'You are senior copy writer. Given the full markdown content, extract all headings from the page.\n",
      "Each heading must be described by a:\n",
      " * 'level' - which is the index of the heading on the page (type=integer)\n",
      " * 'text' - the text of the heading (type=string)\n",
      "Example: {\"headings\":[{\"level\":1,\"text\":\"Concepts\"},{\"level\":2,\"text\":\"Flink’s APIs\"}]}\n",
      "Respond with a valid JSON payload providing the top-level 'headings' key, with it's list.\n",
      "ONLY provide JSON object, with no back-ticks and no further desc' \n",
      "...\n",
      "2026-02-08 16:14:54,404 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for headings, attempt 1/3\n",
      "2026-02-08 16:14:54,404 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:55,918 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:55,919 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:14:56,684 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:14:56,685 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for headings (attempt 1/3): HTTP Error 404: Not Found\n",
      "2026-02-08 16:15:01,686 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for headings, attempt 2/3\n",
      "2026-02-08 16:15:01,687 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:15:02,387 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:15:02,387 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:15:03,335 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:15:03,337 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for headings (attempt 2/3): HTTP Error 404: Not Found\n",
      "2026-02-08 16:15:08,339 - models.processdata.ResponseProcessor - DEBUG - Gemini API call for headings, attempt 3/3\n",
      "2026-02-08 16:15:08,340 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:15:09,882 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:15:09,882 - models.processdata.ResponseProcessor - DEBUG - Attempting Gemini endpoint\n",
      "2026-02-08 16:15:10,544 - models.processdata.ResponseProcessor - WARNING - Gemini HTTP error on endpoint\n",
      "2026-02-08 16:15:10,544 - models.processdata.ResponseProcessor - WARNING - Gemini API call failed for headings (attempt 3/3): HTTP Error 404: Not Found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'slug': '', 'summary': '', 'headings': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc.extract_summaries_with_ollama(concepts_content,timeout=180,provider='gemini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d11ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slug_prompt = (\n",
    "#             \"\\\"\\\"\\\"Your role: senior copy writer. \\n\"\n",
    "#             \"Task: Create a slug given the full markdown content.\\n\"\n",
    "#             \"Slug definition: A 'slug' is a single-word, lowercase identifier (no spaces) that will specifically summarize the page.\\n\"\n",
    "#             \"Response: 'slug' ONLY.\\n\\n\"\n",
    "#             \"MARKDOWN:\\n\" + f\"'''{concepts_content}'''\\\"\\\"\\\"\"\n",
    "#         )\n",
    "\n",
    "# slug_prompt = (\n",
    "#             \"\\\"\\\"\\\"You must output EXACTLY one word. Nothing else. \\n\"\n",
    "#             \"CONTENT:\\n\" + f\"'''{concepts_content}'''\\\"\\\"\\\"\"\n",
    "#         )\n",
    "\n",
    "\n",
    "slug_prompt = (\n",
    "            \"\\\"\\\"\\\"Task: Generate a slug for the content below. \\n\\n \" +\n",
    "            \"Rules: \\n\" +\n",
    "\" - Output ONLY ONE WORD \\n\" +\n",
    "\" - Must be lowercase \\n\" +\n",
    "\" - No spaces, no hyphens, no special characters \\n\" +\n",
    "\" - Do not explain, do not summarize \\n\" +\n",
    "\" - Just the single word slug \\n\\n\" +\n",
    "\"Example1: \\n\" +\n",
    "\"Content: \\n\" + f\"'''{md_content}'''\\n\" +\n",
    "\"Response: 'concepts' \\n\\n\" +\n",
    "            \"CONTENT:\\n\" + f\"'''{concepts_content}'''\\\"\\\"\\\"\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8821a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Task: Generate a slug for the content below. \n",
      "\n",
      " Rules: \n",
      " - Output ONLY ONE WORD \n",
      " - Must be lowercase \n",
      " - No spaces, no hyphens, no special characters \n",
      " - Do not explain, do not summarize \n",
      " - Just the single word slug \n",
      "\n",
      "Example1: \n",
      "Content: \n",
      "'''# Concepts  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\#concepts)\n",
      "\n",
      "\n",
      "\n",
      "The [Hands-on Training](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/overview/) explains the basic concepts\n",
      "\n",
      "of stateful and timely stream processing that underlie Flink’s APIs, and provides examples of how these mechanisms are used in applications. Stateful stream processing is introduced in the context of [Data Pipelines & ETL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/etl/#stateful-transformations)\n",
      "\n",
      "and is further developed in the section on [Fault Tolerance](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/fault_tolerance/).\n",
      "\n",
      "Timely stream processing is introduced in the section on [Streaming Analytics](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/learn-flink/streaming_analytics/).\n",
      "\n",
      "\n",
      "\n",
      "This _Concepts in Depth_ section provides a deeper understanding of how Flink’s architecture and runtime implement these concepts.\n",
      "\n",
      "\n",
      "\n",
      "## Flink’s APIs  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/overview/\\#flinks-apis)\n",
      "\n",
      "\n",
      "\n",
      "Flink offers different levels of abstraction for developing streaming/batch applications.\n",
      "\n",
      "\n",
      "\n",
      "![Programming levels of abstraction](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/levels_of_abstraction.svg)\n",
      "\n",
      "\n",
      "\n",
      "- The lowest level abstraction simply offers **stateful and timely stream processing**. It is\n",
      "\n",
      "embedded into the [DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/) via the [Process\\\\\n",
      "\n",
      "Function](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/operators/process_function/). It allows\n",
      "\n",
      "users to freely process events from one or more streams, and provides consistent, fault tolerant\n",
      "\n",
      "_state_. In addition, users can register event time and processing time callbacks, allowing\n",
      "\n",
      "programs to realize sophisticated computations.\n",
      "\n",
      "\n",
      "\n",
      "- In practice, many applications do not need the low-level\n",
      "\n",
      "abstractions described above, and can instead program against the **Core APIs**: the\n",
      "\n",
      "[DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/overview/)\n",
      "\n",
      "(bounded/unbounded streams). These fluent APIs offer the\n",
      "\n",
      "common building blocks for data processing, like various forms of\n",
      "\n",
      "user-specified transformations, joins, aggregations, windows, state, etc.\n",
      "\n",
      "Data types processed in these APIs are represented as classes in the\n",
      "\n",
      "respective programming languages.\n",
      "\n",
      "\n",
      "\n",
      "The low level _Process Function_ integrates with the _DataStream API_,\n",
      "\n",
      "making it possible to use the lower-level abstraction on an as-needed basis.\n",
      "\n",
      "The _DataSet API_ offers additional primitives on bounded data sets,\n",
      "\n",
      "like loops/iterations.\n",
      "\n",
      "\n",
      "\n",
      "- The **Table API** is a declarative DSL centered around _tables_, which may\n",
      "\n",
      "be dynamically changing tables (when representing streams). The [Table\\\\\n",
      "\n",
      "API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/) follows the\n",
      "\n",
      "(extended) relational model: Tables have a schema attached (similar to\n",
      "\n",
      "tables in relational databases) and the API offers comparable operations,\n",
      "\n",
      "such as select, project, join, group-by, aggregate, etc. Table API\n",
      "\n",
      "programs declaratively define _what logical operation should be done_\n",
      "\n",
      "rather than specifying exactly _how the code for the operation looks_.\n",
      "\n",
      "Though the Table API is extensible by various types of user-defined\n",
      "\n",
      "functions, it is less expressive than the _Core APIs_, and more concise to\n",
      "\n",
      "use (less code to write). In addition, Table API programs also go through\n",
      "\n",
      "an optimizer that applies optimization rules before execution.\n",
      "\n",
      "\n",
      "\n",
      "One can seamlessly convert between tables and _DataStream_/ _DataSet_,\n",
      "\n",
      "allowing programs to mix the _Table API_ with the _DataStream_ and\n",
      "\n",
      "_DataSet_ APIs.\n",
      "\n",
      "\n",
      "\n",
      "- The highest level abstraction offered by Flink is **SQL**. This abstraction\n",
      "\n",
      "is similar to the _Table API_ both in semantics and expressiveness, but\n",
      "\n",
      "represents programs as SQL query expressions. The [SQL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/overview/#sql) abstraction closely interacts with the\n",
      "\n",
      "Table API, and SQL queries can be executed over tables defined in the\n",
      "\n",
      "_Table API_.'''\n",
      "Response: 'concepts' \n",
      "\n",
      "CONTENT:\n",
      "'''> This documentation is for an out-of-date version of Apache Flink. We recommend you use the latest [stable version](https://nightlies.apache.org/flink/flink-docs-stable/docs/concepts/stateful-stream-processing/).\n",
      "\n",
      "\n",
      "\n",
      "# Stateful Stream Processing  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#stateful-stream-processing)\n",
      "\n",
      "\n",
      "\n",
      "## What is State?  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#what-is-state)\n",
      "\n",
      "\n",
      "\n",
      "While many operations in a dataflow simply look at one individual _event at a_\n",
      "\n",
      "_time_ (for example an event parser), some operations remember information\n",
      "\n",
      "across multiple events (for example window operators). These operations are\n",
      "\n",
      "called **stateful**.\n",
      "\n",
      "\n",
      "\n",
      "Some examples of stateful operations:\n",
      "\n",
      "\n",
      "\n",
      "- When an application searches for certain event patterns, the state will\n",
      "\n",
      "store the sequence of events encountered so far.\n",
      "\n",
      "- When aggregating events per minute/hour/day, the state holds the pending\n",
      "\n",
      "aggregates.\n",
      "\n",
      "- When training a machine learning model over a stream of data points, the\n",
      "\n",
      "state holds the current version of the model parameters.\n",
      "\n",
      "- When historic data needs to be managed, the state allows efficient access\n",
      "\n",
      "to events that occurred in the past.\n",
      "\n",
      "\n",
      "\n",
      "Flink needs to be aware of the state in order to make it fault tolerant using\n",
      "\n",
      "[checkpoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/checkpointing/)\n",
      "\n",
      "and [savepoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/savepoints/).\n",
      "\n",
      "\n",
      "\n",
      "Knowledge about the state also allows for rescaling Flink applications, meaning\n",
      "\n",
      "that Flink takes care of redistributing state across parallel instances.\n",
      "\n",
      "\n",
      "\n",
      "When working with state, it might also be useful to read about [Flink’s state\\\\\n",
      "\n",
      "backends](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/state_backends/). Flink\n",
      "\n",
      "provides different state backends that specify how and where state is stored.\n",
      "\n",
      "\n",
      "\n",
      "## Keyed State  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#keyed-state)\n",
      "\n",
      "\n",
      "\n",
      "Keyed state is maintained in what can be thought of as an embedded key/value\n",
      "\n",
      "store. The state is partitioned and distributed strictly together with the\n",
      "\n",
      "streams that are read by the stateful operators. Hence, access to the key/value\n",
      "\n",
      "state is only possible on _keyed streams_, i.e. after a keyed/partitioned data\n",
      "\n",
      "exchange, and is restricted to the values associated with the current event’s\n",
      "\n",
      "key. Aligning the keys of streams and state makes sure that all state updates\n",
      "\n",
      "are local operations, guaranteeing consistency without transaction overhead.\n",
      "\n",
      "This alignment also allows Flink to redistribute the state and adjust the\n",
      "\n",
      "stream partitioning transparently.\n",
      "\n",
      "\n",
      "\n",
      "![State and Partitioning](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/state_partitioning.svg)\n",
      "\n",
      "\n",
      "\n",
      "Keyed State is further organized into so-called _Key Groups_. Key Groups are\n",
      "\n",
      "the atomic unit by which Flink can redistribute Keyed State; there are exactly\n",
      "\n",
      "as many Key Groups as the defined maximum parallelism. During execution each\n",
      "\n",
      "parallel instance of a keyed operator works with the keys for one or more Key\n",
      "\n",
      "Groups.\n",
      "\n",
      "\n",
      "\n",
      "## State Persistence  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#state-persistence)\n",
      "\n",
      "\n",
      "\n",
      "Flink implements fault tolerance using a combination of **stream replay** and\n",
      "\n",
      "**checkpointing**. A checkpoint marks a specific point in each of the\n",
      "\n",
      "input streams along with the corresponding state for each of the operators. A\n",
      "\n",
      "streaming dataflow can be resumed from a checkpoint while maintaining\n",
      "\n",
      "consistency _(exactly-once processing semantics)_ by restoring the state of the\n",
      "\n",
      "operators and replaying the records from the point of the checkpoint.\n",
      "\n",
      "\n",
      "\n",
      "The checkpoint interval is a means of trading off the overhead of fault\n",
      "\n",
      "tolerance during execution with the recovery time (the number of records that\n",
      "\n",
      "need to be replayed).\n",
      "\n",
      "\n",
      "\n",
      "The fault tolerance mechanism continuously draws snapshots of the distributed\n",
      "\n",
      "streaming data flow. For streaming applications with small state, these\n",
      "\n",
      "snapshots are very light-weight and can be drawn frequently without much impact\n",
      "\n",
      "on performance. The state of the streaming applications is stored at a\n",
      "\n",
      "configurable place, usually in a distributed file system.\n",
      "\n",
      "\n",
      "\n",
      "In case of a program failure (due to machine-, network-, or software failure),\n",
      "\n",
      "Flink stops the distributed streaming dataflow. The system then restarts the\n",
      "\n",
      "operators and resets them to the latest successful checkpoint. The input\n",
      "\n",
      "streams are reset to the point of the state snapshot. Any records that are\n",
      "\n",
      "processed as part of the restarted parallel dataflow are guaranteed to not have\n",
      "\n",
      "affected the previously checkpointed state.\n",
      "\n",
      "\n",
      "\n",
      "> By default, checkpointing is disabled. See [Checkpointing](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/fault-tolerance/checkpointing/) for details on how to enable and configure checkpointing.\n",
      "\n",
      "\n",
      "\n",
      "> For this mechanism to realize its full guarantees, the data\n",
      "\n",
      "> stream source (such as message queue or broker) needs to be able to rewind the\n",
      "\n",
      "> stream to a defined recent point. [Apache Kafka](http://kafka.apache.org/) has\n",
      "\n",
      "> this ability and Flink’s connector to Kafka exploits this. See [Fault\\\\\n",
      "\n",
      "> Tolerance Guarantees of Data Sources and Sinks](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/guarantees/) for more information about the guarantees\n",
      "\n",
      "> provided by Flink’s connectors.\n",
      "\n",
      "\n",
      "\n",
      "> Because Flink’s checkpoints are realized through distributed\n",
      "\n",
      "> snapshots, we use the words _snapshot_ and _checkpoint_ interchangeably. Often\n",
      "\n",
      "> we also use the term _snapshot_ to mean either _checkpoint_ or _savepoint_.\n",
      "\n",
      "\n",
      "\n",
      "### Checkpointing  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#checkpointing)\n",
      "\n",
      "\n",
      "\n",
      "The central part of Flink’s fault tolerance mechanism is drawing consistent\n",
      "\n",
      "snapshots of the distributed data stream and operator state. These snapshots\n",
      "\n",
      "act as consistent checkpoints to which the system can fall back in case of a\n",
      "\n",
      "failure. Flink’s mechanism for drawing these snapshots is described in\n",
      "\n",
      "“ [Lightweight Asynchronous Snapshots for Distributed\\\\\n",
      "\n",
      "Dataflows](http://arxiv.org/abs/1506.08603)”. It is inspired by the standard\n",
      "\n",
      "[Chandy-Lamport algorithm](http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf)\n",
      "\n",
      "for distributed snapshots and is specifically tailored to Flink’s execution\n",
      "\n",
      "model.\n",
      "\n",
      "\n",
      "\n",
      "Keep in mind that everything to do with checkpointing can be done\n",
      "\n",
      "asynchronously. The checkpoint barriers don’t travel in lock step and\n",
      "\n",
      "operations can asynchronously snapshot their state.\n",
      "\n",
      "\n",
      "\n",
      "Since Flink 1.11, checkpoints can be taken with or without alignment. In this\n",
      "\n",
      "section, we describe aligned checkpoints first.\n",
      "\n",
      "\n",
      "\n",
      "#### Barriers  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#barriers)\n",
      "\n",
      "\n",
      "\n",
      "A core element in Flink’s distributed snapshotting are the _stream barriers_.\n",
      "\n",
      "These barriers are injected into the data stream and flow with the records as\n",
      "\n",
      "part of the data stream. Barriers never overtake records, they flow strictly in\n",
      "\n",
      "line. A barrier separates the records in the data stream into the set of\n",
      "\n",
      "records that goes into the current snapshot, and the records that go into the\n",
      "\n",
      "next snapshot. Each barrier carries the ID of the snapshot whose records it\n",
      "\n",
      "pushed in front of it. Barriers do not interrupt the flow of the stream and are\n",
      "\n",
      "hence very lightweight. Multiple barriers from different snapshots can be in\n",
      "\n",
      "the stream at the same time, which means that various snapshots may happen\n",
      "\n",
      "concurrently.\n",
      "\n",
      "\n",
      "\n",
      "![Checkpoint barriers in data streams](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/stream_barriers.svg)\n",
      "\n",
      "\n",
      "\n",
      "Stream barriers are injected into the parallel data flow at the stream sources.\n",
      "\n",
      "The point where the barriers for snapshot _n_ are injected (let’s call it\n",
      "\n",
      "_Sn_) is the position in the source stream up to which the\n",
      "\n",
      "snapshot covers the data. For example, in Apache Kafka, this position would be\n",
      "\n",
      "the last record’s offset in the partition. This position _Sn_\n",
      "\n",
      "is reported to the _checkpoint coordinator_ (Flink’s JobManager).\n",
      "\n",
      "\n",
      "\n",
      "The barriers then flow downstream. When an intermediate operator has received a\n",
      "\n",
      "barrier for snapshot _n_ from all of its input streams, it emits a barrier for\n",
      "\n",
      "snapshot _n_ into all of its outgoing streams. Once a sink operator (the end of\n",
      "\n",
      "a streaming DAG) has received the barrier _n_ from all of its input streams, it\n",
      "\n",
      "acknowledges that snapshot _n_ to the checkpoint coordinator. After all sinks\n",
      "\n",
      "have acknowledged a snapshot, it is considered completed.\n",
      "\n",
      "\n",
      "\n",
      "Once snapshot _n_ has been completed, the job will never again ask the source\n",
      "\n",
      "for records from before _Sn_, since at that point these records\n",
      "\n",
      "(and their descendant records) will have passed through the entire data flow\n",
      "\n",
      "topology.\n",
      "\n",
      "\n",
      "\n",
      "![Aligning data streams at operators with multiple inputs](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/stream_aligning.svg)\n",
      "\n",
      "\n",
      "\n",
      "Operators that receive more than one input stream need to _align_ the input\n",
      "\n",
      "streams on the snapshot barriers. The figure above illustrates this:\n",
      "\n",
      "\n",
      "\n",
      "- As soon as the operator receives snapshot barrier _n_ from an incoming\n",
      "\n",
      "stream, it cannot process any further records from that stream until it has\n",
      "\n",
      "received the barrier _n_ from the other inputs as well. Otherwise, it would\n",
      "\n",
      "mix records that belong to snapshot _n_ and with records that belong to\n",
      "\n",
      "snapshot _n+1_.\n",
      "\n",
      "- Once the last stream has received barrier _n_, the operator emits all\n",
      "\n",
      "pending outgoing records, and then emits snapshot _n_ barriers itself.\n",
      "\n",
      "- It snapshots the state and resumes processing records from all input streams,\n",
      "\n",
      "processing records from the input buffers before processing the records\n",
      "\n",
      "from the streams.\n",
      "\n",
      "- Finally, the operator writes the state asynchronously to the state backend.\n",
      "\n",
      "\n",
      "\n",
      "Note that the alignment is needed for all operators with multiple inputs and for\n",
      "\n",
      "operators after a shuffle when they consume output streams of multiple upstream\n",
      "\n",
      "subtasks.\n",
      "\n",
      "\n",
      "\n",
      "#### Snapshotting Operator State  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#snapshotting-operator-state)\n",
      "\n",
      "\n",
      "\n",
      "When operators contain any form of _state_, this state must be part of the\n",
      "\n",
      "snapshots as well.\n",
      "\n",
      "\n",
      "\n",
      "Operators snapshot their state at the point in time when they have received all\n",
      "\n",
      "snapshot barriers from their input streams, and before emitting the barriers to\n",
      "\n",
      "their output streams. At that point, all updates to the state from records\n",
      "\n",
      "before the barriers have been made, and no updates that depend on records\n",
      "\n",
      "from after the barriers have been applied. Because the state of a snapshot may\n",
      "\n",
      "be large, it is stored in a configurable _[state backend](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/state_backends/)_. By default, this is the JobManager’s\n",
      "\n",
      "memory, but for production use a distributed reliable storage should be\n",
      "\n",
      "configured (such as HDFS). After the state has been stored, the operator\n",
      "\n",
      "acknowledges the checkpoint, emits the snapshot barrier into the output\n",
      "\n",
      "streams, and proceeds.\n",
      "\n",
      "\n",
      "\n",
      "The resulting snapshot now contains:\n",
      "\n",
      "\n",
      "\n",
      "- For each parallel stream data source, the offset/position in the stream\n",
      "\n",
      "when the snapshot was started\n",
      "\n",
      "- For each operator, a pointer to the state that was stored as part of the\n",
      "\n",
      "snapshot\n",
      "\n",
      "\n",
      "\n",
      "![Illustration of the Checkpointing Mechanism](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/checkpointing.svg)\n",
      "\n",
      "\n",
      "\n",
      "#### Recovery  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#recovery)\n",
      "\n",
      "\n",
      "\n",
      "Recovery under this mechanism is straightforward: Upon a failure, Flink selects\n",
      "\n",
      "the latest completed checkpoint _k_. The system then re-deploys the entire\n",
      "\n",
      "distributed dataflow, and gives each operator the state that was snapshotted as\n",
      "\n",
      "part of checkpoint _k_. The sources are set to start reading the stream from\n",
      "\n",
      "position _Sk_. For example in Apache Kafka, that means telling\n",
      "\n",
      "the consumer to start fetching from offset _Sk_.\n",
      "\n",
      "\n",
      "\n",
      "If state was snapshotted incrementally, the operators start with the state of\n",
      "\n",
      "the latest full snapshot and then apply a series of incremental snapshot\n",
      "\n",
      "updates to that state.\n",
      "\n",
      "\n",
      "\n",
      "See [Restart Strategies](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/task_failure_recovery/#restart-strategies) for more information.\n",
      "\n",
      "\n",
      "\n",
      "### Unaligned Checkpointing  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#unaligned-checkpointing)\n",
      "\n",
      "\n",
      "\n",
      "Checkpointing can also be performed unaligned.\n",
      "\n",
      "The basic idea is that checkpoints can overtake all in-flight data as long as\n",
      "\n",
      "the in-flight data becomes part of the operator state.\n",
      "\n",
      "\n",
      "\n",
      "Note that this approach is actually closer to the [Chandy-Lamport algorithm](http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf), but\n",
      "\n",
      "Flink still inserts the barrier in the sources to avoid overloading the\n",
      "\n",
      "checkpoint coordinator.\n",
      "\n",
      "\n",
      "\n",
      "![Unaligned checkpointing](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/stream_unaligning.svg)\n",
      "\n",
      "\n",
      "\n",
      "The figure depicts how an operator handles unaligned checkpoint barriers:\n",
      "\n",
      "\n",
      "\n",
      "- The operator reacts on the first barrier that is stored in its input buffers.\n",
      "\n",
      "- It immediately forwards the barrier to the downstream operator by adding it\n",
      "\n",
      "to the end of the output buffers.\n",
      "\n",
      "- The operator marks all overtaken records to be stored asynchronously and\n",
      "\n",
      "creates a snapshot of its own state.\n",
      "\n",
      "\n",
      "\n",
      "Consequently, the operator only briefly stops the processing of input to mark\n",
      "\n",
      "the buffers, forwards the barrier, and creates the snapshot of the other state.\n",
      "\n",
      "\n",
      "\n",
      "Unaligned checkpointing ensures that barriers are arriving at the sink as fast\n",
      "\n",
      "as possible. It’s especially suited for applications with at least one slow\n",
      "\n",
      "moving data path, where alignment times can reach hours. However, since it’s\n",
      "\n",
      "adding additional I/O pressure, it doesn’t help when the I/O to the state\n",
      "\n",
      "backends is the bottleneck. See the more in-depth discussion in\n",
      "\n",
      "[ops](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/checkpoints/#unaligned-checkpoints)\n",
      "\n",
      "for other limitations.\n",
      "\n",
      "\n",
      "\n",
      "Note that savepoints will always be aligned.\n",
      "\n",
      "\n",
      "\n",
      "#### Unaligned Recovery  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#unaligned-recovery)\n",
      "\n",
      "\n",
      "\n",
      "Operators first recover the in-flight data before starting processing any data\n",
      "\n",
      "from upstream operators in unaligned checkpointing. Aside from that, it\n",
      "\n",
      "performs the same steps as during [recovery of aligned checkpoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/#recovery).\n",
      "\n",
      "\n",
      "\n",
      "### State Backends  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#state-backends)\n",
      "\n",
      "\n",
      "\n",
      "The exact data structures in which the key/values indexes are stored depends on\n",
      "\n",
      "the chosen [state backend](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/state_backends/). One state backend stores data in an in-memory\n",
      "\n",
      "hash map, another state backend uses [RocksDB](http://rocksdb.org/) as the\n",
      "\n",
      "key/value store. In addition to defining the data structure that holds the\n",
      "\n",
      "state, the state backends also implement the logic to take a point-in-time\n",
      "\n",
      "snapshot of the key/value state and store that snapshot as part of a\n",
      "\n",
      "checkpoint. State backends can be configured without changing your application\n",
      "\n",
      "logic.\n",
      "\n",
      "\n",
      "\n",
      "![checkpoints and snapshots](https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/checkpoints.svg)\n",
      "\n",
      "\n",
      "\n",
      "### Savepoints  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#savepoints)\n",
      "\n",
      "\n",
      "\n",
      "All programs that use checkpointing can resume execution from a **savepoint**.\n",
      "\n",
      "Savepoints allow both updating your programs and your Flink cluster without\n",
      "\n",
      "losing any state.\n",
      "\n",
      "\n",
      "\n",
      "[Savepoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/savepoints/) are\n",
      "\n",
      "**manually triggered checkpoints**, which take a snapshot of the program and\n",
      "\n",
      "write it out to a state backend. They rely on the regular checkpointing\n",
      "\n",
      "mechanism for this.\n",
      "\n",
      "\n",
      "\n",
      "Savepoints are similar to checkpoints except that they are\n",
      "\n",
      "**triggered by the user** and **don’t automatically expire** when newer\n",
      "\n",
      "checkpoints are completed.\n",
      "\n",
      "To make proper use of savepoints, it’s important to understand the differences between\n",
      "\n",
      "[checkpoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/checkpoints/) and [savepoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/savepoints/)\n",
      "\n",
      "which is described in [checkpoints vs. savepoints](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/checkpoints_vs_savepoints/).\n",
      "\n",
      "\n",
      "\n",
      "### Exactly Once vs. At Least Once  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#exactly-once-vs-at-least-once)\n",
      "\n",
      "\n",
      "\n",
      "The alignment step may add latency to the streaming program. Usually, this\n",
      "\n",
      "extra latency is on the order of a few milliseconds, but we have seen cases\n",
      "\n",
      "where the latency of some outliers increased noticeably. For applications that\n",
      "\n",
      "require consistently super low latencies (few milliseconds) for all records,\n",
      "\n",
      "Flink has a switch to skip the stream alignment during a checkpoint. Checkpoint\n",
      "\n",
      "snapshots are still drawn as soon as an operator has seen the checkpoint\n",
      "\n",
      "barrier from each input.\n",
      "\n",
      "\n",
      "\n",
      "When the alignment is skipped, an operator keeps processing all inputs, even\n",
      "\n",
      "after some checkpoint barriers for checkpoint _n_ arrived. That way, the\n",
      "\n",
      "operator also processes elements that belong to checkpoint _n+1_ before the\n",
      "\n",
      "state snapshot for checkpoint _n_ was taken. On a restore, these records will\n",
      "\n",
      "occur as duplicates, because they are both included in the state snapshot of\n",
      "\n",
      "checkpoint _n_, and will be replayed as part of the data after checkpoint _n_.\n",
      "\n",
      "\n",
      "\n",
      "> Alignment happens only for operators with multiple predecessors\n",
      "\n",
      "> (joins) as well as operators with multiple senders (after a stream\n",
      "\n",
      "> repartitioning/shuffle). Because of that, dataflows with only embarrassingly\n",
      "\n",
      "> parallel streaming operations (`map()`, `flatMap()`, `filter()`, …) actually\n",
      "\n",
      "> give _exactly once_ guarantees even in _at least once_ mode.\n",
      "\n",
      "\n",
      "\n",
      "## State and Fault Tolerance in Batch Programs  [\\#](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/concepts/stateful-stream-processing/\\#state-and-fault-tolerance-in-batch-programs)\n",
      "\n",
      "\n",
      "\n",
      "Flink executes batch programs as a special case of\n",
      "\n",
      "streaming programs in BATCH [ExecutionMode](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/execution/execution_configuration/)\n",
      "\n",
      ", where the streams are bounded (finite number of elements).\n",
      "\n",
      "The concepts above thus apply to batch programs in the same way as well as they apply to streaming\n",
      "\n",
      "programs, with minor exceptions:\n",
      "\n",
      "\n",
      "\n",
      "- [Fault tolerance for batch programs](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/ops/state/task_failure_recovery/)\n",
      "\n",
      "does not use checkpointing. Recovery happens by fully replaying the\n",
      "\n",
      "streams. That is possible, because inputs are bounded. This pushes the\n",
      "\n",
      "cost more towards the recovery, but makes the regular processing cheaper,\n",
      "\n",
      "because it avoids checkpoints.\n",
      "\n",
      "\n",
      "\n",
      "- State backend in batch execution mode use simplified in-memory/out-of-core\n",
      "\n",
      "data structures, rather than key/value indexes.'''\"\"\"\n"
     ]
    }
   ],
   "source": [
    "print(slug_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-3-flash-preview\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fda385",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ uv sync\n",
    "warning: `VIRTUAL_ENV=/home/path/to/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\n",
    "Resolved 95 packages in 1ms\n",
    "Audited 91 packages in 1ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f8ccb",
   "metadata": {},
   "source": [
    "## /NEXT-STEPS\n",
    "\n",
    "* [x] Limit scraping to specific domain\n",
    "* [x] Allow pruning of child urls depending on limited domain\n",
    "* [ ] Allow DB updates to existing records with LLM calls\n",
    "* [ ] Check each db records child urls to only include existing urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1651a0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
